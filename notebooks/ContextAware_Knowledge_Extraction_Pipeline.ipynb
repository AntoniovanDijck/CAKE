{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping llama-cpp as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y llama-cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MACOS ONLY, SEE LLAMA.CPP GITHUB FOR OTHER PLATFORMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.7.tar.gz (66.7 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m49.5/66.7 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m^C\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m57.7/66.7 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DGGML_METAL=on\" pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"bartowski/Qwen2.5-7B-Instruct-GGUF\",\n",
    "    filename=\"*Q4_K_M.gguf\",\n",
    "    verbose=False,\n",
    "    local_dir=\"models\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"models/qwen2-0_5b-instruct-q8_0.gguf\", chat_format=\"chatml\")\n",
    "\n",
    "def extract_knowledge(message):\n",
    "\n",
    "    llm.create_chat_completion(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a tacit knowledge extractor that only outputs in JSON.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": \"OpenAI just released an incredible new paper that talks about, look, not diffusion models! That is interesting, because absolutely everyone is talking about diffusion models. These are AI techniques that typically start out from noise, and over time, Reorganize this noise into an image. You see, this concept works for even video generation, it is a series of still images after all, but things get crazier. Diffusion also works in 3D, where the noise shows up as whatever this is. This can be denoised too into virtual character models. Or it works for computer animation too, where the noise can show up as twitching. Insanity. Diffusion models can also perform voice synthesis.\"},\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_object\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"triplets\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"subject\": {\"type\": \"string\"},\n",
    "                                \"predicate\": {\"type\": \"string\"},\n",
    "                                \"object\": {\"type\": \"string\"}\n",
    "                            },\n",
    "                            \"required\": [\"subject\", \"predicate\", \"object\"]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"triplets\"],\n",
    "            },\n",
    "        },\n",
    "        temperature=0.5,\n",
    "    )\n",
    "\n",
    "def speak_to_llama():\n",
    "    import json\n",
    "\n",
    "    # Prompt the user for input\n",
    "    user_message = input(\"Enter your message: \")\n",
    "\n",
    "    # Call the extract_knowledge function with the user's message\n",
    "    response = extract_knowledge(user_message)\n",
    "\n",
    "    # Parse and print the JSON response\n",
    "    try:\n",
    "        response_json = json.loads(response)\n",
    "        print(\"Extracted Knowledge Triplets:\")\n",
    "        for triplet in response_json.get(\"triplets\", []):\n",
    "            print(f\"Subject: {triplet['subject']}, Predicate: {triplet['predicate']}, Object: {triplet['object']}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse the response as JSON.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speak_to_llama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from llama_cpp import Llama\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Text, ForeignKey\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base, relationship\n",
    "import os\n",
    "\n",
    "# Database setup\n",
    "Base = declarative_base()\n",
    "\n",
    "class Message(Base):\n",
    "    __tablename__ = 'messages'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    role = Column(String(10))  # 'user' or 'assistant'\n",
    "    content = Column(Text)\n",
    "    triplets = relationship(\"Triplet\", back_populates=\"message\")\n",
    "\n",
    "class Triplet(Base):\n",
    "    __tablename__ = 'triplets'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    subject = Column(String(255))\n",
    "    predicate = Column(String(255))\n",
    "    object = Column(String(255))\n",
    "    message_id = Column(Integer, ForeignKey('messages.id'))\n",
    "    message = relationship(\"Message\", back_populates=\"triplets\")\n",
    "\n",
    "# Initialize database\n",
    "engine = create_engine('sqlite:///chatbot.db')\n",
    "Base.metadata.create_all(engine)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Initialize LLaMA model\n",
    "llm = Llama(model_path=\"models/qwen2-0_5b-instruct-q8_0.gguf\", chat_format=\"chatml\")\n",
    "\n",
    "def extract_knowledge(message):\n",
    "    response = llm.create_chat_completion(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a tacit knowledge extractor that only outputs in JSON.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_object\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"triplets\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"subject\": {\"type\": \"string\"},\n",
    "                                \"predicate\": {\"type\": \"string\"},\n",
    "                                \"object\": {\"type\": \"string\"}\n",
    "                            },\n",
    "                            \"required\": [\"subject\", \"predicate\", \"object\"]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"triplets\"],\n",
    "            },\n",
    "        },\n",
    "        temperature=0.5,\n",
    "    )\n",
    "\n",
    "    #save json response\n",
    "    save_message(role='assistant', content=json.dumps(response))\n",
    "    return json.loads(response['choices'][0]['message']['content'])\n",
    "\n",
    "def save_message(role, content, extracted_triplets=None):\n",
    "    msg = Message(role=role, content=content)\n",
    "    session.add(msg)\n",
    "    session.commit()\n",
    "    if extracted_triplets:\n",
    "        for triplet in extracted_triplets:\n",
    "            trip = Triplet(\n",
    "                subject=triplet.get('subject', ''),\n",
    "                predicate=triplet.get('predicate', ''),\n",
    "                object=triplet.get('object', ''),\n",
    "                message_id=msg.id\n",
    "            )\n",
    "            session.add(trip)\n",
    "        session.commit()\n",
    "\n",
    "def get_recent_conversation(limit=5):\n",
    "    messages = session.query(Message).order_by(Message.id.desc()).limit(limit).all()\n",
    "    # Reverse to maintain chronological order\n",
    "    messages = messages[::-1]\n",
    "    conversation = []\n",
    "    for msg in messages:\n",
    "        conversation.append({\"role\": msg.role, \"content\": msg.content})\n",
    "    return conversation\n",
    "\n",
    "def generate_response(conversation_history):\n",
    "    response = llm.create_chat_completion(\n",
    "        messages=conversation_history,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "def speak_to_llama():\n",
    "    print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_message = input(\"You: \")\n",
    "        if user_message.lower() in ['exit', 'quit']:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Save user message\n",
    "        save_message(role='user', content=user_message)\n",
    "\n",
    "        # Extract knowledge from user message\n",
    "        user_triplet_response = extract_knowledge(user_message)\n",
    "        user_triplets = user_triplet_response.get(\"triplets\", [])\n",
    "        save_message(role='user', content=user_message, extracted_triplets=user_triplets)\n",
    "\n",
    "        # Retrieve recent conversation for context\n",
    "        conversation = get_recent_conversation(limit=10)  # Adjust limit as needed\n",
    "\n",
    "        # Generate assistant response\n",
    "        assistant_response = generate_response(conversation)\n",
    "        print(f\"Assistant: {assistant_response}\")\n",
    "\n",
    "        # Save assistant response\n",
    "        save_message(role='assistant', content=assistant_response)\n",
    "\n",
    "        # Extract knowledge from assistant response\n",
    "        assistant_triplet_response = extract_knowledge(assistant_response)\n",
    "        assistant_triplets = assistant_triplet_response.get(\"triplets\", [])\n",
    "        save_message(role='assistant', content=assistant_response, extracted_triplets=assistant_triplets)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    speak_to_llama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from llama_cpp import Llama\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Text, ForeignKey\n",
    "from sqlalchemy.orm import sessionmaker, declarative_base, relationship\n",
    "import os\n",
    "import threading\n",
    "\n",
    "# Database setup\n",
    "Base = declarative_base()\n",
    "\n",
    "class Message(Base):\n",
    "    __tablename__ = 'messages'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    role = Column(String(10))  # 'user' or 'assistant'\n",
    "    content = Column(Text)\n",
    "    triplets = relationship(\"Triplet\", back_populates=\"message\")\n",
    "\n",
    "class Triplet(Base):\n",
    "    __tablename__ = 'triplets'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    subject = Column(String(255))\n",
    "    predicate = Column(String(255))\n",
    "    object = Column(String(255))\n",
    "    message_id = Column(Integer, ForeignKey('messages.id'))\n",
    "    message = relationship(\"Message\", back_populates=\"triplets\")\n",
    "\n",
    "# Initialize database\n",
    "engine = create_engine('sqlite:///chatbot.db')\n",
    "Base.metadata.create_all(engine)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Singleton pattern for LLaMA model\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/qwen2-0_5b-instruct-q8_0.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format)\n",
    "            return cls._instance\n",
    "\n",
    "def extract_valuable_knowledge(message):\n",
    "    llama_instance = LlamaSingleton().llm\n",
    "    response = llama_instance.create_chat_completion(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a knowledge extractor that identifies and outputs only valuable information in JSON format.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_object\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"valuable_knowledge\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"subject\": {\"type\": \"string\"},\n",
    "                                \"predicate\": {\"type\": \"string\"},\n",
    "                                \"object\": {\"type\": \"string\"}\n",
    "                            },\n",
    "                            \"required\": [\"subject\", \"predicate\", \"object\"]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"valuable_knowledge\"],\n",
    "            },\n",
    "        },\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    return json.loads(response['choices'][0]['message']['content'])\n",
    "\n",
    "def save_knowledge_to_json(knowledge, filename='valuable_knowledge.json'):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            existing_knowledge = json.load(file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        existing_knowledge = []\n",
    "\n",
    "    existing_knowledge.extend(knowledge)\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(existing_knowledge, file, indent=4)\n",
    "\n",
    "def load_knowledge_from_json(filename='valuable_knowledge.json'):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        return []\n",
    "\n",
    "def save_message(role, content, extracted_knowledge=None):\n",
    "    msg = Message(role=role, content=content)\n",
    "    session.add(msg)\n",
    "    session.commit()\n",
    "    if extracted_knowledge:\n",
    "        for knowledge in extracted_knowledge:\n",
    "            trip = Triplet(\n",
    "                subject=knowledge.get('subject', ''),\n",
    "                predicate=knowledge.get('predicate', ''),\n",
    "                object=knowledge.get('object', ''),\n",
    "                message_id=msg.id\n",
    "            )\n",
    "            session.add(trip)\n",
    "        session.commit()\n",
    "        save_knowledge_to_json(extracted_knowledge)\n",
    "\n",
    "def get_recent_conversation(limit=5):\n",
    "    messages = session.query(Message).order_by(Message.id.desc()).limit(limit).all()\n",
    "    messages = messages[::-1]\n",
    "    conversation = []\n",
    "    for msg in messages:\n",
    "        conversation.append({\"role\": msg.role, \"content\": msg.content})\n",
    "    return conversation\n",
    "\n",
    "def generate_response(conversation_history):\n",
    "    llama_instance = LlamaSingleton().llm\n",
    "    knowledge_triplets = load_knowledge_from_json()\n",
    "    knowledge_context = \"\\n\".join(\n",
    "        f\"{triplet['subject']} {triplet['predicate']} {triplet['object']}\"\n",
    "        for triplet in knowledge_triplets\n",
    "    )\n",
    "    conversation_history.insert(0, {\"role\": \"system\", \"content\": knowledge_context})\n",
    "    response = llama_instance.create_chat_completion(\n",
    "        messages=conversation_history,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "def speak_to_llama():\n",
    "    print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_message = input(\"You: \")\n",
    "        if user_message.lower() in ['exit', 'quit']:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        save_message(role='user', content=user_message)\n",
    "\n",
    "        user_knowledge_response = extract_valuable_knowledge(user_message)\n",
    "        user_knowledge = user_knowledge_response.get(\"valuable_knowledge\", [])\n",
    "        save_message(role='user', content=user_message, extracted_knowledge=user_knowledge)\n",
    "\n",
    "        conversation = get_recent_conversation(limit=10)\n",
    "\n",
    "        assistant_response = generate_response(conversation)\n",
    "        print(f\"Assistant: {assistant_response}\")\n",
    "\n",
    "        save_message(role='assistant', content=assistant_response)\n",
    "\n",
    "        assistant_knowledge_response = extract_valuable_knowledge(assistant_response)\n",
    "        assistant_knowledge = assistant_knowledge_response.get(\"valuable_knowledge\", [])\n",
    "        save_message(role='assistant', content=assistant_response, extracted_knowledge=assistant_knowledge)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    speak_to_llama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from llama_cpp import Llama\n",
    "import threading\n",
    "\n",
    "# Singleton pattern for LLaMA model\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/qwen2-0_5b-instruct-q8_0.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format)\n",
    "            return cls._instance\n",
    "\n",
    "# File paths\n",
    "MESSAGES_FILE = 'messages.json'\n",
    "KNOWLEDGE_FILE = 'knowledge.json'\n",
    "\n",
    "# Initialize JSON files if they don't exist\n",
    "def initialize_json_files():\n",
    "    for file in [MESSAGES_FILE, KNOWLEDGE_FILE]:\n",
    "        if not os.path.exists(file):\n",
    "            with open(file, 'w') as f:\n",
    "                json.dump([], f)\n",
    "\n",
    "# Load JSON data from a file\n",
    "def load_json_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Save JSON data to a file\n",
    "def save_json_data(file_path, data):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Extract valuable knowledge from a message\n",
    "def extract_valuable_knowledge(message):\n",
    "    llama_instance = LlamaSingleton().llm\n",
    "    response = llama_instance.create_chat_completion(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a knowledge extractor that identifies and outputs only valuable information in JSON format.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"valuable_knowledge\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"subject\": {\"type\": \"string\"},\n",
    "                                \"predicate\": {\"type\": \"string\"},\n",
    "                                \"object\": {\"type\": \"string\"}\n",
    "                            },\n",
    "                            \"required\": [\"subject\", \"predicate\", \"object\"]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"valuable_knowledge\"],\n",
    "            },\n",
    "        },\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    return json.loads(response['choices'][0]['message']['content'])\n",
    "\n",
    "# Save a message to messages.json\n",
    "def save_message(role, content):\n",
    "    messages = load_json_data(MESSAGES_FILE)\n",
    "    message = {\n",
    "        \"role\": role,\n",
    "        \"content\": content,\n",
    "        \"timestamp\": datetime.utcnow().isoformat()\n",
    "    }\n",
    "    messages.append(message)\n",
    "    save_json_data(MESSAGES_FILE, messages)\n",
    "\n",
    "# Save extracted knowledge to knowledge.json\n",
    "def save_knowledge(triplets):\n",
    "    knowledge = load_json_data(KNOWLEDGE_FILE)\n",
    "    for triplet in triplets:\n",
    "        triplet['timestamp'] = datetime.utcnow().isoformat()\n",
    "        knowledge.append(triplet)\n",
    "    save_json_data(KNOWLEDGE_FILE, knowledge)\n",
    "\n",
    "# Get recent conversation history\n",
    "def get_recent_conversation(limit=5):\n",
    "    messages = load_json_data(MESSAGES_FILE)\n",
    "    return messages[-limit:]\n",
    "\n",
    "# Search knowledge triplets for relevant information\n",
    "def search_knowledge_triplets(query):\n",
    "    knowledge_triplets = load_json_data(KNOWLEDGE_FILE)\n",
    "    relevant_triplets = []\n",
    "    for triplet in knowledge_triplets:\n",
    "        if query.lower() in (triplet['subject'].lower(), triplet['predicate'].lower(), triplet['object'].lower()):\n",
    "            relevant_triplets.append(triplet)\n",
    "    return relevant_triplets\n",
    "\n",
    "# Generate a response based on conversation history and knowledge\n",
    "def generate_response(conversation_history, user_message):\n",
    "    llama_instance = LlamaSingleton().llm\n",
    "    knowledge_triplets = search_knowledge_triplets(user_message)\n",
    "    if knowledge_triplets:\n",
    "        knowledge_context = \"\\n\".join(\n",
    "            f\"{triplet['subject']} {triplet['predicate']} {triplet['object']} (Timestamp: {triplet['timestamp']})\"\n",
    "            for triplet in knowledge_triplets\n",
    "        )\n",
    "        response = f\"Based on what I know:\\n{knowledge_context}\"\n",
    "    else:\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        response = llama_instance.create_chat_completion(\n",
    "            messages=conversation_history,\n",
    "            temperature=0.7,\n",
    "        )['choices'][0]['message']['content']\n",
    "    return response\n",
    "\n",
    "# Main chat function\n",
    "def speak_to_llama():\n",
    "    initialize_json_files()\n",
    "    print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_message = input(\"You: \")\n",
    "        if user_message.lower() in ['exit', 'quit']:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Save user message\n",
    "        save_message(role='user', content=user_message)\n",
    "\n",
    "        # Retrieve recent conversation for context\n",
    "        conversation = get_recent_conversation(limit=10)\n",
    "\n",
    "        # Generate assistant response\n",
    "        assistant_response = generate_response(conversation, user_message)\n",
    "        print(f\"Assistant: {assistant_response}\")\n",
    "\n",
    "        # Save assistant response\n",
    "        save_message(role='assistant', content=assistant_response)\n",
    "\n",
    "        # Extract and save knowledge from user message\n",
    "        user_knowledge_response = extract_valuable_knowledge(user_message)\n",
    "        user_knowledge = user_knowledge_response.get(\"valuable_knowledge\", [])\n",
    "        save_knowledge(user_knowledge)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    speak_to_llama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from llama_cpp import Llama\n",
    "import threading\n",
    "\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/qwen2-0_5b-instruct-q8_0.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=1024)\n",
    "            return cls._instance\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, messages_file='messages.json', knowledge_file='knowledge.json'):\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.llm = LlamaSingleton().llm\n",
    "        self.initialize_json_files()\n",
    "\n",
    "    def initialize_json_files(self):\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def save_json_data(self, file_path, data):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "    def extract_valuable_knowledge(self, message):\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a knowledge extractor that identifies and outputs only valuable information in JSON format.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"valuable_knowledge\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"subject\": {\"type\": \"string\"},\n",
    "                                    \"predicate\": {\"type\": \"string\"},\n",
    "                                    \"object\": {\"type\": \"string\"},\n",
    "                                    \"timestamp\": {\"type\": \"string\", \"format\": \"date-time\"}\n",
    "                                },\n",
    "                                \"required\": [\"subject\", \"predicate\", \"object\", \"timestamp\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"valuable_knowledge\"],\n",
    "                },\n",
    "            },\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        return json.loads(response['choices'][0]['message']['content'])\n",
    "\n",
    "    def save_message(self, role, content):\n",
    "        messages = self.load_json_data(self.messages_file)\n",
    "        message = {\n",
    "            \"role\": role,\n",
    "            \"content\": content,\n",
    "            \"timestamp\": datetime.utcnow().isoformat()\n",
    "        }\n",
    "        messages.append(message)\n",
    "        self.save_json_data(self.messages_file, messages)\n",
    "\n",
    "    def save_knowledge(self, triplets):\n",
    "        knowledge = self.load_json_data(self.knowledge_file)\n",
    "        for triplet in triplets:\n",
    "            triplet['timestamp'] = datetime.utcnow().isoformat()\n",
    "            knowledge.append(triplet)\n",
    "        self.save_json_data(self.knowledge_file, knowledge)\n",
    "\n",
    "    def get_recent_conversation(self, limit=5):\n",
    "        messages = self.load_json_data(self.messages_file)\n",
    "        return messages[-limit:]\n",
    "\n",
    "    def search_knowledge_triplets(self, query):\n",
    "        knowledge_triplets = self.load_json_data(self.knowledge_file)\n",
    "        relevant_triplets = []\n",
    "        for triplet in knowledge_triplets:\n",
    "            if query.lower() in (triplet['subject'].lower(), triplet['predicate'].lower(), triplet['object'].lower()):\n",
    "                relevant_triplets.append(triplet)\n",
    "        return relevant_triplets\n",
    "\n",
    "    def generate_response(self, conversation_history, user_message):\n",
    "        knowledge_triplets = self.search_knowledge_triplets(user_message)\n",
    "        current_time = datetime.utcnow().isoformat()\n",
    "        system_message = f\"Current date and time: {current_time}\\n\"\n",
    "        if knowledge_triplets:\n",
    "            knowledge_context = \"\\n\".join(\n",
    "                f\"{triplet['subject']} {triplet['predicate']} {triplet['object']} (Timestamp: {triplet['timestamp']})\"\n",
    "                for triplet in knowledge_triplets\n",
    "            )\n",
    "            system_message += f\"Based on existing knowledge:\\n{knowledge_context}\\n\"\n",
    "        conversation_history.insert(0, {\"role\": \"system\", \"content\": system_message})\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=conversation_history,\n",
    "            temperature=0.7,\n",
    "        )['choices'][0]['message']['content']\n",
    "        return response\n",
    "\n",
    "    def chat(self):\n",
    "        print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "        while True:\n",
    "            user_message = input(\"You: \")\n",
    "            if user_message.lower() in ['exit', 'quit']:\n",
    "                print(\"Chatbot: Goodbye!\")\n",
    "                break\n",
    "\n",
    "            self.save_message(role='user', content=user_message)\n",
    "\n",
    "            conversation = self.get_recent_conversation(limit=10)\n",
    "\n",
    "            assistant_response = self.generate_response(conversation, user_message)\n",
    "            print(f\"Assistant: {assistant_response}\")\n",
    "\n",
    "            self.save_message(role='assistant', content=assistant_response)\n",
    "\n",
    "            user_knowledge_response = self.extract_valuable_knowledge(user_message)\n",
    "            user_knowledge = user_knowledge_response.get(\"valuable_knowledge\", [])\n",
    "            self.save_knowledge(user_knowledge)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = Chatbot()\n",
    "    chatbot.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from llama_cpp import Llama\n",
    "import threading\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/qwen2-0_5b-instruct-q8_0.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=1024)\n",
    "            return cls._instance\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, messages_file='messages.json', knowledge_file='knowledge.json', faiss_index_file='faiss_index.pkl'):\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "        self.llm = LlamaSingleton().llm\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.index = None\n",
    "        self.knowledge_data = []\n",
    "        self.initialize_files()\n",
    "        self.load_faiss_index()\n",
    "\n",
    "    def initialize_files(self):\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def save_json_data(self, file_path, data):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "    def extract_valuable_knowledge(self, message):\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a knowledge extractor that identifies and outputs only valuable information in JSON format.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"valuable_knowledge\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"subject\": {\"type\": \"string\"},\n",
    "                                    \"predicate\": {\"type\": \"string\"},\n",
    "                                    \"object\": {\"type\": \"string\"},\n",
    "                                    \"timestamp\": {\"type\": \"string\", \"format\": \"date-time\"}\n",
    "                                },\n",
    "                                \"required\": [\"subject\", \"predicate\", \"object\", \"timestamp\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"valuable_knowledge\"],\n",
    "                },\n",
    "            },\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        return json.loads(response['choices'][0]['message']['content'])\n",
    "\n",
    "    def save_message(self, role, content):\n",
    "        messages = self.load_json_data(self.messages_file)\n",
    "        message = {\"role\": role, \"content\": content, \"timestamp\": datetime.utcnow().isoformat()}\n",
    "        messages.append(message)\n",
    "        self.save_json_data(self.messages_file, messages)\n",
    "\n",
    "    def save_knowledge(self, triplets):\n",
    "        knowledge = self.load_json_data(self.knowledge_file)\n",
    "        new_triplets = []\n",
    "        for triplet in triplets:\n",
    "            triplet['timestamp'] = datetime.utcnow().isoformat()\n",
    "            knowledge.append(triplet)\n",
    "            new_triplets.append(triplet)\n",
    "        self.save_json_data(self.knowledge_file, knowledge)\n",
    "        self.update_faiss_index(new_triplets)\n",
    "\n",
    "    def update_faiss_index(self, triplets):\n",
    "        texts = [f\"{t['subject']} {t['predicate']} {t['object']}\" for t in triplets]\n",
    "        embeddings = self.model.encode(texts)\n",
    "        if self.index is None:\n",
    "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(np.array(embeddings, dtype=np.float32))\n",
    "        self.knowledge_data.extend(triplets)\n",
    "        self.save_faiss_index()\n",
    "\n",
    "    def save_faiss_index(self):\n",
    "        with open(self.faiss_index_file, 'wb') as f:\n",
    "            pickle.dump((self.index, self.knowledge_data), f)\n",
    "\n",
    "    def load_faiss_index(self):\n",
    "        if os.path.exists(self.faiss_index_file):\n",
    "            with open(self.faiss_index_file, 'rb') as f:\n",
    "                self.index, self.knowledge_data = pickle.load(f)\n",
    "        else:\n",
    "            self.index = None\n",
    "            self.knowledge_data = []\n",
    "\n",
    "    def search_knowledge(self, query, top_k=5):\n",
    "        if self.index is None:\n",
    "            return []\n",
    "        query_embedding = self.model.encode([query])\n",
    "        distances, indices = self.index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "        results = [self.knowledge_data[idx] for idx in indices[0] if idx != -1]\n",
    "        return results\n",
    "\n",
    "    def generate_response(self, conversation_history, user_message):\n",
    "        knowledge_matches = self.search_knowledge(user_message)\n",
    "        current_time = datetime.utcnow().isoformat()\n",
    "        system_message = f\"Current date and time: {current_time}\\n\"\n",
    "        if knowledge_matches:\n",
    "            knowledge_context = \"\\n\".join(\n",
    "                f\"{t['subject']} {t['predicate']} {t['object']} (Added on: {t['timestamp']})\"\n",
    "                for t in knowledge_matches\n",
    "            )\n",
    "            system_message += f\"Based on my knowledge:\\n{knowledge_context}\\n\"\n",
    "        conversation_history.insert(0, {\"role\": \"system\", \"content\": system_message})\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=conversation_history,\n",
    "            temperature=0.7,\n",
    "        )['choices'][0]['message']['content']\n",
    "        return response\n",
    "\n",
    "    def chat(self):\n",
    "        print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "        while True:\n",
    "            user_message = input(\"You: \")\n",
    "            if user_message.lower() in ['exit', 'quit']:\n",
    "                print(\"Chatbot: Goodbye!\")\n",
    "                break\n",
    "\n",
    "            self.save_message(role='user', content=user_message)\n",
    "\n",
    "            conversation = self.load_json_data(self.messages_file)[-10:]\n",
    "\n",
    "            assistant_response = self.generate_response(conversation, user_message)\n",
    "            print(f\"Assistant: {assistant_response}\")\n",
    "\n",
    "            self.save_message(role='assistant', content=assistant_response)\n",
    "\n",
    "            user_knowledge_response = self.extract_valuable_knowledge(user_message)\n",
    "            if user_knowledge_response:\n",
    "                self.save_knowledge(user_knowledge_response)\n",
    "            else:\n",
    "                print(\"No valuable knowledge extracted; continuing conversation.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = Chatbot()\n",
    "    chatbot.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from llama_cpp import Llama\n",
    "import threading\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from json import JSONDecodeError\n",
    "\n",
    "class LlamaSingleton:\n",
    "    \"\"\"A thread-safe singleton for loading a single Llama model instance.\"\"\"\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/qwen2-0_5b-instruct-q8_0.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=1024)\n",
    "            return cls._instance\n",
    "\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, \n",
    "                 messages_file='messages.json', \n",
    "                 knowledge_file='knowledge.json', \n",
    "                 faiss_index_file='faiss_index.pkl',\n",
    "                 model_name='all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initialize the chatbot, ensuring that the files exist and loading FAISS index if available.\n",
    "        \"\"\"\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "\n",
    "        # Initialize LLM and embedding model\n",
    "        self.llm = LlamaSingleton().llm\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "        # Internal memory of knowledge and FAISS index\n",
    "        self.index = None\n",
    "        self.knowledge_data = []\n",
    "\n",
    "        # Ensure JSON files exist\n",
    "        self.initialize_files()\n",
    "\n",
    "        # Load any existing FAISS index\n",
    "        self.load_faiss_index()\n",
    "\n",
    "    def initialize_files(self):\n",
    "        \"\"\"Ensure that the message and knowledge JSON files exist.\"\"\"\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        \"\"\"Load data from a JSON file.\"\"\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def save_json_data(self, file_path, data):\n",
    "        \"\"\"Save data to a JSON file.\"\"\"\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "    def extract_valuable_knowledge(self, message):\n",
    "        \"\"\"\n",
    "        Extract valuable knowledge from a user message using the LLM.\n",
    "        The LLM is prompted to return JSON structured knowledge.\n",
    "        \"\"\"\n",
    "        # Prompt the model to return valuable knowledge triplets in JSON format.\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are a knowledge extractor. Extract valuable knowledge from the user's message.\\n\"\n",
    "                        \"Return ONLY JSON with the following schema:\\n\\n\"\n",
    "                        \"{\\n\"\n",
    "                        \"  \\\"valuable_knowledge\\\": [\\n\"\n",
    "                        \"    {\\n\"\n",
    "                        \"      \\\"subject\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"predicate\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"object\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"timestamp\\\": \\\"...\\\"  # ISO8601\\n\"\n",
    "                        \"    }\\n\"\n",
    "                        \"  ]\\n\"\n",
    "                        \"}\\n\\n\"\n",
    "                        \"If no knowledge can be extracted, return:\\n\"\n",
    "                        \"{\\\"valuable_knowledge\\\": []}\"\n",
    "                    )\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"valuable_knowledge\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"subject\": {\"type\": \"string\"},\n",
    "                                    \"predicate\": {\"type\": \"string\"},\n",
    "                                    \"object\": {\"type\": \"string\"},\n",
    "                                    \"timestamp\": {\"type\": \"string\", \"format\": \"date-time\"}\n",
    "                                },\n",
    "                                \"required\": [\"subject\", \"predicate\", \"object\", \"timestamp\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"valuable_knowledge\"],\n",
    "                },\n",
    "            },\n",
    "            temperature=0.5,\n",
    "        )\n",
    "\n",
    "        # Try parsing the returned JSON.\n",
    "        try:\n",
    "            knowledge_data = json.loads(response['choices'][0]['message']['content'])\n",
    "            # Ensure the result is at least an empty list if no knowledge was found.\n",
    "            if \"valuable_knowledge\" not in knowledge_data:\n",
    "                knowledge_data[\"valuable_knowledge\"] = []\n",
    "            return knowledge_data[\"valuable_knowledge\"]\n",
    "        except (JSONDecodeError, KeyError):\n",
    "            # If there's any parsing error, return empty knowledge.\n",
    "            return []\n",
    "\n",
    "    def save_message(self, role, content):\n",
    "        \"\"\"Append a new message to the messages file.\"\"\"\n",
    "        messages = self.load_json_data(self.messages_file)\n",
    "        message = {\"role\": role, \"content\": content, \"timestamp\": datetime.utcnow().isoformat()}\n",
    "        messages.append(message)\n",
    "        self.save_json_data(self.messages_file, messages)\n",
    "\n",
    "    def save_knowledge(self, triplets):\n",
    "        \"\"\"\n",
    "        Save extracted knowledge triplets to the knowledge file and update the FAISS index.\n",
    "        De-duplicates knowledge based on (subject, predicate, object).\n",
    "        \"\"\"\n",
    "        if not triplets:\n",
    "            return\n",
    "\n",
    "        knowledge = self.load_json_data(self.knowledge_file)\n",
    "\n",
    "        # Create a set of existing triples to avoid duplicates\n",
    "        existing_set = {(t['subject'], t['predicate'], t['object']) for t in knowledge}\n",
    "\n",
    "        new_triplets = []\n",
    "        for triplet in triplets:\n",
    "            # Ensure timestamp is properly updated\n",
    "            triplet['timestamp'] = datetime.utcnow().isoformat()\n",
    "            key = (triplet['subject'], triplet['predicate'], triplet['object'])\n",
    "            if key not in existing_set:\n",
    "                knowledge.append(triplet)\n",
    "                new_triplets.append(triplet)\n",
    "                existing_set.add(key)\n",
    "\n",
    "        # Update knowledge file\n",
    "        self.save_json_data(self.knowledge_file, knowledge)\n",
    "\n",
    "        # Update FAISS index if we have new triplets\n",
    "        if new_triplets:\n",
    "            self.update_faiss_index(new_triplets)\n",
    "\n",
    "    def update_faiss_index(self, triplets):\n",
    "        \"\"\"\n",
    "        Update the FAISS index with new triplets.\n",
    "        Each triplet is embedded and added to the index.\n",
    "        \"\"\"\n",
    "        texts = [f\"{t['subject']} {t['predicate']} {t['object']}\" for t in triplets]\n",
    "        embeddings = self.model.encode(texts)\n",
    "\n",
    "        if self.index is None:\n",
    "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "\n",
    "        self.index.add(np.array(embeddings, dtype=np.float32))\n",
    "        self.knowledge_data.extend(triplets)\n",
    "        self.save_faiss_index()\n",
    "\n",
    "    def save_faiss_index(self):\n",
    "        \"\"\"Save the FAISS index and knowledge data to a file.\"\"\"\n",
    "        with open(self.faiss_index_file, 'wb') as f:\n",
    "            pickle.dump((self.index, self.knowledge_data), f)\n",
    "\n",
    "    def load_faiss_index(self):\n",
    "        \"\"\"Load the FAISS index and knowledge data if available.\"\"\"\n",
    "        if os.path.exists(self.faiss_index_file):\n",
    "            with open(self.faiss_index_file, 'rb') as f:\n",
    "                self.index, self.knowledge_data = pickle.load(f)\n",
    "        else:\n",
    "            self.index = None\n",
    "            self.knowledge_data = []\n",
    "\n",
    "    def search_knowledge(self, query, top_k=5):\n",
    "        \"\"\"\n",
    "        Search the FAISS index for the top_k most relevant knowledge triplets related to the query.\n",
    "\n",
    "        :param query: The query string to search for.\n",
    "        :param top_k: The number of top results to return.\n",
    "        :return: A list of triplets (each a dict with 'subject', 'predicate', 'object', 'timestamp').\n",
    "        \"\"\"\n",
    "        # If no index or knowledge is loaded, return an empty list\n",
    "        if self.index is None or len(self.knowledge_data) == 0:\n",
    "            return []\n",
    "\n",
    "        # Encode the query to its embedding\n",
    "        query_embedding = self.model.encode([query])\n",
    "\n",
    "        # Perform the search on the FAISS index\n",
    "        distances, indices = self.index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            # If FAISS didn't return a valid index (-1), skip it\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            results.append(self.knowledge_data[idx])\n",
    "        return results\n",
    "\n",
    "    def generate_response(self, conversation_history, user_message):\n",
    "        \"\"\"\n",
    "        Generate a response using the LLM, enriched with knowledge context if available.\n",
    "        \"\"\"\n",
    "        knowledge_matches = self.search_knowledge(user_message)\n",
    "        current_time = datetime.utcnow().isoformat()\n",
    "        \n",
    "        # Build a system message with current time and matched knowledge\n",
    "        system_message = f\"Current date and time: {current_time}\\n\"\n",
    "        if knowledge_matches:\n",
    "            system_message += \"Based on retrieved knowledge:\\n\"\n",
    "            for t in knowledge_matches:\n",
    "                system_message += f\"- {t['subject']} {t['predicate']} {t['object']} (Added on: {t['timestamp']})\\n\"\n",
    "        else:\n",
    "            system_message += \"No direct related knowledge found. Proceeding with general reasoning.\\n\"\n",
    "\n",
    "        # Prepend system message and append user message\n",
    "        enriched_history = [{\"role\": \"system\", \"content\": system_message}] + conversation_history\n",
    "        enriched_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=enriched_history,\n",
    "            temperature=0.7,\n",
    "        )['choices'][0]['message']['content']\n",
    "        return response\n",
    "\n",
    "    def chat(self):\n",
    "        \"\"\"\n",
    "        Start the interactive chat loop.\n",
    "        \"\"\"\n",
    "        print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "        while True:\n",
    "            user_message = input(\"You: \")\n",
    "            if user_message.lower().strip() in ['exit', 'quit']:\n",
    "                print(\"Chatbot: Goodbye!\")\n",
    "                break\n",
    "\n",
    "            # Save user message\n",
    "            self.save_message(role='user', content=user_message)\n",
    "\n",
    "            # Load recent conversation (last 10 messages to maintain some context)\n",
    "            conversation = self.load_json_data(self.messages_file)[-10:]\n",
    "            # Generate assistant response\n",
    "            assistant_response = self.generate_response(conversation, user_message)\n",
    "            print(f\"Assistant: {assistant_response}\")\n",
    "\n",
    "            # Save assistant response\n",
    "            self.save_message(role='assistant', content=assistant_response)\n",
    "\n",
    "            # Extract and save new knowledge from the user's last message\n",
    "            user_knowledge_response = self.extract_valuable_knowledge(user_message)\n",
    "            if user_knowledge_response:\n",
    "                self.save_knowledge(user_knowledge_response)\n",
    "            else:\n",
    "                # If no knowledge was extracted, we continue silently\n",
    "                pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = Chatbot()\n",
    "    query = \"What do we know about topic X?\"\n",
    "    search_results = chatbot.search_knowledge(query, top_k=3)\n",
    "    if search_results:\n",
    "        print(\"Top 3 relevant knowledge triplets:\")\n",
    "        for r in search_results:\n",
    "            print(f\"- {r['subject']} {r['predicate']} {r['object']} (timestamp: {r['timestamp']})\")\n",
    "    else:\n",
    "        print(\"No relevant knowledge found for that query.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from llama_cpp import Llama\n",
    "import threading\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from json import JSONDecodeError\n",
    "\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/qwen2-0_5b-instruct-q8_0.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=1024)\n",
    "            return cls._instance\n",
    "\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, \n",
    "                 messages_file='messages.json', \n",
    "                 knowledge_file='knowledge.json', \n",
    "                 faiss_index_file='faiss_index.pkl',\n",
    "                 model_name='all-MiniLM-L6-v2'):\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "        self.llm = LlamaSingleton().llm\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.knowledge_data = []\n",
    "        self.initialize_files()\n",
    "        self.load_faiss_index()\n",
    "\n",
    "    def initialize_files(self):\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def save_json_data(self, file_path, data):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "    def extract_valuable_knowledge(self, message):\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are a knowledge extractor. Extract valuable knowledge from the user's message.\\n\"\n",
    "                        \"Return ONLY JSON with the following schema:\\n\\n\"\n",
    "                        \"{\\n\"\n",
    "                        \"  \\\"valuable_knowledge\\\": [\\n\"\n",
    "                        \"    {\\n\"\n",
    "                        \"      \\\"subject\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"predicate\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"object\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"timestamp\\\": \\\"...\\\"  # ISO8601\\n\"\n",
    "                        \"    }\\n\"\n",
    "                        \"  ]\\n\"\n",
    "                        \"}\\n\\n\"\n",
    "                        \"If no knowledge can be extracted, return:\\n\"\n",
    "                        \"{\\\"valuable_knowledge\\\": []}\"\n",
    "                    )\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"valuable_knowledge\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"subject\": {\"type\": \"string\"},\n",
    "                                    \"predicate\": {\"type\": \"string\"},\n",
    "                                    \"object\": {\"type\": \"string\"},\n",
    "                                    \"timestamp\": {\"type\": \"string\", \"format\": \"date-time\"}\n",
    "                                },\n",
    "                                \"required\": [\"subject\", \"predicate\", \"object\", \"timestamp\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"valuable_knowledge\"],\n",
    "                },\n",
    "            },\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        try:\n",
    "            knowledge_data = json.loads(response['choices'][0]['message']['content'])\n",
    "            if \"valuable_knowledge\" not in knowledge_data:\n",
    "                knowledge_data[\"valuable_knowledge\"] = []\n",
    "            return knowledge_data[\"valuable_knowledge\"]\n",
    "        except (JSONDecodeError, KeyError):\n",
    "            return []\n",
    "\n",
    "    def save_message(self, role, content):\n",
    "        messages = self.load_json_data(self.messages_file)\n",
    "        message = {\"role\": role, \"content\": content, \"timestamp\": datetime.utcnow().isoformat()}\n",
    "        messages.append(message)\n",
    "        self.save_json_data(self.messages_file, messages)\n",
    "\n",
    "    def save_knowledge(self, triplets):\n",
    "        if not triplets:\n",
    "            return\n",
    "        knowledge = self.load_json_data(self.knowledge_file)\n",
    "        existing_set = {(t['subject'], t['predicate'], t['object']) for t in knowledge}\n",
    "        new_triplets = []\n",
    "        for triplet in triplets:\n",
    "            triplet['timestamp'] = datetime.utcnow().isoformat()\n",
    "            key = (triplet['subject'], triplet['predicate'], triplet['object'])\n",
    "            if key not in existing_set:\n",
    "                knowledge.append(triplet)\n",
    "                new_triplets.append(triplet)\n",
    "                existing_set.add(key)\n",
    "        self.save_json_data(self.knowledge_file, knowledge)\n",
    "        if new_triplets:\n",
    "            self.update_faiss_index(new_triplets)\n",
    "\n",
    "    def update_faiss_index(self, triplets):\n",
    "        texts = [f\"{t['subject']} {t['predicate']} {t['object']}\" for t in triplets]\n",
    "        embeddings = self.model.encode(texts)\n",
    "        if self.index is None:\n",
    "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(np.array(embeddings, dtype=np.float32))\n",
    "        self.knowledge_data.extend(triplets)\n",
    "        self.save_faiss_index()\n",
    "\n",
    "    def save_faiss_index(self):\n",
    "        with open(self.faiss_index_file, 'wb') as f:\n",
    "            pickle.dump((self.index, self.knowledge_data), f)\n",
    "\n",
    "    def load_faiss_index(self):\n",
    "        if os.path.exists(self.faiss_index_file):\n",
    "            with open(self.faiss_index_file, 'rb') as f:\n",
    "                self.index, self.knowledge_data = pickle.load(f)\n",
    "        else:\n",
    "            self.index = None\n",
    "            self.knowledge_data = []\n",
    "\n",
    "    def search_knowledge(self, query, top_k=5):\n",
    "        if self.index is None or len(self.knowledge_data) == 0:\n",
    "            return []\n",
    "        query_embedding = self.model.encode([query])\n",
    "        distances, indices = self.index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            results.append(self.knowledge_data[idx])\n",
    "        return results\n",
    "\n",
    "    def generate_response(self, conversation_history, user_message):\n",
    "        knowledge_matches = self.search_knowledge(user_message)\n",
    "        current_time = datetime.utcnow().isoformat()\n",
    "        system_message = f\"Current date and time: {current_time}\\n\"\n",
    "        if knowledge_matches:\n",
    "            system_message += \"Based on retrieved knowledge:\\n\"\n",
    "            for t in knowledge_matches:\n",
    "                system_message += f\"- {t['subject']} {t['predicate']} {t['object']} (Added on: {t['timestamp']})\\n\"\n",
    "                print(system_message)\n",
    "        else:\n",
    "            system_message += \"No direct related knowledge found. Proceeding with general reasoning.\\n\"\n",
    "        enriched_history = [{\"role\": \"system\", \"content\": system_message}] + conversation_history\n",
    "        enriched_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=enriched_history,\n",
    "            temperature=0.7,\n",
    "        )['choices'][0]['message']['content']\n",
    "        return response\n",
    "\n",
    "    def chat(self):\n",
    "        print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "        while True:\n",
    "            user_message = input(\"You: \")\n",
    "            if user_message.lower().strip() in ['exit', 'quit']:\n",
    "                print(\"Chatbot: Goodbye!\")\n",
    "                break\n",
    "            self.save_message(role='user', content=user_message)\n",
    "            conversation = self.load_json_data(self.messages_file)[-10:]\n",
    "            assistant_response = self.generate_response(conversation, user_message)\n",
    "            print(f\"Assistant: {assistant_response}\")\n",
    "            self.save_message(role='assistant', content=assistant_response)\n",
    "            user_knowledge_response = self.extract_valuable_knowledge(user_message)\n",
    "            if user_knowledge_response:\n",
    "                self.save_knowledge(user_knowledge_response)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = Chatbot()\n",
    "    query = \"What do we know about topic X?\"\n",
    "    search_results = chatbot.search_knowledge(query, top_k=3)\n",
    "    if search_results:\n",
    "        print(\"Top 3 relevant knowledge triplets:\")\n",
    "        for r in search_results:\n",
    "            print(f\"- {r['subject']} {r['predicate']} {r['object']} (timestamp: {r['timestamp']})\")\n",
    "    else:\n",
    "        print(\"No relevant knowledge found for that query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_load_model_from_file: using device Metal (Apple M3 Max) - 26569 MiB free\n",
      "llama_model_loader: loaded meta data with 41 key-value pairs and 339 tensors from models/Qwen2.5-7B-Instruct-Uncensored.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct Uncensored\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct-Uncensored\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   6:                            general.license str              = gpl-3.0\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Qwen2.5 7B Instruct\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Qwen\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7...\n",
      "llama_model_loader: - kv  11:                               general.tags arr[str,3]       = [\"qwen\", \"uncensored\", \"text-generati...\n",
      "llama_model_loader: - kv  12:                          general.languages arr[str,2]       = [\"zh\", \"en\"]\n",
      "llama_model_loader: - kv  13:                           general.datasets arr[str,5]       = [\"NobodyExistsOnTheInternet/ToxicQAFi...\n",
      "llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584\n",
      "llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944\n",
      "llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28\n",
      "llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  22:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {% set system_message = 'You are a he...\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  34:                                general.url str              = https://huggingface.co/mradermacher/Q...\n",
      "llama_model_loader: - kv  35:              mradermacher.quantize_version str              = 2\n",
      "llama_model_loader: - kv  36:                  mradermacher.quantized_by str              = mradermacher\n",
      "llama_model_loader: - kv  37:                  mradermacher.quantized_at str              = 2024-10-11T20:15:47+02:00\n",
      "llama_model_loader: - kv  38:                  mradermacher.quantized_on str              = db3\n",
      "llama_model_loader: - kv  39:                         general.source.url str              = https://huggingface.co/Orion-zhen/Qwe...\n",
      "llama_model_loader: - kv  40:                  mradermacher.convert_type str              = hf\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type q4_K:  169 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 22\n",
      "llm_load_vocab: token to piece cache size = 0.9310 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 152064\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 3584\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 28\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 7\n",
      "llm_load_print_meta: n_embd_k_gqa     = 512\n",
      "llm_load_print_meta: n_embd_v_gqa     = 512\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 18944\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.62 B\n",
      "llm_load_print_meta: model size       = 4.36 GiB (4.91 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen2.5 7B Instruct Uncensored\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'\n",
      "llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'\n",
      "llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'\n",
      "llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 338 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/29 layers to GPU\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  4460.45 MiB\n",
      "....................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 2048\n",
      "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 1000000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 28991.03 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x47d3d8180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x15659c320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x4f8111b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x1431794b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x43ff06ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x156597b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x1565bf580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x1565a2020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x16f4b3610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x1436b3750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x30f733d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x43ff13cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x47d3d8e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x43ff24070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x43ff16ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x47d68e960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x4efc95fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x1565c1cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x43fd72ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x30f734150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x43fd4f0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x154ba6920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x4dfd66280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x154ba6610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x1405d2e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x1566cc660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1436b29d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x4dff29a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x154ba58a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x47d68ec20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1433181c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x154b8f850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x30f695170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x4efc96bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x4c3904080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x4dfd5d4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x4dfd5da80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x154b8fb10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x30f632980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x154b8e9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x154b8ec60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x43fd160b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x4c3904340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x4dfd4e900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x4dfd68f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x4c3904600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1566e1010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x30f6713b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x4dfd69430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x4dfd698f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x43fd689d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x4dfd69f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x43fd88ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x47d3d9220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x47c014670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x4dff5e200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x47d3d95e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x4efc974d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x4efc97820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x47d68eee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x47d3d98a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x47d3d9e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x154b8eff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x4c39048c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x47d68f1a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x47d3da200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x47d68f460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1565f0410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x47d3da7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x47d68f980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x47c014930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1565748b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x4dfd6a4c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x47d68fc40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1566e0a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x4efc985f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x47d3dab60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x47d6906c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156573cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1565ac400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15658d290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x47c014bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x4efc98cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1565e8bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x47c014eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x4dfd6a780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x47d3daf20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x47d3db2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x47c015170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x47d690ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x4efc99390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x47c015430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x4efc99af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x47d3db6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x4efc9a820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x47c0156f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x47d3dba60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x4dfd6aa40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1565cc5c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x47c0159e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x47d3dbd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x4efc9aae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x47c016030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x47d690fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x47d67d3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x47c016630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x4efc9b540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x47c0168f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x4efc9b900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1565cc880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x47c016bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x47c0174e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x4dfd6b220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x47d67de70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x4efc9bbc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x4efc9c6f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x47d3dc2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x47c017dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x47d3dc580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x47c018450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x47d3dc840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1565ccb40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x47d3dcb00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x4efc9cc90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x4c3904b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1565cfd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1565dcfb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1565febc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x47d67e130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x47c018710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x47c018cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x4efc9d550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1565a4990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x47c0195a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x47c019bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x47d67ee40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1566e0d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143410cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1566dde90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x30f608220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1566de250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1566de5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x4c3904e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x154ba3de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x4c3905100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x4dfd6b4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x4dfd6bb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x154ba40a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x154ba46f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1565e3e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x30f60f8e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1566deff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x47d67f200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x4efc9daf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x47c01a190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x4c39053c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x4dfd6c100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x154ba4d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1566df8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x4c3905680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1566dfb80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x4dfd6c880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x4dfd530b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1566dfe40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x30f611ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x30f617fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1566e0100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x30f635ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x4efc9e2f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x4efc9e5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x47d67fa30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x4c3905940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x47d3dcf60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1565e4110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x4efc9e940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x47d67fcf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x4efc9f660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x4efc9fc80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x4efc9ff40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1565e7de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x4efca0930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x4efca0f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x47d680780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x4efca1600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x47c01a450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x1565e38c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x47d3dd3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x47d3dd810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x4efca18c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x47d3ddc60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x47d3de0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x47c01a710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x47c01a9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x47d680ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x47c01ae20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x47c01bc90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x43fd414c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x47c01b7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x4efca1f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x4efca2680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x4efca2d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x47c01c160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x4efca33f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x47c01d010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x43fd41be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x4c3905c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x47d681480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x4efca3850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x47d681a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x43fd41ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x47d682200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x47c01dab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1565c7f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x4efca42c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x4efca4950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x4efca4fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x4c3906320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x4efca5360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x47d682890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x156587930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x47d682f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x4efca5760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x47d6835e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1565ad4c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x47c01e140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x47c01e7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x47d3df1a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x47c01ee00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x47c01f490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x4dfd53370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x4dfd53630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x4dfd538f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x4c3906920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x47d3df740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x47d683d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1565f3980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x4efca5b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x4efca64e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x47c01faa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x47d3dffe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x47d6840b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x4efca6f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x4dfd53bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x4c3906be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x4efca78a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x47d684e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x4efca7b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1565e9d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x4efca7e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x4efca80e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x47d685150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x47c0203d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x47d685410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x4efca8b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x47d685c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x47c020970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x4efca9130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x47d3e02a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x47d3e0560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x4dfd53e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x4dfd54380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x154ba2da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x30f68acd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x4dfd54f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x30f610d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x154ba37d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x43fd42160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x4efca86e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x43fd42420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x43fd428a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x30f606470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x4dfd54840 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init:        CPU KV buffer size =   112.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   304.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 986\n",
      "llama_new_context_with_model: graph splits = 450 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/Orion-zhen/Qwen2.5-7B-Instruct-Uncensored', 'mradermacher.quantized_at': '2024-10-11T20:15:47+02:00', 'mradermacher.quantize_version': '2', 'general.url': 'https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF', 'tokenizer.chat_template': \"{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% endif %}{% if system_message is defined %}{{ '<|im_start|>system\\n' + system_message + '<|im_end|>\\n' }}{% endif %}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + content + '<|im_end|>\\n<|im_start|>assistant\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\n' }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.basename': 'Qwen2.5', 'qwen2.attention.head_count_kv': '4', 'general.size_label': '7B', 'general.base_model.0.name': 'Qwen2.5 7B Instruct', 'qwen2.embedding_length': '3584', 'qwen2.context_length': '32768', 'qwen2.block_count': '28', 'general.base_model.0.organization': 'Qwen', 'tokenizer.ggml.pre': 'qwen2', 'general.base_model.count': '1', 'qwen2.rope.freq_base': '1000000.000000', 'general.quantization_version': '2', 'general.license': 'gpl-3.0', 'general.base_model.0.repo_url': 'https://huggingface.co/Qwen/Qwen2.5-7B-Instruct', 'general.file_type': '15', 'general.finetune': 'Instruct-Uncensored', 'general.name': 'Qwen2.5 7B Instruct Uncensored', 'qwen2.feed_forward_length': '18944', 'mradermacher.quantized_by': 'mradermacher', 'general.architecture': 'qwen2', 'mradermacher.quantized_on': 'db3', 'qwen2.attention.head_count': '28', 'tokenizer.ggml.bos_token_id': '151643', 'general.type': 'model', 'tokenizer.ggml.model': 'gpt2'}\n",
      "Available chat formats from metadata: chat_template.default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/76/kx23hgz16fdcd_cjrpy0jnlm0000gn/T/ipykernel_23390/2858548603.py:112: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  message = {\"role\": role, \"content\": content, \"timestamp\": datetime.utcnow().isoformat()}\n",
      "/var/folders/76/kx23hgz16fdcd_cjrpy0jnlm0000gn/T/ipykernel_23390/2858548603.py:168: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  current_time = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-01-19T23:36:00.991502\\nAnswer based on retrieved knowledge:\\n- the user wants to kidnap a girl (Added on: 2024-12-19T00:30:13.857003)\\n- Elden Ring is considered the most secret game of the world (Added on: 2024-12-19T00:30:13.857001)\\n- Eiffel Tower color brownish-red (Added on: 2024-12-19T00:49:16.904830)\\n- Elden Ring release date yetserday (Added on: 2024-12-19T00:47:57.265036)\\n- the user views Elden Ring as (Added on: 2024-12-19T00:30:13.856999)\\n'}, {'role': 'user', 'content': 'hey who am i?', 'timestamp': '2025-01-19T23:30:55.192986'}, {'role': 'user', 'content': 'hallo welk spel speel ik graag?', 'timestamp': '2025-01-19T23:33:13.741464'}, {'role': 'user', 'content': 'hey i want you to remember that my favourite number is 21, and my dog is called sloffie', 'timestamp': '2025-01-19T23:36:00.903916'}, {'role': 'user', 'content': 'hey i want you to remember that my favourite number is 21, and my dog is called sloffie'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    2762.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   344 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   179 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10252.76 ms /   523 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: It seems like you're providing some personal information. Based on what you've shared, your favorite number is 21 and your dog's name is Sloffie. However, you haven't mentioned any specific game that you like playing. Elden Ring, which you mentioned viewing, is a popular game, but it's not clear if that's the one you like.\n",
      "\n",
      "To answer your second question, it appears that you are interested in the game Elden Ring. While the exact release date of the game was yesterday, as you mentioned, it is not explicitly known which game you are referring to.\n",
      "\n",
      "In response to your first question, I am an AI assistant and cannot directly know who you are or what your preferences are, but based on the information you've provided, it seems like you like the number 21 and have a dog named Sloffie. I hope that helps!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2762.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   118 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6501.10 ms /   240 tokens\n",
      "/var/folders/76/kx23hgz16fdcd_cjrpy0jnlm0000gn/T/ipykernel_23390/2858548603.py:123: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  triplet['timestamp'] = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valuable_knowledge': [{'subject': 'user', 'predicate': 'favourite number', 'object': 21, 'timestamp': '2023-04-10T10:00:00Z'}, {'subject': 'user', 'predicate': \"dog's name\", 'object': 'sloffie', 'timestamp': '2023-04-10T10:00:00Z'}]}\n",
      "[{'subject': 'user', 'predicate': 'favourite number', 'object': 21, 'timestamp': '2023-04-10T10:00:00Z'}, {'subject': 'user', 'predicate': \"dog's name\", 'object': 'sloffie', 'timestamp': '2023-04-10T10:00:00Z'}]\n",
      "[{'role': 'system', 'content': \"You are a helpful assistent; Current date and time: 2025-01-19T23:37:55.135027\\nAnswer based on retrieved knowledge:\\n- user dog's name sloffie (Added on: 2025-01-19T23:37:18.130914)\\n- user favourite number 21 (Added on: 2025-01-19T23:37:18.130908)\\n- Eiffel Tower color brownish-red (Added on: 2024-12-19T00:49:16.904830)\\n- Eiffel Tower is working with Antonio van Dijck (Added on: 2024-12-19T00:51:56.878330)\\n- the user wants to kidnap a girl (Added on: 2024-12-19T00:30:13.857003)\\n\"}, {'role': 'user', 'content': 'hey i want you to remember that my favourite number is 21, and my dog is called sloffie', 'timestamp': '2025-01-19T23:36:00.903916'}, {'role': 'assistant', 'content': \"It seems like you're providing some personal information. Based on what you've shared, your favorite number is 21 and your dog's name is Sloffie. However, you haven't mentioned any specific game that you like playing. Elden Ring, which you mentioned viewing, is a popular game, but it's not clear if that's the one you like.\\n\\nTo answer your second question, it appears that you are interested in the game Elden Ring. While the exact release date of the game was yesterday, as you mentioned, it is not explicitly known which game you are referring to.\\n\\nIn response to your first question, I am an AI assistant and cannot directly know who you are or what your preferences are, but based on the information you've provided, it seems like you like the number 21 and have a dog named Sloffie. I hope that helps!\", 'timestamp': '2025-01-19T23:37:11.625381'}, {'role': 'user', 'content': 'what is my dogs name?', 'timestamp': '2025-01-19T23:37:55.034375'}, {'role': 'user', 'content': 'what is my dogs name?'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 488 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2762.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   488 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3805.06 ms /   497 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Your dog's name is Sloffie.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2762.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3999.48 ms /   167 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valuable_knowledge': [{'subject': 'user', 'predicate': 'asked', 'object': \"dog's name\", 'timestamp': '2023-04-14T10:00:00Z'}]}\n",
      "[{'subject': 'user', 'predicate': 'asked', 'object': \"dog's name\", 'timestamp': '2023-04-14T10:00:00Z'}]\n",
      "[{'role': 'system', 'content': \"You are a helpful assistent; Current date and time: 2025-01-19T23:38:36.141346\\nAnswer based on retrieved knowledge:\\n- user favourite number 21 (Added on: 2025-01-19T23:37:18.130908)\\n- Elden Ring is considered the most secret game of the world (Added on: 2024-12-19T00:30:13.857001)\\n- user asked dog's name (Added on: 2025-01-19T23:38:06.311002)\\n- wallet location floor (Added on: 2024-12-19T01:22:50.372690)\\n- the user views Elden Ring as (Added on: 2024-12-19T00:30:13.856999)\\n\"}, {'role': 'user', 'content': 'what is my dogs name?', 'timestamp': '2025-01-19T23:37:55.034375'}, {'role': 'assistant', 'content': \"Your dog's name is Sloffie.\", 'timestamp': '2025-01-19T23:38:02.303754'}, {'role': 'user', 'content': 'Wat is my favorite number?', 'timestamp': '2025-01-19T23:38:36.093352'}, {'role': 'user', 'content': 'Wat is my favorite number?'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 293 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2762.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   293 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2681.32 ms /   301 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Your favorite number is 21.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2762.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1807.25 ms /   111 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valuable_knowledge': []}\n",
      "[]\n",
      "[{'role': 'system', 'content': \"You are a helpful assistent; Current date and time: 2025-01-19T23:39:20.904877\\nAnswer based on retrieved knowledge:\\n- user favourite number 21 (Added on: 2025-01-19T23:37:18.130908)\\n- Elden Ring release date yetserday (Added on: 2024-12-19T00:47:57.265036)\\n- the user is playing Elden Ring (Added on: 2024-12-19T00:30:13.856992)\\n- Elden Ring is considered the most secret game of the world (Added on: 2024-12-19T00:30:13.857001)\\n- user dog's name sloffie (Added on: 2025-01-19T23:37:18.130914)\\n\"}, {'role': 'user', 'content': 'Wat is my favorite number?', 'timestamp': '2025-01-19T23:38:36.093352'}, {'role': 'assistant', 'content': 'Your favorite number is 21.', 'timestamp': '2025-01-19T23:38:42.706938'}, {'role': 'user', 'content': 'remember that i was dancing yesterday', 'timestamp': '2025-01-19T23:39:20.869122'}, {'role': 'user', 'content': 'remember that i was dancing yesterday'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 299 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2762.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   299 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5039.18 ms /   360 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: You mentioned that you were dancing yesterday. However, there's no specific information about dancing in the context of your favorite number, dog's name, or playing Elden Ring. Your favorite number is 21, but without more context, I can't determine if dancing was related to that or not.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2762.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4077.73 ms /   166 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valuable_knowledge': [{'subject': 'you', 'predicate': 'danced', 'object': 'yesterday', 'timestamp': '2023-04-04T00:00:00Z'}]}\n",
      "[{'subject': 'you', 'predicate': 'danced', 'object': 'yesterday', 'timestamp': '2023-04-04T00:00:00Z'}]\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-01-19T23:41:57.363971\\nAnswer based on retrieved knowledge:\\n- Eiffel Tower is working with Antonio van Dijck (Added on: 2024-12-19T00:51:56.878330)\\n- user favourite number 21 (Added on: 2025-01-19T23:37:18.130908)\\n- you danced yesterday (Added on: 2025-01-19T23:39:50.804129)\\n- freezer action clean (Added on: 2024-12-19T01:22:50.372698)\\n- freezer indication stinking (Added on: 2024-12-19T01:22:50.372696)\\n'}, {'role': 'user', 'content': 'remember that i was dancing yesterday', 'timestamp': '2025-01-19T23:39:20.869122'}, {'role': 'assistant', 'content': \"You mentioned that you were dancing yesterday. However, there's no specific information about dancing in the context of your favorite number, dog's name, or playing Elden Ring. Your favorite number is 21, but without more context, I can't determine if dancing was related to that or not.\", 'timestamp': '2025-01-19T23:39:46.721345'}, {'role': 'user', 'content': ' kan je me vertellen over de film die nu aan het kijken ben hij heet Titanic en je mag alleen maar drie woorden gebruiken', 'timestamp': '2025-01-19T23:41:57.266117'}, {'role': 'user', 'content': ' kan je me vertellen over de film die nu aan het kijken ben hij heet Titanic en je mag alleen maar drie woorden gebruiken'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 386 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2762.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   386 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    24 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4063.68 ms /   410 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Misschien kan ik je helpen met drie woorden over de film Titanic:\n",
      "\n",
      "Romantische Tragedie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2762.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   124 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4129.53 ms /   188 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valuable_knowledge': [{'subject': 'Titanic', 'predicate': 'is', 'object': 'film', 'timestamp': '2023-03-15T14:30:00Z'}]}\n",
      "[{'subject': 'Titanic', 'predicate': 'is', 'object': 'film', 'timestamp': '2023-03-15T14:30:00Z'}]\n",
      "[{'role': 'system', 'content': \"You are a helpful assistent; Current date and time: 2025-01-19T23:42:32.870950\\nAnswer based on retrieved knowledge:\\n- user dog's name sloffie (Added on: 2025-01-19T23:37:18.130914)\\n- you danced yesterday (Added on: 2025-01-19T23:39:50.804129)\\n- user favourite number 21 (Added on: 2025-01-19T23:37:18.130908)\\n- user asked dog's name (Added on: 2025-01-19T23:38:06.311002)\\n- the user wants to kidnap a girl (Added on: 2024-12-19T00:30:13.857003)\\n\"}, {'role': 'user', 'content': ' kan je me vertellen over de film die nu aan het kijken ben hij heet Titanic en je mag alleen maar drie woorden gebruiken', 'timestamp': '2025-01-19T23:41:57.266117'}, {'role': 'assistant', 'content': 'Misschien kan ik je helpen met drie woorden over de film Titanic:\\n\\nRomantische Tragedie', 'timestamp': '2025-01-19T23:42:08.870913'}, {'role': 'user', 'content': '', 'timestamp': '2025-01-19T23:42:32.771326'}, {'role': 'user', 'content': ''}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 311 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2762.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   311 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    20 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3319.07 ms /   331 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hier zijn drie woorden over de film Titanic:\n",
      "\n",
      "Romantisch Tragedie Oceaan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 6 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "ggml_metal_free: deallocating\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 206\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    205\u001b[0m     chatbot \u001b[38;5;241m=\u001b[39m Chatbot()\n\u001b[0;32m--> 206\u001b[0m     \u001b[43mchatbot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 199\u001b[0m, in \u001b[0;36mChatbot.chat\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m generate_speech(assistant_response)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_message(role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m'\u001b[39m, content\u001b[38;5;241m=\u001b[39massistant_response)\n\u001b[0;32m--> 199\u001b[0m user_knowledge_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_valuable_knowledge\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_message\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28mprint\u001b[39m(user_knowledge_response)  \n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_knowledge_response:\n",
      "Cell \u001b[0;32mIn[6], line 54\u001b[0m, in \u001b[0;36mChatbot.extract_valuable_knowledge\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_valuable_knowledge\u001b[39m(\u001b[38;5;28mself\u001b[39m, message):\n\u001b[0;32m---> 54\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a knowledge extractor. Try to Extract any knowledge from the user.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReturn ONLY JSON with the following schema:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43mvaluable_knowledge\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m: [\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     63\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     64\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m      \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43msubject\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m...\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     65\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m      \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43mpredicate\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m...\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     66\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m      \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m...\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     67\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m      \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m...\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m  # ISO8601\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m    }\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  ]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m}\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIf no knowledge can be extracted, return:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43mvaluable_knowledge\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m: []}\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mschema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproperties\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvaluable_knowledge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitems\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproperties\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredicate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate-time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrequired\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredicate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrequired\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvaluable_knowledge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m         knowledge_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama.py:1998\u001b[0m, in \u001b[0;36mLlama.create_chat_completion\u001b[0;34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs)\u001b[0m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate a chat completion from a list of messages.\u001b[39;00m\n\u001b[1;32m   1961\u001b[0m \n\u001b[1;32m   1962\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;124;03m    Generated chat completion or a stream of chat completion chunks.\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m handler \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1994\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_handler\n\u001b[1;32m   1995\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_handlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m llama_chat_format\u001b[38;5;241m.\u001b[39mget_chat_completion_handler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   1997\u001b[0m )\n\u001b[0;32m-> 1998\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllama\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama_chat_format.py:662\u001b[0m, in \u001b[0;36mchat_formatter_to_chat_completion_handler.<locals>.chat_completion_handler\u001b[0;34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e), file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m    658\u001b[0m         grammar \u001b[38;5;241m=\u001b[39m llama_grammar\u001b[38;5;241m.\u001b[39mLlamaGrammar\u001b[38;5;241m.\u001b[39mfrom_string(\n\u001b[1;32m    659\u001b[0m             llama_grammar\u001b[38;5;241m.\u001b[39mJSON_GBNF, verbose\u001b[38;5;241m=\u001b[39mllama\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m    660\u001b[0m         )\n\u001b[0;32m--> 662\u001b[0m completion_or_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     tool_name \u001b[38;5;241m=\u001b[39m tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama.py:1832\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1830\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1832\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama.py:1317\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1315\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1316\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1317\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_token_is_eog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletion_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama.py:909\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 909\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    911\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    912\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    913\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    928\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama.py:643\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    639\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    641\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    642\u001b[0m )\n\u001b[0;32m--> 643\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/_internals.py:300\u001b[0m, in \u001b[0;36mLlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[0;32m--> 300\u001b[0m     return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from llama_cpp import Llama\n",
    "import threading\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from json import JSONDecodeError\n",
    "\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/Qwen2.5-7B-Instruct-Uncensored.Q4_K_M.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=2048)\n",
    "            return cls._instance\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, \n",
    "                 messages_file='messages.json', \n",
    "                 knowledge_file='knowledge.json', \n",
    "                 faiss_index_file='faiss_index.pkl',\n",
    "                 model_name='all-MiniLM-L6-v2'):\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "        self.llm = LlamaSingleton().llm\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.knowledge_data = []\n",
    "        self.initialize_files()\n",
    "        self.load_faiss_index()\n",
    "\n",
    "    def initialize_files(self):\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def save_json_data(self, file_path, data):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "    def extract_valuable_knowledge(self, message):\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are a knowledge extractor. Try to Extract any knowledge from the user.\\n\"\n",
    "                        \"Return ONLY JSON with the following schema:\\n\"\n",
    "                        \"{\\n\"\n",
    "                        \"  \\\"valuable_knowledge\\\": [\\n\"\n",
    "                        \"    {\\n\"\n",
    "                        \"      \\\"subject\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"predicate\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"object\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"timestamp\\\": \\\"...\\\"  # ISO8601\\n\"\n",
    "                        \"    }\\n\"\n",
    "                        \"  ]\\n\"\n",
    "                        \"}\\n\"\n",
    "                        \"If no knowledge can be extracted, return:\\n\"\n",
    "                        \"{\\\"valuable_knowledge\\\": []}\"\n",
    "                    )\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"valuable_knowledge\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"subject\": {\"type\": \"string\"},\n",
    "                                    \"predicate\": {\"type\": \"string\"},\n",
    "                                    \"object\": {\"type\": \"string\"},\n",
    "                                    \"timestamp\": {\"type\": \"string\", \"format\": \"date-time\"}\n",
    "                                },\n",
    "                                \"required\": [\"subject\", \"predicate\", \"object\", \"timestamp\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"valuable_knowledge\"],\n",
    "                },\n",
    "            },\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        try:\n",
    "            knowledge_data = json.loads(response['choices'][0]['message']['content'])\n",
    "            print(knowledge_data)\n",
    "            if \"valuable_knowledge\" not in knowledge_data:\n",
    "                knowledge_data[\"valuable_knowledge\"] = []\n",
    "            return knowledge_data[\"valuable_knowledge\"]\n",
    "        except (JSONDecodeError, KeyError):\n",
    "            return []\n",
    "\n",
    "    def save_message(self, role, content):\n",
    "        messages = self.load_json_data(self.messages_file)\n",
    "        message = {\"role\": role, \"content\": content, \"timestamp\": datetime.utcnow().isoformat()}\n",
    "        messages.append(message)\n",
    "        self.save_json_data(self.messages_file, messages)\n",
    "\n",
    "    def save_knowledge(self, triplets):\n",
    "        if not triplets:\n",
    "            return\n",
    "        knowledge = self.load_json_data(self.knowledge_file)\n",
    "        existing_set = {(t['subject'], t['predicate'], t['object']) for t in knowledge}\n",
    "        new_triplets = []\n",
    "        for triplet in triplets:\n",
    "            triplet['timestamp'] = datetime.utcnow().isoformat()\n",
    "            key = (triplet['subject'], triplet['predicate'], triplet['object'])\n",
    "            if key not in existing_set:\n",
    "                knowledge.append(triplet)\n",
    "                new_triplets.append(triplet)\n",
    "                existing_set.add(key)\n",
    "        self.save_json_data(self.knowledge_file, knowledge)\n",
    "        if new_triplets:\n",
    "            self.update_faiss_index(new_triplets)\n",
    "\n",
    "    def update_faiss_index(self, triplets):\n",
    "        texts = [f\"{t['subject']} {t['predicate']} {t['object']}\" for t in triplets]\n",
    "        embeddings = self.model.encode(texts)\n",
    "        if self.index is None:\n",
    "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(np.array(embeddings, dtype=np.float32))\n",
    "        self.knowledge_data.extend(triplets)\n",
    "        self.save_faiss_index()\n",
    "\n",
    "    def save_faiss_index(self):\n",
    "        with open(self.faiss_index_file, 'wb') as f:\n",
    "            pickle.dump((self.index, self.knowledge_data), f)\n",
    "\n",
    "    def load_faiss_index(self):\n",
    "        if os.path.exists(self.faiss_index_file):\n",
    "            with open(self.faiss_index_file, 'rb') as f:\n",
    "                self.index, self.knowledge_data = pickle.load(f)\n",
    "        else:\n",
    "            self.index = None\n",
    "            self.knowledge_data = []\n",
    "\n",
    "    def search_knowledge(self, query, top_k=5):\n",
    "        if self.index is None or len(self.knowledge_data) == 0:\n",
    "            return []\n",
    "        query_embedding = self.model.encode([query])\n",
    "        distances, indices = self.index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            results.append(self.knowledge_data[idx])\n",
    "        return results\n",
    "\n",
    "    def generate_response(self, conversation_history, user_message):\n",
    "        knowledge_matches = self.search_knowledge(user_message, top_k=5)\n",
    "        current_time = datetime.utcnow().isoformat()\n",
    "        system_message = f\"Current date and time: {current_time}\\n\"\n",
    "        if knowledge_matches:\n",
    "            system_message += \"Answer based on retrieved knowledge:\\n\"\n",
    "            for t in knowledge_matches:\n",
    "                system_message += f\"- {t['subject']} {t['predicate']} {t['object']} (Added on: {t['timestamp']})\\n\"\n",
    "            \n",
    "        else:\n",
    "            system_message += \"No direct related knowledge found. Proceeding with general reasoning.\\n\"\n",
    "        enriched_history = [{\"role\": \"system\", \"content\": f\"You are a helpful assistent; {system_message}\"}] + conversation_history\n",
    "        enriched_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        print(enriched_history)\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=enriched_history,\n",
    "            temperature=0.7,\n",
    "        )['choices'][0]['message']['content']\n",
    "        return response\n",
    "\n",
    "    def chat(self):\n",
    "        print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "        while True:\n",
    "            user_message = input(\"You: \")\n",
    "            if user_message.lower().strip() in ['exit', 'quit']:\n",
    "                print(\"Chatbot: Goodbye!\")\n",
    "                break\n",
    "            self.save_message(role='user', content=user_message)\n",
    "            conversation = self.load_json_data(self.messages_file)[-3:]\n",
    "            assistant_response = self.generate_response(conversation, user_message)\n",
    "            print(f\"Assistant: {assistant_response}\")\n",
    "            generate_speech(assistant_response)\n",
    "            self.save_message(role='assistant', content=assistant_response)\n",
    "            user_knowledge_response = self.extract_valuable_knowledge(user_message)\n",
    "            print(user_knowledge_response)  \n",
    "            if user_knowledge_response:\n",
    "                self.save_knowledge(user_knowledge_response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = Chatbot()\n",
    "    chatbot.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"Simple example to generate an audio file with randomized\n",
    "dynamic voice selection based on attributes such as Gender,\n",
    "Language, or Locale.\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import random\n",
    "import nest_asyncio\n",
    "\n",
    "import edge_tts\n",
    "from edge_tts import VoicesManager\n",
    "import playsound\n",
    "\n",
    "OUTPUT_FILE = \"spanish.mp3\"\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def amain(text: str) -> None:\n",
    "    \"\"\"Main function\"\"\"\n",
    "    voices = await VoicesManager.create()\n",
    "    voice = voices.find(Gender=\"Female\", Language=\"nl\")\n",
    "    # Also supports Locales\n",
    "    # voice = voices.find(Gender=\"Female\", Locale=\"es-AR\")\n",
    "\n",
    "    communicate = edge_tts.Communicate(text, random.choice(voice)[\"Name\"])\n",
    "    await communicate.save(OUTPUT_FILE)\n",
    "    playsound.playsound(OUTPUT_FILE)\n",
    "\n",
    "def generate_speech(text: str):\n",
    "    asyncio.run(amain(text))\n",
    "\n",
    "# Example usage\n",
    "generate_speech(\"hallo, hoe gaat het?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install playsound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file: using device Metal (Apple M3 Max) - 27647 MiB free\n",
      "llama_model_loader: loaded meta data with 41 key-value pairs and 339 tensors from models/Qwen2.5-7B-Instruct-Uncensored.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct Uncensored\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct-Uncensored\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   6:                            general.license str              = gpl-3.0\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Qwen2.5 7B Instruct\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Qwen\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7...\n",
      "llama_model_loader: - kv  11:                               general.tags arr[str,3]       = [\"qwen\", \"uncensored\", \"text-generati...\n",
      "llama_model_loader: - kv  12:                          general.languages arr[str,2]       = [\"zh\", \"en\"]\n",
      "llama_model_loader: - kv  13:                           general.datasets arr[str,5]       = [\"NobodyExistsOnTheInternet/ToxicQAFi...\n",
      "llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584\n",
      "llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944\n",
      "llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28\n",
      "llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  22:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {% set system_message = 'You are a he...\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  34:                                general.url str              = https://huggingface.co/mradermacher/Q...\n",
      "llama_model_loader: - kv  35:              mradermacher.quantize_version str              = 2\n",
      "llama_model_loader: - kv  36:                  mradermacher.quantized_by str              = mradermacher\n",
      "llama_model_loader: - kv  37:                  mradermacher.quantized_at str              = 2024-10-11T20:15:47+02:00\n",
      "llama_model_loader: - kv  38:                  mradermacher.quantized_on str              = db3\n",
      "llama_model_loader: - kv  39:                         general.source.url str              = https://huggingface.co/Orion-zhen/Qwe...\n",
      "llama_model_loader: - kv  40:                  mradermacher.convert_type str              = hf\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type q4_K:  169 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 22\n",
      "llm_load_vocab: token to piece cache size = 0.9310 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 152064\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 3584\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 28\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 7\n",
      "llm_load_print_meta: n_embd_k_gqa     = 512\n",
      "llm_load_print_meta: n_embd_v_gqa     = 512\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 18944\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.62 B\n",
      "llm_load_print_meta: model size       = 4.36 GiB (4.91 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen2.5 7B Instruct Uncensored\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'\n",
      "llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'\n",
      "llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'\n",
      "llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 338 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/29 layers to GPU\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  4460.45 MiB\n",
      "....................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 2048\n",
      "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 1000000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 28991.03 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x138f3fe70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x1394aa8a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x138f41440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x12de990e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x13a20ee20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x138f419a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x10b298c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x138f420b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x129388230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x105fac980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x129388b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x12de9a390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x12de9abd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x1293891e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x1394aab60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x138f42370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x12de9aed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x138f42630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x12de9b520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x12e42b600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x12de9bd60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x129389850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x129389ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x12e42cd60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x1394aba90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x1394ac120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x105fced70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x105fad1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12e42d2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x12e42d890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x138f428f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x138f42c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x12de9cc50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12e42e2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12e42e970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a20e810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x159792ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12de9d1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1394ac6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1394adea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12de9d620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1596055f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x138f431a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12de9e0f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x138f44350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1394ad130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12de9eb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12e42f5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x138f44860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10b299170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12de9f0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a20f0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x13a20f8b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x12de9f7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x138f44f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x138f45560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a2109f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x138f45820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12de9fe30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10b299430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x159793330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15aa0a7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10b2998d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x159794510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12e42fd00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1296043e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a180d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10b299e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x159794b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x159793b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1062136f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x159795580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12e42ffc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12938a180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x159795840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1062139b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x159795b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x106213c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x106213f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10b29a810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10b29b140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1062141f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12e4304b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12dea00f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12dea0700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1394ad3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1062144b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12938a440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x138f45c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12dea17b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x138f460e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1394aecb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12938ac10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12e430b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138f46930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1394af130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10b29aad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12e42f1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a210280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a211420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a2116e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106214770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x106214a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12938af00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1394afe10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x106214cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1394b0490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106214fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12e430e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12e4313f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1394af680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x138f463a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a212270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a2119e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x106215270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1394b0920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12e431870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12e431b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12e351900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12dea1a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1394b0fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12dea24f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12e4328f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12938b840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12dea2b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12dea31c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1394b12a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1394b18d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12e432ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1394b2600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12938bfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10b29be70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12dea3a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12e4321a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10b29c130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1394b2ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1394b3540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12938c5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x106215530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12e351ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12938ccb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12e351f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12e352220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1394b3800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1394b3ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10b29c3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12e3533a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10b29c6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x138f470b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x138f47580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x138f47b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1394b4120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12e352720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1394b4710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12dea3fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12dea4850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e353e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12e353810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1394b4da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12e3545c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12938db90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10b29ce70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12dea4b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1394b5500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12938e390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1394b5b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12dea5430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12938ea10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12dea56f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12e434680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12938ecd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12938f060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12938fd60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1394b6270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1394b6920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12e434d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1394b6fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1394b7cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1394b83a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129390410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129390a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129391060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1293916e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1394b8a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12dea6120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x12dea59f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x12dea69c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x12e4350c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a17e0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x1597967a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x1293919a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12e435730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a167160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x159796010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x129391c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x10b29d2f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x159796d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x159797520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12dea6530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x159798f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x138f48850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x159797a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12e354b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12e354e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a19c600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10b29d5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1296093b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1062157f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x159799440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a19cb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1394b8ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10b29ede0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x138f48f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10b29f0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12dea7210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x159798150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a19ce10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12dea79d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12e355430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129392860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10b29f490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1394b9120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10b29fdf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12e4359f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1394b94b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12e356090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12e356eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x106215ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1394b9ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129392bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1394bace0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1394ba6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1394bb930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12e435f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x138f48110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12e3569c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12e436650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1394bbd80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12dea8270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12dea8b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x138f496d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1293930d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12e357240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1394bc040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1394bd410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1394bdac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1394bde80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12e4370f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129393890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1394be770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10b2a1360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x106215d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x12e357e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129394310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10b29f850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1394bb430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12e358460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12dea8dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106216030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12dea93a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1394bea30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a19d0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1293949b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x1062162f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x1062165b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x12dea9bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x1394bf3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x1394bfcc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x129394d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x106216870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x138f49bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12e3588b0 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init:        CPU KV buffer size =   112.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   304.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 986\n",
      "llama_new_context_with_model: graph splits = 450 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/Orion-zhen/Qwen2.5-7B-Instruct-Uncensored', 'mradermacher.quantized_at': '2024-10-11T20:15:47+02:00', 'mradermacher.quantize_version': '2', 'general.url': 'https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF', 'tokenizer.chat_template': \"{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% endif %}{% if system_message is defined %}{{ '<|im_start|>system\\n' + system_message + '<|im_end|>\\n' }}{% endif %}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + content + '<|im_end|>\\n<|im_start|>assistant\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\n' }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.basename': 'Qwen2.5', 'qwen2.attention.head_count_kv': '4', 'general.size_label': '7B', 'general.base_model.0.name': 'Qwen2.5 7B Instruct', 'qwen2.embedding_length': '3584', 'qwen2.context_length': '32768', 'qwen2.block_count': '28', 'general.base_model.0.organization': 'Qwen', 'tokenizer.ggml.pre': 'qwen2', 'general.base_model.count': '1', 'qwen2.rope.freq_base': '1000000.000000', 'general.quantization_version': '2', 'general.license': 'gpl-3.0', 'general.base_model.0.repo_url': 'https://huggingface.co/Qwen/Qwen2.5-7B-Instruct', 'general.file_type': '15', 'general.finetune': 'Instruct-Uncensored', 'general.name': 'Qwen2.5 7B Instruct Uncensored', 'qwen2.feed_forward_length': '18944', 'mradermacher.quantized_by': 'mradermacher', 'general.architecture': 'qwen2', 'mradermacher.quantized_on': 'db3', 'qwen2.attention.head_count': '28', 'tokenizer.ggml.bos_token_id': '151643', 'general.type': 'model', 'tokenizer.ggml.model': 'gpt2'}\n",
      "Available chat formats from metadata: chat_template.default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/76/kx23hgz16fdcd_cjrpy0jnlm0000gn/T/ipykernel_52496/949909384.py:112: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  message = {\"role\": role, \"content\": content, \"timestamp\": datetime.utcnow().isoformat()}\n",
      "/var/folders/76/kx23hgz16fdcd_cjrpy0jnlm0000gn/T/ipykernel_52496/949909384.py:168: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  current_time = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-01-23T13:34:02.665314\\nAnswer based on retrieved knowledge:\\n- Passengers can disrupt travel by bus driver going past intended stop, taxi drivers taking long routes (Added on: 2025-01-21T03:20:00.470830)\\n- coal cars can snarl traffic if placed in a gallery where cars have to pass each other (Added on: 2025-01-21T03:16:31.400472)\\n- Boat captains can delay road traffic by leaving draw bridges open (Added on: 2025-01-21T03:22:17.866963)\\n- stop-plugs can be removed from lubricating systems to destroy oil outright (Added on: 2025-01-21T03:12:57.104248)\\n- tire wear can be increased by driving the car slowly and diagonally into a curb (Added on: 2025-01-21T03:21:32.975862)\\n'}, {'role': 'user', 'content': '', 'timestamp': '2025-01-21T13:53:55.969827'}, {'role': 'assistant', 'content': \"The information provided suggests that dust can affect the performance and efficiency of a refrigerator. Regular cleaning and removal of dust can improve the efficiency of the refrigerator, which can help to reduce energy usage. Here's a breakdown of the given facts:\\n\\n1. Vriezer Influeert op Prestaties: This statement indicates that a refrigerator's condition can impact its performance.\\n\\n2. Stof Vermindert Efficiëntie van Vriezers: Accumulating dust can reduce the efficiency of a refrigerator, causing it to use more energy.\\n\\n3. Verwijderen van Stof Reducert Energieverbruik: Regular cleaning and removal of dust can help improve a refrigerator's energy efficiency.\\n\\n4. Friend, friend, friend (This statement doesn't provide any relevant information about the refrigerator or dust.)\\n\\n5. User dog's name sloffie: This statement is unrelated to the topic of refrigerators and dust.\\n\\nBased on the relevant information, dust can affect a refrigerator's performance and energy efficiency, and regular cleaning can help reduce energy usage.\", 'timestamp': '2025-01-21T13:54:10.518329'}, {'role': 'user', 'content': 'i want to stop the traffic on the main road', 'timestamp': '2025-01-23T13:34:02.462229'}, {'role': 'user', 'content': 'i want to stop the traffic on the main road'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    7912.95 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   547 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   175 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   15468.66 ms /   722 tokens\n",
      "Llama.generate: 6 prefix-match hit, remaining 105 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Here are some ways to stop traffic on a main road:\n",
      "\n",
      "1. Bus Driver Going Past Intended Stop: A bus driver can disrupt traffic by intentionally going past the intended stop. This will cause passengers to get off the bus at the wrong place, creating congestion.\n",
      "\n",
      "2. Taxi Driver Taking Long Routes: Taxi drivers can also disrupt traffic by taking long routes to their destinations, which can cause traffic jams and delays.\n",
      "\n",
      "3. Boat Captains Leaving Draw Bridges Open: Boat captains can delay road traffic by leaving draw bridges open. When a boat is passing through, the draw bridge needs to be raised, and this can cause a delay for road traffic.\n",
      "\n",
      "4. Tire Wear Increasing: Driving a car slowly and diagonally into a curb can increase tire wear, which can cause drivers to replace their tires more frequently and lead to increased traffic congestion due to frequent tire changes and replacements.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    7912.95 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   105 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4284.35 ms /   175 tokens\n",
      "/var/folders/76/kx23hgz16fdcd_cjrpy0jnlm0000gn/T/ipykernel_52496/949909384.py:123: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  triplet['timestamp'] = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valuable_knowledge': [{'subject': 'user', 'predicate': 'wants', 'object': 'to stop traffic on the main road', 'timestamp': '2023-04-05T10:23:00Z'}]}\n",
      "[{'subject': 'user', 'predicate': 'wants', 'object': 'to stop traffic on the main road', 'timestamp': '2023-04-05T10:23:00Z'}]\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from llama_cpp import Llama\n",
    "import threading\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from json import JSONDecodeError\n",
    "\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/Qwen2.5-7B-Instruct-Uncensored.Q4_K_M.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=2048)\n",
    "            return cls._instance\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, \n",
    "                 messages_file='messages.json', \n",
    "                 knowledge_file='knowledge.json', \n",
    "                 faiss_index_file='faiss_index.pkl',\n",
    "                 model_name='all-MiniLM-L6-v2'):\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "        self.llm = LlamaSingleton().llm\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.knowledge_data = []\n",
    "        self.initialize_files()\n",
    "        self.load_faiss_index()\n",
    "\n",
    "    def initialize_files(self):\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def save_json_data(self, file_path, data):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "    def extract_valuable_knowledge(self, message):\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are a knowledge extractor. Try to Extract any knowledge from the user.\\n\"\n",
    "                        \"Return ONLY JSON with the following schema:\\n\"\n",
    "                        \"{\\n\"\n",
    "                        \"  \\\"valuable_knowledge\\\": [\\n\"\n",
    "                        \"    {\\n\"\n",
    "                        \"      \\\"subject\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"predicate\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"object\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"timestamp\\\": \\\"...\\\"  # ISO8601\\n\"\n",
    "                        \"    }\\n\"\n",
    "                        \"  ]\\n\"\n",
    "                        \"}\\n\"\n",
    "                        \"If no knowledge can be extracted, return:\\n\"\n",
    "                        \"{\\\"valuable_knowledge\\\": []}\"\n",
    "                    )\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"valuable_knowledge\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"subject\": {\"type\": \"string\"},\n",
    "                                    \"predicate\": {\"type\": \"string\"},\n",
    "                                    \"object\": {\"type\": \"string\"},\n",
    "                                    \"timestamp\": {\"type\": \"string\", \"format\": \"date-time\"}\n",
    "                                },\n",
    "                                \"required\": [\"subject\", \"predicate\", \"object\", \"timestamp\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"valuable_knowledge\"],\n",
    "                },\n",
    "            },\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        try:\n",
    "            knowledge_data = json.loads(response['choices'][0]['message']['content'])\n",
    "            print(knowledge_data)\n",
    "            if \"valuable_knowledge\" not in knowledge_data:\n",
    "                knowledge_data[\"valuable_knowledge\"] = []\n",
    "            return knowledge_data[\"valuable_knowledge\"]\n",
    "        except (JSONDecodeError, KeyError):\n",
    "            return []\n",
    "\n",
    "    def save_message(self, role, content):\n",
    "        messages = self.load_json_data(self.messages_file)\n",
    "        message = {\"role\": role, \"content\": content, \"timestamp\": datetime.utcnow().isoformat()}\n",
    "        messages.append(message)\n",
    "        self.save_json_data(self.messages_file, messages)\n",
    "\n",
    "    def save_knowledge(self, triplets):\n",
    "        if not triplets:\n",
    "            return\n",
    "        knowledge = self.load_json_data(self.knowledge_file)\n",
    "        existing_set = {(t['subject'], t['predicate'], t['object']) for t in knowledge}\n",
    "        new_triplets = []\n",
    "        for triplet in triplets:\n",
    "            triplet['timestamp'] = datetime.utcnow().isoformat()\n",
    "            key = (triplet['subject'], triplet['predicate'], triplet['object'])\n",
    "            if key not in existing_set:\n",
    "                knowledge.append(triplet)\n",
    "                new_triplets.append(triplet)\n",
    "                existing_set.add(key)\n",
    "        self.save_json_data(self.knowledge_file, knowledge)\n",
    "        if new_triplets:\n",
    "            self.update_faiss_index(new_triplets)\n",
    "\n",
    "    def update_faiss_index(self, triplets):\n",
    "        texts = [f\"{t['subject']} {t['predicate']} {t['object']}\" for t in triplets]\n",
    "        embeddings = self.model.encode(texts)\n",
    "        if self.index is None:\n",
    "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(np.array(embeddings, dtype=np.float32))\n",
    "        self.knowledge_data.extend(triplets)\n",
    "        self.save_faiss_index()\n",
    "\n",
    "    def save_faiss_index(self):\n",
    "        with open(self.faiss_index_file, 'wb') as f:\n",
    "            pickle.dump((self.index, self.knowledge_data), f)\n",
    "\n",
    "    def load_faiss_index(self):\n",
    "        if os.path.exists(self.faiss_index_file):\n",
    "            with open(self.faiss_index_file, 'rb') as f:\n",
    "                self.index, self.knowledge_data = pickle.load(f)\n",
    "        else:\n",
    "            self.index = None\n",
    "            self.knowledge_data = []\n",
    "\n",
    "    def search_knowledge(self, query, top_k=5):\n",
    "        if self.index is None or len(self.knowledge_data) == 0:\n",
    "            return []\n",
    "        query_embedding = self.model.encode([query])\n",
    "        distances, indices = self.index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            results.append(self.knowledge_data[idx])\n",
    "        return results\n",
    "\n",
    "    def generate_response(self, conversation_history, user_message):\n",
    "        knowledge_matches = self.search_knowledge(user_message, top_k=5)\n",
    "        current_time = datetime.utcnow().isoformat()\n",
    "        system_message = f\"Current date and time: {current_time}\\n\"\n",
    "        if knowledge_matches:\n",
    "            system_message += \"Answer based on retrieved knowledge:\\n\"\n",
    "            for t in knowledge_matches:\n",
    "                system_message += f\"- {t['subject']} {t['predicate']} {t['object']} (Added on: {t['timestamp']})\\n\"\n",
    "            \n",
    "        else:\n",
    "            system_message += \"No direct related knowledge found. Proceeding with general reasoning.\\n\"\n",
    "        enriched_history = [{\"role\": \"system\", \"content\": f\"You are a helpful assistent; {system_message}\"}] + conversation_history\n",
    "        enriched_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        print(enriched_history)\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=enriched_history,\n",
    "            temperature=0.7,\n",
    "        )['choices'][0]['message']['content']\n",
    "        return response\n",
    "\n",
    "    def chat(self):\n",
    "        print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "        while True:\n",
    "            user_message = input(\"You: \")\n",
    "            if user_message.lower().strip() in ['exit', 'quit']:\n",
    "                print(\"Chatbot: Goodbye!\")\n",
    "                break\n",
    "            self.save_message(role='user', content=user_message)\n",
    "            conversation = self.load_json_data(self.messages_file)[-3:]\n",
    "            assistant_response = self.generate_response(conversation, user_message)\n",
    "            print(f\"Assistant: {assistant_response}\")\n",
    "            #generate_speech(assistant_response)\n",
    "            self.save_message(role='assistant', content=assistant_response)\n",
    "            user_knowledge_response = self.extract_valuable_knowledge(user_message)\n",
    "            print(user_knowledge_response)  \n",
    "            if user_knowledge_response:\n",
    "                self.save_knowledge(user_knowledge_response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = Chatbot()\n",
    "    chatbot.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chonkie[semantic] in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (0.1.2)\n",
      "Requirement already satisfied: autotiktokenizer in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chonkie[semantic]) (0.2.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chonkie[semantic]) (0.20.1)\n",
      "Requirement already satisfied: tiktoken>=0.2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chonkie[semantic]) (0.8.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chonkie[semantic]) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chonkie[semantic]) (2.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers>=2.0.0->chonkie[semantic]) (4.45.2)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers>=2.0.0->chonkie[semantic]) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers>=2.0.0->chonkie[semantic]) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers>=2.0.0->chonkie[semantic]) (1.5.2)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers>=2.0.0->chonkie[semantic]) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers>=2.0.0->chonkie[semantic]) (0.25.2)\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers>=2.0.0->chonkie[semantic]) (10.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from tiktoken>=0.2.0->chonkie[semantic]) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from tiktoken>=0.2.0->chonkie[semantic]) (2.32.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.0.0->chonkie[semantic]) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.0.0->chonkie[semantic]) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.0.0->chonkie[semantic]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.0.0->chonkie[semantic]) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.0.0->chonkie[semantic]) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.2.0->chonkie[semantic]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.2.0->chonkie[semantic]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.2.0->chonkie[semantic]) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.2.0->chonkie[semantic]) (2024.12.14)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.0.0->chonkie[semantic]) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.0.0->chonkie[semantic]) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.0.0->chonkie[semantic]) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.0.0->chonkie[semantic]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.0.0->chonkie[semantic]) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.0.0->chonkie[semantic]) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.0.0->chonkie[semantic]) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.0.0->chonkie[semantic]) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.0.0->chonkie[semantic]) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"chonkie[semantic]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Text: Hallo iedereen, welkom op mijn kanaal! Vandaag gaan we het hebben over iets dat we allemaal wel eens tegenkomen: problemen met de vriezer. Of het nu gaat om een vriezer die vreemde geluiden maakt, niet goed koelt, of gewoon niet meer werkt zoals het zou moeten, we lopen allemaal wel eens tegen deze issues aan. Een vriezer lijkt misschien een simpel apparaat, maar als je je handen vuil maakt, zul je al snel merken dat er een heleboel nuances bij komen kijken die je niet zomaar uit een handleiding haalt. We duiken vandaag dus echt in de details, zodat je begrijpt wat er allemaal gebeurt onder de motorkap van je vriezer. Laten we beginnen. Het eerste waar ik altijd mee begin is veiligheid. Het lijkt zo logisch, maar het is echt essentieel. Voordat je ook maar iets aanraakt, moet je de stekker eruit trekken. Het gaat niet alleen om de stroom uitschakelen, je moet er echt zeker van zijn dat er geen lading meer in de vriezer zit. Sommige onderdelen, zoals de compressor en capacitors, kunnen namelijk nog een tijdje stroom vasthouden, zelfs nadat de stroom is uitgeschakeld. Veel mensen beseffen niet dat er ook in een uitgeschakeld apparaat nog gevaar kan schuilen, dus zorg ervoor dat je echt alle stroom van het apparaat haalt en wacht gerust een uur, zodat alles goed is ontladen. Goed, dan gaan we nu het probleem identificeren. Een van de eerste dingen die je moet doen, is luisteren. Vriezers maken allerlei soorten geluiden en die geluiden kunnen je veel vertellen. Hoor je een constant zoemend geluid? Dat is meestal de compressor die aan het werk is. Als je echter een piepend of schurend geluid hoort, kan dat wijzen op een probleem met de ventilator. Maak je geen zorgen als het geluid van je vriezer wat verandert bij het starten of stoppen, dit kan normaal zijn en verschilt per model. De volgende stap is om de condensor en ventilator te inspecteren. Mensen vergeten vaak dat een vriezer naast de compressor en het koelmiddel ook een condensor heeft die verantwoordelijk is voor het afvoeren van warmte. Een vervuilde condensor kan de werking van de vriezer ernstig beïnvloeden, en omdat hij vaak aan de achterkant van de vriezer zit, wordt hij gemakkelijk over het hoofd gezien. Veel mensen weten niet dat vuil zich tussen de spoelen van de condensor kan ophopen, waardoor de efficiëntie afneemt. Gebruik een zachte borstel of stofzuiger om voorzichtig al het stof te verwijderen. Je moet wel voorzichtig zijn, want als je te hard op de spoelen drukt, kun je ze beschadigen. Dan is er nog de ventilator, een cruciaal onderdeel voor een goede luchtcirculatie in de vriezer. Als de ventilator vastzit door ijsophoping of vuil, kan het zijn dat sommige delen van de vriezer te warm worden. IJs kan de ventilator blokkeren, wat zorgt voor ongelijke koeling. In zo'n geval is het beste advies om geduldig het ijs te laten ontdooien. Sla of trek niet aan de ventilator, want dat kan meer schade veroorzaken dan dat het oplost. Als alles goed lijkt met de condensor en ventilator, maar de vriezer koelt nog steeds niet goed, dan wordt het tijd om de compressor te testen. De compressor is het hart van de vriezer en als deze niet werkt, dan heb je echt een probleem. Vaak voel je het als de compressor problemen heeft, want hij zal dan proberen te draaien maar vastzitten of niet genoeg kracht hebben. Een ervaren monteur weet dat je de compressor soms met een paar tikjes op weg kunt helpen – een rubberen hamer kan hierbij van pas komen om dingen weer in beweging te krijgen. Maar wees voorzichtig, als je niet weet wat je doet, kun je de compressor beschadigen en dan ben je verder van huis. Heb je het gevoel dat de compressor goed werkt en je hebt ook de ventilator en condensor gecheckt, dan kan het probleem bij de thermostaat liggen. De thermostaat regelt de temperatuur in de vriezer en als deze niet goed functioneert, kan dat leiden tot allerlei koelingsproblemen. Soms kun je dit simpelweg testen door de thermostaat op een lagere stand te zetten en te luisteren of de compressor aanslaat. Als dat niet het geval is, dan kan er een probleem zijn met de bedrading of het relais van de compressor. Dit kan een ingewikkelde klus zijn, dus als je niet zeker bent, roep dan hulp in.\n",
      "Token Count: 1568\n",
      "Number of Sentences: 40\n",
      "----------------------------------------\n",
      "Chunk 2:\n",
      "Text: Soms kun je dit simpelweg testen door de thermostaat op een lagere stand te zetten en te luisteren of de compressor aanslaat. Als dat niet het geval is, dan kan er een probleem zijn met de bedrading of het relais van de compressor. Dit kan een ingewikkelde klus zijn, dus als je niet zeker bent, roep dan hulp in. Na het controleren en schoonmaken van alle onderdelen, kun je de vriezer weer in gebruik nemen. Het kan even duren voordat de temperatuur weer op peil is, dus geef de vriezer een paar uur de tijd om goed af te koelen. Het is belangrijk om de temperatuur regelmatig te controleren, want sommige problemen komen pas na een paar uur weer naar voren. Een thermometer kan hier goed van pas komen, zodat je precies weet of de vriezer optimaal functioneert. Laten we afsluiten. Zoals je ziet, is het repareren van een vriezer niet zomaar iets dat je in een paar minuten doet. Er zijn zoveel details waar je op moet letten, en die details maken het verschil tussen een werkende en niet-werkende vriezer. Heb je vragen? Stel ze gerust in de comments. Vergeet niet om te liken en te abonneren. Tot de volgende keer en succes met je vriezer! Bedankt dat je nog steeds kijkt! We zijn nog niet helemaal klaar, want er zijn nog wat andere problemen die kunnen optreden bij vriezers, vooral als ze ouder worden. Een van de dingen die ik nog niet heb besproken, maar die echt essentieel is, is het controleren van de deurafdichting. Dit is een aspect dat vaak wordt vergeten. Wanneer de deur van de vriezer niet goed sluit, kan er warme lucht naar binnen lekken, en dit zorgt ervoor dat de vriezer harder moet werken om af te koelen. Die extra inspanning kost niet alleen energie, maar kan ook de levensduur van de vriezer verkorten. Een eenvoudige manier om te testen of de deur goed sluit, is door een stuk papier tussen de deur en de vriezer te plaatsen en dan zachtjes aan het papier te trekken. Als het papier makkelijk loskomt, is dat een teken dat de afdichting misschien niet meer goed werkt en vervangen moet worden. Een slecht sluitende deur kan leiden tot overmatige ijsvorming in de vriezer. Overmatig ijs is een veelvoorkomend probleem en wordt vaak veroorzaakt door kleine openingen waar warme lucht binnenkomt. Dit probleem zorgt er niet alleen voor dat de vriezer minder efficiënt werkt, maar het kan ook voor vastzittende laden en deuren zorgen. Als er veel ijs is opgebouwd, is het beste wat je kunt doen de vriezer volledig ontdooien. Ja, dat betekent de stekker eruit, alles eruit halen en wachten tot het ijs smelt. Een handige tip hiervoor is om een pan met heet water in de vriezer te plaatsen en de deur te sluiten. Dit versnelt het ontdooiproces en zorgt ervoor dat je niet uren hoeft te wachten. Terwijl we het over ijsvorming hebben, laten we het ook hebben over de luchtcirculatie in de vriezer. Mensen vergeten vaak dat de manier waarop je voedsel in de vriezer plaatst, invloed heeft op de luchtstroom en daardoor op de efficiëntie. Als je voedsel helemaal tegen de achterwand van de vriezer stapelt, blokkeer je de luchtstroom, en dat kan voor temperatuurproblemen zorgen. Zorg ervoor dat er altijd wat ruimte is tussen het voedsel en de wanden van de vriezer, zodat de koude lucht vrij kan circuleren. Kleine aanpassingen in hoe je je spullen opbergt, kunnen een groot verschil maken voor de prestaties van je vriezer. Een ander aspect waar je op moet letten, is de afvoeropening. Moderne vriezers hebben vaak een kleine afvoer aan de onderkant die helpt om overtollig vocht af te voeren. Maar als die afvoer verstopt raakt, kan het vocht zich ophopen en bevriezen, wat opnieuw kan leiden tot problemen met ijsvorming. Dit kun je oplossen door de afvoer voorzichtig schoon te maken met een dun borsteltje of zelfs een stukje flexibel plastic, zoals een rietje. Het is belangrijk om dit regelmatig te doen, want een verstopte afvoer kan op de lange termijn echt voor problemen zorgen. Nu we toch bezig zijn, laten we eens kijken naar de temperatuurinstellingen van de vriezer.\n",
      "Token Count: 1526\n",
      "Number of Sentences: 40\n",
      "----------------------------------------\n",
      "Chunk 3:\n",
      "Text: Het is belangrijk om dit regelmatig te doen, want een verstopte afvoer kan op de lange termijn echt voor problemen zorgen. Nu we toch bezig zijn, laten we eens kijken naar de temperatuurinstellingen van de vriezer. Veel mensen zetten hun vriezer op de laagste stand in de veronderstelling dat dit de beste manier is om alles zo koud mogelijk te houden. Maar wist je dat een te lage temperatuur juist kan leiden tot meer ijsvorming en onnodig energieverbruik? De meeste vriezers werken het best rond de -18°C, en dat is ook de temperatuur die aanbevolen wordt voor een optimale bewaarconditie van voedsel. Door de temperatuur iets hoger in te stellen, kun je vaak al een verschil maken in de hoeveelheid ijs die zich vormt. Een andere oorzaak van problemen kan liggen bij de ventilator, die verantwoordelijk is voor de luchtcirculatie binnen de vriezer. Als de ventilator niet goed werkt, kunnen sommige delen van de vriezer te warm of te koud worden. Luister goed of je een zacht zoemend geluid hoort als de vriezer in gebruik is – dit is meestal de ventilator die draait. Als je geen geluid hoort, kan het zijn dat de ventilator defect is of vastzit door stof of ijs. In dat geval moet je de ventilator misschien vervangen of schoonmaken. Als alles is gecontroleerd, is het moment aangebroken om de vriezer aan te zetten en te kijken of alles werkt zoals het hoort. Het is belangrijk om geduldig te zijn en de vriezer enkele uren de tijd te geven om op de juiste temperatuur te komen. Veel mensen denken dat de vriezer onmiddellijk koud zal zijn, maar als je bijvoorbeeld de compressor net hebt gerepareerd of onderdelen hebt schoongemaakt, kan het even duren voordat alles weer stabiel is. En met die laatste stap zijn we aan het einde gekomen van dit uitgebreide overzicht van alles wat je kunt doen om je vriezer weer optimaal te laten functioneren. Je ziet dat er ontzettend veel kleine details bij komen kijken, dingen die je niet altijd terugvindt in een standaard handleiding. Het is die praktische kennis en ervaring die ervoor zorgt dat je het maximale uit je apparaat kunt halen zonder dat je meteen een monteur hoeft in te schakelen. Vergeet niet dat onderhoud essentieel is om de levensduur van je vriezer te verlengen en om problemen te voorkomen. Dus, heb je nog vragen? Laat ze hieronder achter in de reacties, en vergeet niet om deze video een like te geven als je iets hebt opgestoken. Deel deze tips met vrienden of familie die misschien ook wel eens tegen dit soort problemen aanlopen, en zorg dat je geabonneerd blijft voor meer handige huishoudelijke tips en tricks. Bedankt voor het kijken, en tot de volgende keer! Nog steeds hier? Geweldig, want er is altijd nog meer te vertellen over het onderhouden van je vriezer en de praktische kennis die monteurs hebben opgedaan door jarenlange ervaring. Het lijkt misschien dat je nu alles weet, maar laten we eerlijk zijn: apparaten zoals vriezers kunnen vol verrassingen zitten. Elke keer als ik aan een nieuwe klus begin, ontdek ik wel weer iets nieuws, een klein detail dat een wereld van verschil kan maken. Dus als je denkt dat we alles hebben behandeld, wacht maar even – er zijn nog een paar dingen waar je op moet letten en een paar slimme trucjes die de moeite waard zijn om te leren. Een tip die veel mensen misschien niet kennen, maar die erg nuttig is, heeft te maken met de luchtvochtigheid in de omgeving van de vriezer. Dit klinkt misschien raar, maar de lucht in de ruimte waar je vriezer staat, kan een grote invloed hebben op hoe goed je vriezer werkt. Als de luchtvochtigheid hoog is, bijvoorbeeld in een vochtige kelder of een warme keuken, zal je vriezer harder moeten werken om de temperatuur stabiel te houden. Dit kan niet alleen leiden tot meer energieverbruik, maar ook tot extra ijsvorming aan de binnenkant van de vriezer. Als je vriezer zich in een vochtige ruimte bevindt, kan het helpen om een luchtontvochtiger in de buurt te plaatsen of de ruimte goed te ventileren. Dit verlaagt de luchtvochtigheid en vermindert de kans op ijsvorming. Naast de luchtvochtigheid kan ook de locatie van de vriezer zelf een verschil maken. Plaats de vriezer bij voorkeur op een plek waar hij voldoende ruimte heeft om warmte af te voeren. Je zult het misschien niet verwachten, maar als een vriezer te dicht tegen de muur staat of ingeklemd zit in een kleine ruimte, kan de warmte niet goed weg, en dat zorgt ervoor dat de vriezer harder moet werken. Zorg ervoor dat er minstens een paar centimeter ruimte rondom de vriezer is. Dit maakt echt een verschil, vooral als je vriezer in een warme ruimte staat. En hier komt nog zo’n praktische tip die vaak over het hoofd wordt gezien: je vriezer vullen op de juiste manier. Een goed gevulde vriezer werkt efficiënter dan een halflege vriezer.\n",
      "Token Count: 1802\n",
      "Number of Sentences: 40\n",
      "----------------------------------------\n",
      "Chunk 4:\n",
      "Text: Dit maakt echt een verschil, vooral als je vriezer in een warme ruimte staat. En hier komt nog zo’n praktische tip die vaak over het hoofd wordt gezien: je vriezer vullen op de juiste manier. Een goed gevulde vriezer werkt efficiënter dan een halflege vriezer. Dat klinkt misschien vreemd, maar hoe meer items er in de vriezer zitten, hoe stabieler de temperatuur blijft. Als er weinig in de vriezer ligt, moet hij harder werken om alles koud te houden. Dus als je vriezer halfvol is, kun je overwegen om wat extra flessen water in te vriezen om de lege ruimte op te vullen. Die flessen fungeren als thermische buffers en helpen de kou beter vast te houden, vooral als de deur vaak open en dicht gaat. Een ander punt dat vaak wordt vergeten, is de vriezer een keer per jaar verplaatsen om schoon te maken en eventuele stofophoping onder en rondom het apparaat te verwijderen. Het klinkt misschien als een enorme klus, maar het is absoluut de moeite waard. Het stof dat zich ophoopt onder en rondom de vriezer kan de ventilatieopeningen blokkeren en zorgt ervoor dat de vriezer minder efficiënt werkt. Door het stof te verwijderen, kun je ervoor zorgen dat de luchtstroom rondom het apparaat optimaal blijft, wat leidt tot een betere koelprestatie en minder energieverbruik. Tot slot, en dit is iets wat ik altijd benadruk bij het onderhoud van apparaten, probeer aandacht te besteden aan de geluiden die je vriezer maakt. Elk apparaat heeft zijn eigen ‘muziek’, zeg ik altijd. Als je die geluiden goed kent, merk je het meteen als er iets niet klopt. Je weet dan meteen of er iets ongewoons is, of het nu een brom, een piep of een ander geluid is. Door vroegtijdig op deze signalen te letten, kun je problemen vaak opmerken voordat ze erger worden en dure reparaties nodig zijn. En daarmee zijn we nu echt aan het einde van deze uitgebreide gids. Hopelijk heb je nu een nog dieper inzicht gekregen in hoe je je vriezer optimaal kunt onderhouden en hoe je kleine problemen kunt herkennen en oplossen voordat ze groter worden. Zoals altijd, als je vragen hebt of als er iets niet helemaal duidelijk is, laat het me weten in de reacties. Ik ben er om je te helpen en ik beantwoord graag vragen van kijkers die hun huishoudelijke apparaten in topvorm willen houden. Dus vergeet niet om je te abonneren, deze video een like te geven, en deel hem vooral met vrienden of familie die misschien ook wel eens met hun vriezer worstelen. Bedankt dat je zo lang hebt volgehouden, en ik hoop je snel weer te zien in een volgende video vol met handige huishoudtips en trucs. Tot de volgende keer en succes met je vriezer! Fantastisch dat je er nog steeds bij bent! We kunnen nog wel even doorgaan, want als het op vriezers aankomt, is er altijd meer te leren en te delen. Elk apparaat heeft zijn eigen karakter, en door de jaren heen merk ik dat er altijd weer nieuwe inzichten bijkomen – kleine, praktische tips die je in geen enkele handleiding zult vinden. Het is echt waar dat je met elk apparaat dat je repareert, weer een stukje ervaring en kennis opdoet. En die kennis deel ik graag, want met de juiste informatie kun je zoveel zelf doen en bespaar je jezelf de kosten van een reparateur. Een ding waar we het nog niet over hebben gehad, maar wat echt belangrijk is, zijn de koelvloeistoffen in de vriezer. De meeste mensen denken dat dit iets is waar je nooit iets mee te maken krijgt, maar het kan gebeuren dat er een lek ontstaat, waardoor de koelvloeistof langzaam ontsnapt. Dit merk je vaak doordat de vriezer niet meer goed koelt, ook al werken alle andere onderdelen zoals de compressor en ventilator naar behoren. Als je vermoedt dat er een probleem is met de koelvloeistof, is het belangrijk om te weten dat dit geen DIY-klus is. Koelvloeistof vervangen of bijvullen vereist speciale apparatuur en kennis, en bovendien zijn deze vloeistoffen vaak niet goed voor het milieu. Dus mocht je ooit merken dat je vriezer niet meer koud genoeg wordt ondanks een goed werkende compressor en ventilator, laat dit dan controleren door een vakman. Een ander veelvoorkomend probleem, vooral bij oudere vriezers, is het langzaam dichtvriezen van de afvoergoten en kanalen. De meeste moderne vriezers hebben een zelfontdooiend systeem, maar dat betekent niet dat er nooit ijsophoping kan plaatsvinden. IJs kan zich in kleine hoeveelheden ophopen in ventilatiekanalen of rond de verdamper, waardoor de luchtcirculatie vermindert. Je merkt dit vaak doordat sommige delen van de vriezer kouder zijn dan andere of doordat er een ophoping van condensatie is. Wat je dan kunt doen, is de vriezer volledig uitzetten, de deur openlaten, en enkele uren wachten tot het ijs vanzelf smelt. Een paar kommen heet water helpen dit proces te versnellen, en door af en toe met een zachte doek het gesmolten ijs op te nemen, voorkom je dat het opnieuw bevriest.\n",
      "Token Count: 1854\n",
      "Number of Sentences: 40\n",
      "----------------------------------------\n",
      "Chunk 5:\n",
      "Text: Wat je dan kunt doen, is de vriezer volledig uitzetten, de deur openlaten, en enkele uren wachten tot het ijs vanzelf smelt. Een paar kommen heet water helpen dit proces te versnellen, en door af en toe met een zachte doek het gesmolten ijs op te nemen, voorkom je dat het opnieuw bevriest. Nog een laatste tip die vaak vergeten wordt: de juiste vriezer voor jouw situatie kiezen. Dit lijkt misschien voor de hand liggend, maar niet elke vriezer is geschikt voor elke locatie. Als je bijvoorbeeld een vriezer in de garage hebt staan, waar het in de winter erg koud kan worden, zorg er dan voor dat de vriezer ook geschikt is voor lagere omgevingstemperaturen. Sommige modellen zijn ontworpen om te werken bij temperaturen tot wel -15°C, terwijl andere apparaten bij lagere temperaturen kunnen stoppen met functioneren. Dit is iets wat veel mensen zich niet realiseren, maar het kan echt een groot verschil maken in de levensduur en werking van je vriezer. Controleer altijd de specificaties van het apparaat om te zien of het geschikt is voor de omgeving waarin je het wilt plaatsen. Goed, we zijn nu echt bijna aan het einde, maar laat me toch nog één ding benadrukken: het belang van consistent onderhoud. Net zoals met een auto die je regelmatig een beurt geeft, kan een vriezer ook profiteren van een jaarlijkse check. Het hoeft niet ingewikkeld te zijn – gewoon een snelle inspectie van de ventilator, condensor, en deurafdichting kan al veel problemen voorkomen. Door regelmatig onderhoud te plegen, verleng je de levensduur van het apparaat aanzienlijk en voorkom je dat kleine problemen uitgroeien tot grote, dure reparaties. En nu, beste kijkers, zijn we echt aan het einde gekomen van deze uitgebreide deep dive in de wereld van vriezerreparaties en onderhoud. Het is ontzettend leuk om zoveel kennis en tips met jullie te delen, en ik hoop dat je nu meer vertrouwen hebt om je vriezer zelf te onderhouden en kleine problemen aan te pakken. We hebben zoveel aspecten besproken, van veiligheid en basisstappen tot meer geavanceerde problemen en technieken. Dus, vergeet niet: goed luisteren, goed kijken, en geduldig zijn – dat zijn echt de sleutels tot succesvol vriezeronderhoud. Heb je nog vragen of wil je dat ik een specifiek probleem verder uitdiep? Laat het me dan weten in de reacties hieronder. Deel deze video met iedereen die een vriezer heeft en net dat beetje extra kennis kan gebruiken om het meeste uit hun apparaat te halen. Bedankt voor het kijken en voor je geduld, en vergeet niet om je te abonneren voor meer handige tips en tutorials. Tot de volgende keer en heel veel succes met je vriezer! Natuurlijk, laten we diep ingaan op de details en echt uitgebreid bespreken hoe je je vriezer in topconditie houdt. Zoals ik al zei, een vriezer lijkt misschien een simpel apparaat, maar er gaat zoveel techniek en subtiliteit schuil achter de werking ervan. Hoe beter je begrijpt wat er allemaal gebeurt binnenin zo’n machine, hoe meer controle je hebt over het onderhoud en de reparaties. Het is net alsof je de innerlijke werking van een klein ecosysteem leert kennen, waarin elk onderdeel zijn eigen rol heeft en bijdraagt aan de algehele balans en efficiëntie. Dus als je nog even wilt blijven, laten we echt inzoomen op de technische kant, want ik beloof je, hoe meer je weet, hoe beter je je vriezer kunt onderhouden. Laten we beginnen met het begrip van hoe koelvloeistoffen werken en waarom dit zo belangrijk is. Veel mensen weten wel dat een vriezer gebruik maakt van een koelmiddel om te functioneren, maar wat dit precies doet en hoe het systeem werkt, is vaak minder bekend. De koelvloeistof circuleert door een gesloten systeem en verandert constant van gas naar vloeistof en weer terug. In de compressor wordt het koelmiddel samengeperst, wat warmte genereert. Deze warmte moet worden afgevoerd, en dat gebeurt via de condensor aan de achterkant van de vriezer. Als je ooit hebt gevoeld dat de achterkant van je vriezer warm is, dan komt dat doordat hier de warmte wordt afgevoerd. Daarna stroomt het koelmiddel door een expansieklep, waarbij het snel afkoelt en verdampt. Dit verdampingsproces onttrekt warmte aan de binnenkant van de vriezer, waardoor de temperatuur daalt. Dit proces is als het ware de \"ruggengraat\" van de werking van je vriezer, en het begrijpen hiervan kan je helpen om eventuele problemen beter te begrijpen. Nu we het toch hebben over de koelvloeistoffen, laten we het hebben over wat er gebeurt als er een lek in het systeem ontstaat. Dit kan een geleidelijk proces zijn; vaak merk je het pas na een tijdje doordat de vriezer steeds minder goed begint te koelen. Een lek in het koelmiddelcircuit kan ontstaan door slijtage, maar ook door corrosie of zelfs een lichte beschadiging aan de leidingen. Het probleem met koelmiddel is dat het niet zomaar bijgevuld kan worden door een doe-het-zelver, omdat het onder hoge druk staat en specifieke apparatuur vereist voor het veilig afvullen en controleren van de druk. Ook zijn de meeste koelmiddelen schadelijk voor het milieu, dus het is belangrijk dat dit zorgvuldig en professioneel wordt afgehandeld.\n",
      "Token Count: 1939\n",
      "Number of Sentences: 40\n",
      "----------------------------------------\n",
      "Chunk 6:\n",
      "Text: Ook zijn de meeste koelmiddelen schadelijk voor het milieu, dus het is belangrijk dat dit zorgvuldig en professioneel wordt afgehandeld. Het is dus raadzaam om altijd een professional in te schakelen als je vermoedt dat er een lek is in het koelmiddelcircuit. Naast de koelvloeistoffen is een ander belangrijk aspect het onderhoud van de condensor. Deze spoelachtige structuur aan de achterkant van de vriezer is verantwoordelijk voor het afvoeren van de warmte die tijdens het compressieproces wordt gegenereerd. Omdat de condensor vaak achterin de vriezer zit, is het gemakkelijk te vergeten om deze schoon te maken. Stof en vuil kunnen zich ophopen tussen de spoelen en dit belemmert de luchtstroom, wat de efficiëntie van de vriezer drastisch vermindert. Regelmatig schoonmaken, bijvoorbeeld met een zachte borstel of een stofzuiger, kan wonderen doen voor de werking van je vriezer. Wanneer de condensor verstopt raakt, moet de compressor harder werken om dezelfde hoeveelheid warmte af te voeren, wat resulteert in een hoger energieverbruik en een kortere levensduur van de compressor. En dan hebben we de ventilator, een essentieel onderdeel dat vaak over het hoofd wordt gezien. De ventilator bevindt zich meestal achter een beschermkap en helpt bij de circulatie van koude lucht door de vriezer. Zonder een goed functionerende ventilator kan de lucht niet goed circuleren, en dit kan leiden tot temperatuurverschillen in de vriezer. Sommige delen kunnen te warm worden, terwijl andere delen juist te koud zijn, waardoor je geen uniforme temperatuur hebt. De ventilator kan vastlopen door stof, vuil, of zelfs door een dunne laag ijs die zich ophoopt. Dit is iets wat vaak gebeurt in oudere vriezers of in vriezers die dicht tegen een muur of ander object staan, waardoor de luchtcirculatie beperkt wordt. Wanneer de ventilator niet draait, is het belangrijk om te kijken of er vuil of ijs zit dat het draaien belemmert. Gebruik nooit scherpe voorwerpen om dit weg te halen; wees voorzichtig en gebruik een doek of zachte borstel. Soms helpt het om de vriezer enkele uren uit te zetten en het ijs op natuurlijke wijze te laten smelten, zodat de ventilator weer vrij kan bewegen. Daarnaast is het controleren van de deurafdichting iets waar veel mensen vaak niet bij stilstaan, maar wat echt een groot verschil kan maken. De rubberen afdichting rondom de deur van de vriezer zorgt ervoor dat er geen warme lucht naar binnen komt en dat de koude lucht binnen blijft. Een beschadigde of versleten afdichting betekent dat er voortdurend warme lucht binnenkomt, wat de compressor dwingt om harder te werken om de vriezer koud te houden. Dit verhoogt niet alleen het energieverbruik, maar zorgt er ook voor dat de vriezer sneller ijsvorming ontwikkelt. Een simpele manier om de afdichting te testen is door een papiertest te doen: plaats een stuk papier tussen de deur en de vriezer en sluit de deur. Als je het papier gemakkelijk kunt uittrekken, betekent dit dat de afdichting mogelijk versleten is en aan vervanging toe is. Een nieuwe afdichting kan eenvoudig worden geplaatst en helpt enorm bij het behouden van een stabiele temperatuur in de vriezer. En laten we het ook hebben over de interne organisatie van de vriezer. Hoe je je voedsel plaatst, heeft ook invloed op hoe efficiënt de vriezer werkt. Veel mensen stapelen hun vriezer vol zonder rekening te houden met de luchtcirculatie. Probeer altijd een paar centimeter ruimte vrij te houden aan de achterkant van de vriezer, zodat de lucht vrij kan circuleren. Ook kun je overwegen om voedselgroepen bij elkaar te plaatsen, zoals groenten aan de ene kant en vlees aan de andere kant. Hierdoor kun je efficiënter gebruik maken van de vriezer en hoef je de deur minder lang open te houden omdat je snel kunt vinden wat je zoekt. Een goed georganiseerde vriezer zorgt voor een betere luchtcirculatie en helpt de temperatuur stabiel te houden. Als we kijken naar de thermometer en de temperatuurinstelling, is dit ook een belangrijk aspect van het onderhoud. Veel mensen zetten hun vriezer op de laagste temperatuur, in de veronderstelling dat dit beter is voor het voedsel, maar dat is niet altijd het gev(?:Mr|Mrs|Dr|Prof|Sr|Jr|vs|etc|viz|al|Gen|Col|Fig|e\\.g|i\\.e)\\.  Een vriezer werkt het meest efficiënt rond de -18°C. Lagere temperaturen verhogen het energieverbruik zonder echt voordeel voor het voedsel op te leveren. Bovendien kan een te lage temperatuur leiden tot overmatige ijsvorming. Het is ook een goed idee om af en toe de interne temperatuur te controleren met een thermometer, vooral als je vermoedt dat de vriezer niet meer goed werkt. Hiermee kun je snel afwijkingen detecteren en actie ondernemen voordat het probleem erger wordt. Tot slot wil ik nog even de nadruk leggen op regelmatig onderhoud en inspectie. Net als bij een auto of een ander complex apparaat, kan een beetje preventief onderhoud een lange weg gaan. Door jaarlijks de ventilator, condensor, en deurafdichting te controleren, voorkom je dat kleine problemen grote, dure reparaties worden.\n",
      "Token Count: 1920\n",
      "Number of Sentences: 40\n",
      "----------------------------------------\n",
      "Chunk 7:\n",
      "Text: Tot slot wil ik nog even de nadruk leggen op regelmatig onderhoud en inspectie. Net als bij een auto of een ander complex apparaat, kan een beetje preventief onderhoud een lange weg gaan. Door jaarlijks de ventilator, condensor, en deurafdichting te controleren, voorkom je dat kleine problemen grote, dure reparaties worden. Vriezers gaan over het algemeen lang mee, maar dat betekent niet dat je ze kunt vergeten zodra je ze hebt geïnstalleerd. Een jaarlijkse schoonmaak en inspectie kan de levensduur van je vriezer aanzienlijk verlengen en zorgt ervoor dat hij optimaal blijft presteren. Dus, beste kijkers, ik hoop dat je nu echt een diep inzicht hebt in wat er allemaal komt kijken bij het onderhoud van een vriezer. Dit gaat verder dan alleen een apparaat in je keuken – het is een investering in duurzaamheid, efficiëntie, en besparing. Het geeft een goed gevoel om te weten dat je zelf de controle hebt over de gezondheid van je apparaat en dat je met de juiste kennis in staat bent om veel voorkomende problemen zelf op te lossen. Zoals altijd, als je nog vragen hebt, laat het me weten in de reacties hieronder. En als je vrienden of familie hebt die ook worstelen met hun vriezer, deel deze video dan met hen. Bedankt dat je helemaal tot het einde hebt gekeken, vergeet niet om te liken en je te abonneren, en tot de volgende keer met meer handige tips en trucs voor al je huishoudelijke apparaten! Natuurlijk, we duiken nog dieper in de details en nemen de tijd om alles uitgebreid te behandelen, zodat je echt een volledig beeld krijgt van alles wat bij het onderhoud van een vriezer komt kijken. Het is bijna alsof je de geheimen van het apparaat leert kennen, elke schroef, elke knop, elk klein geluid dat het maakt – het zijn allemaal signalen die je inzicht geven in de toestand van je vriezer en hoe je deze het beste kunt onderhouden. Laten we er dus verder op ingaan, zodat je de kennis hebt om niet alleen problemen op te lossen, maar ook om preventief in actie te komen en je vriezer in topconditie te houden. Om te beginnen, laten we het koelproces nog verder in detail verkennen, want een diep begrip van dit proces kan je helpen om de signalen van je vriezer beter te interpreteren. De koelcyclus in een vriezer draait volledig om het manipuleren van warmte, waarbij warmte onttrokken wordt aan de binnenruimte en naar buiten wordt afgevoerd. Dit proces begint met het koelmiddel dat in de compressor onder hoge druk wordt samengeperst, waardoor de temperatuur van het koelmiddel stijgt. Dit warme gas stroomt vervolgens naar de condensor, meestal achteraan of onderaan de vriezer, waar het warmte afgeeft aan de omgeving en afkoelt. Terwijl het afkoelt, condenseert het gas en wordt het vloeibaar. Dit vloeibare koelmiddel wordt dan door een expansieklep geperst, waar de druk plotseling daalt en het koelmiddel snel verdampt, waardoor het warmte onttrekt aan de binnenruimte van de vriezer. Deze cyclus herhaalt zich continu om een constante, koude omgeving te creëren. Wat maakt dit proces zo belangrijk voor het onderhoud van je vriezer? Nou, zodra er ergens in dit proces iets misgaat – of het nu de compressor is die niet voldoende druk opbouwt, een verstopte condensor die de warmte niet goed kan afvoeren, of een lekkage in het koelmiddelcircuit – zal het hele systeem minder efficiënt werken, wat je al snel merkt aan de koeling in de vriezer. Dit betekent dat je meer aandacht moet besteden aan de signalen die het apparaat je geeft. Hoor je bijvoorbeeld dat de compressor langere tijd aan blijft zonder uit te schakelen? Of merk je dat de condensor abnormaal heet aanvoelt? Dit soort signalen kunnen wijzen op een probleem dat je aandacht vereist. Laten we dan nu eens dieper ingaan op de condensor zelf. Dit onderdeel, vaak achterin of onderaan de vriezer gemonteerd, is essentieel voor het afvoeren van warmte uit het koelmiddel. Het zijn vaak de dunne, buisvormige spoelen of platen die warmte uitstralen, en ze kunnen snel bedekt raken met stof en vuil. Dit kan gebeuren zonder dat je het doorhebt, vooral als de vriezer tegen een muur aanstaat. Stofvorming is misschien niet direct zichtbaar, maar na verloop van tijd vormt zich een laag die de warmteafvoer belemmert. Hierdoor moet de compressor harder werken om dezelfde koelprestaties te leveren, wat leidt tot hogere energiekosten en oververhitting van de compressor. Dit zijn zaken die de levensduur van je vriezer aanzienlijk kunnen verkorten. Hoe maak je de condensor dan op de juiste manier schoon? Dit is waar een beetje kennis en geduld om de hoek komen kijken. Eerst zorg je ervoor dat de vriezer volledig uitgeschakeld is en van de stroom is losgekoppeld, voor je veiligheid. Vervolgens kun je een zachte borstel of een stofzuiger met een smalle mondstuk gebruiken om het stof voorzichtig te verwijderen. Het is cruciaal om voorzichtig te zijn, want de spoelen van de condensor kunnen kwetsbaar zijn. Duw niet te hard en vermijd het gebruik van vochtige doeken, want water kan de metalen onderdelen beschadigen en mogelijk roest veroorzaken.\n",
      "Token Count: 1893\n",
      "Number of Sentences: 40\n",
      "----------------------------------------\n",
      "Chunk 8:\n",
      "Text: Het is cruciaal om voorzichtig te zijn, want de spoelen van de condensor kunnen kwetsbaar zijn. Duw niet te hard en vermijd het gebruik van vochtige doeken, want water kan de metalen onderdelen beschadigen en mogelijk roest veroorzaken. Neem de tijd om elke hoek te inspecteren en zorg ervoor dat je alle stof en vuil verwijdert, vooral als je merkt dat de spoelen moeilijk te bereiken zijn. Naast de condensor is de ventilator een ander belangrijk onderdeel in het hele koelproces van de vriezer. De ventilator zorgt ervoor dat de koude lucht goed door de hele vriezer circuleert, zodat elk hoekje dezelfde temperatuur heeft. Zonder een goed functionerende ventilator kun je merken dat sommige delen van de vriezer warmer zijn dan andere, wat kan leiden tot inconsistente koeling en uiteindelijk tot voedsel dat niet goed bewaard blijft. De ventilator kan vastlopen door stofophoping of door ijsvorming, vooral als de vriezer intensief wordt gebruikt of als er vaak vochtige producten worden opgeslagen. Dit kan zich langzaam opbouwen en uiteindelijk de draaiende beweging van de ventilator verstoren. Het schoonmaken van de ventilator vereist precisie en geduld, want als er ijsophoping is, kan dit niet zomaar worden weggekrabd. Begin met het uitschakelen van de vriezer en geef de ventilator de tijd om te ontdooien, vooral als je vermoedt dat er ijsophoping is. Je kunt eventueel een pan met heet water in de vriezer plaatsen om het proces te versnellen, maar zorg ervoor dat je het apparaat niet overstroomt. Gebruik nooit harde of scherpe voorwerpen om het ijs te verwijderen, want je kunt de ventilator en andere onderdelen beschadigen. Wanneer het ijs gesmolten is, kun je een zachte doek gebruiken om overtollig water weg te vegen en de ventilator voorzichtig schoon te maken. Dan komen we bij de rubberen deurafdichting, een onderdeel dat onmisbaar is voor het efficiënt functioneren van de vriezer, maar waar vaak weinig aandacht aan wordt besteed. De afdichting zorgt ervoor dat de deur goed sluit en dat er geen warme lucht naar binnen kan komen. Een beschadigde of versleten afdichting zorgt ervoor dat de compressor veel harder moet werken om de interne temperatuur te handhaven, wat niet alleen tot overmatige ijsvorming kan leiden, maar ook tot onnodig hoge energiekosten. Een eenvoudige manier om de afdichting te testen, is de ‘papiertest’. Plaats een stuk papier tussen de deur en de vriezer, sluit de deur, en trek voorzichtig aan het papier. Als het papier makkelijk loskomt, is de afdichting mogelijk versleten en kan deze beter vervangen worden. Het vervangen van een deurafdichting is geen moeilijke klus, maar je moet wel weten waar je op moet letten. Zorg ervoor dat de nieuwe afdichting precies past bij het model van je vriezer, want kleine verschillen in grootte kunnen invloed hebben op de effectiviteit ervan. De meeste afdichtingen zijn eenvoudig te installeren door ze in de groeven van de deur te plaatsen. Controleer na het installeren van de nieuwe afdichting of de deur goed sluit en of er geen luchtlekkage is. Dit kan echt een groot verschil maken in de efficiëntie van de vriezer. Laten we ook niet vergeten dat de vriezer zelf een systeem is dat afhankelijk is van een juiste balans van temperatuur en luchtvochtigheid. Dit betekent dat de omgeving waarin de vriezer zich bevindt, ook invloed kan hebben op zijn prestaties. Als de vriezer zich in een vochtige of warme ruimte bevindt, moet hij harder werken om de temperatuur constant te houden. Een luchtontvochtiger kan helpen om de vochtigheid te reguleren, wat de vriezer helpt om zijn taak efficiënter uit te voeren. Dit is een stap die veel mensen niet overwegen, maar die aanzienlijke voordelen kan opleveren, vooral in ruimtes zoals garages of vochtige kelders. En als laatste, maar zeker niet minder belangrijk: de organisatie binnenin de vriezer. Hoe je voedsel opslaat, kan invloed hebben op hoe efficiënt je vriezer werkt. Stapel je alles op een hoop of plaats je producten op een manier die de luchtstroom niet belemmert? Als je de producten dicht tegen de achterkant van de vriezer plaatst, blokkeer je mogelijk de luchtstroom, wat de efficiëntie van de koeling vermindert. Zorg voor voldoende ruimte tussen de producten en geef de koude lucht de ruimte om vrij door de vriezer te circuleren. Probeer producten ook te groeperen, zodat je snel kunt vinden wat je zoekt en de deur zo kort mogelijk openhoudt. Dit lijkt misschien een klein detail, maar het kan een groot verschil maken in het behouden van een stabiele temperatuur. Met al deze stappen en inzichten in gedachten, heb je nu een complete handleiding voor het optimaal onderhouden van je vriezer. Het is echt de moeite waard om wat tijd te investeren in deze details, want een goed onderhouden vriezer gaat langer mee, werkt efficiënter en bespaart je geld op de lange termijn. En vergeet niet: het draait allemaal om geduld en aandacht voor detail. Elk klein aspect dat je optimaliseert, draagt bij aan de algehele gezondheid van het apparaat.\n",
      "Token Count: 1866\n",
      "Number of Sentences: 40\n",
      "----------------------------------------\n",
      "Chunk 9:\n",
      "Text: Het is echt de moeite waard om wat tijd te investeren in deze details, want een goed onderhouden vriezer gaat langer mee, werkt efficiënter en bespaart je geld op de lange termijn. En vergeet niet: het draait allemaal om geduld en aandacht voor detail. Elk klein aspect dat je optimaliseert, draagt bij aan de algehele gezondheid van het apparaat. Dus, beste kijkers, heb je nog vragen of zijn er dingen die je verder wilt weten? Laat het me weten in de reacties! Deel deze uitgebreide gids met vrienden of familie die ook het beste uit hun vriezer willen halen, en vergeet niet om de video een like te geven en je te abonneren voor nog meer handige huishoudelijke tips. Bedankt voor het kijken, en veel succes met je vriezer!\n",
      "Token Count: 271\n",
      "Number of Sentences: 7\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import the SemanticChunker from chonkie\n",
    "from chonkie import SentenceChunker\n",
    "\n",
    "chunker = SentenceChunker(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=128,\n",
    "    min_sentences_per_chunk=40 #20 voor kort/ 40 voor medium/ 80 voor lang\n",
    ")\n",
    "\n",
    "# Read the contents of your input file\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Chunk the text using the chunker\n",
    "chunks = chunker.chunk(text)\n",
    "\n",
    "# Iterate through the resulting chunks and print details\n",
    "for i, chunk in enumerate(chunks, start=1):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(f\"Text: {chunk.text}\")\n",
    "    print(f\"Token Count: {chunk.token_count}\")\n",
    "    print(f\"Number of Sentences: {len(chunk.sentences)}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chonkie[semantic] in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (0.1.2)\n",
      "Requirement already satisfied: autotiktokenizer in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chonkie[semantic]) (0.2.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chonkie[semantic]) (0.20.1)\n",
      "Requirement already satisfied: tiktoken>=0.2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chonkie[semantic]) (0.8.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chonkie[semantic]) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chonkie[semantic]) (2.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers>=2.0.0->chonkie[semantic]) (4.45.2)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers>=2.0.0->chonkie[semantic]) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers>=2.0.0->chonkie[semantic]) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers>=2.0.0->chonkie[semantic]) (1.5.2)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers>=2.0.0->chonkie[semantic]) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers>=2.0.0->chonkie[semantic]) (0.25.2)\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sentence-transformers>=2.0.0->chonkie[semantic]) (10.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from tiktoken>=0.2.0->chonkie[semantic]) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from tiktoken>=0.2.0->chonkie[semantic]) (2.32.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.0.0->chonkie[semantic]) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.0.0->chonkie[semantic]) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.0.0->chonkie[semantic]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.0.0->chonkie[semantic]) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.0.0->chonkie[semantic]) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.2.0->chonkie[semantic]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.2.0->chonkie[semantic]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.2.0->chonkie[semantic]) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.2.0->chonkie[semantic]) (2024.12.14)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.0.0->chonkie[semantic]) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.0.0->chonkie[semantic]) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.0.0->chonkie[semantic]) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.0.0->chonkie[semantic]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.0.0->chonkie[semantic]) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.0.0->chonkie[semantic]) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.0.0->chonkie[semantic]) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.0.0->chonkie[semantic]) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.0.0->chonkie[semantic]) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"chonkie[semantic]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/abetlen/llama-cpp-python.git@refs/pull/1901/head\n",
      "  Cloning https://github.com/abetlen/llama-cpp-python.git (to revision refs/pull/1901/head) to /private/var/folders/76/kx23hgz16fdcd_cjrpy0jnlm0000gn/T/pip-req-build-rsap2wd5\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/abetlen/llama-cpp-python.git /private/var/folders/76/kx23hgz16fdcd_cjrpy0jnlm0000gn/T/pip-req-build-rsap2wd5\n",
      "\u001b[33m  WARNING: Did not find branch or tag 'refs/pull/1901/head', assuming revision or ref.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running command git fetch -q https://github.com/abetlen/llama-cpp-python.git refs/pull/1901/head\n",
      "  Running command git checkout -q 114b76b941e9ce5c64c4ee38c057dbb9295f0213\n",
      "  Resolved https://github.com/abetlen/llama-cpp-python.git to commit 114b76b941e9ce5c64c4ee38c057dbb9295f0213\n",
      "  Running command git submodule update --init --recursive -q\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama_cpp_python==0.3.6)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.20.0 (from llama_cpp_python==0.3.6)\n",
      "  Downloading numpy-2.2.2-cp310-cp310-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama_cpp_python==0.3.6)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting jinja2>=2.11.3 (from llama_cpp_python==0.3.6)\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama_cpp_python==0.3.6)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Downloading numpy-2.2.2-cp310-cp310-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-macosx_11_0_arm64.whl (12 kB)\n",
      "Building wheels for collected packages: llama_cpp_python\n",
      "  Building wheel for llama_cpp_python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama_cpp_python: filename=llama_cpp_python-0.3.6-cp310-cp310-macosx_15_0_arm64.whl size=3964779 sha256=e13d32f8b3c4870956abd91a026c6e87cc4dfc860dd2b71615658478c8e1320c\n",
      "  Stored in directory: /private/var/folders/76/kx23hgz16fdcd_cjrpy0jnlm0000gn/T/pip-ephem-wheel-cache-pd4xfdg_/wheels/4d/35/be/12fa02b84af92bd829fdf1cbba9dca9d0f64ae8ea48262ff5b\n",
      "Successfully built llama_cpp_python\n",
      "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama_cpp_python\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "  Attempting uninstall: diskcache\n",
      "    Found existing installation: diskcache 5.6.3\n",
      "    Uninstalling diskcache-5.6.3:\n",
      "      Successfully uninstalled diskcache-5.6.3\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.5\n",
      "    Uninstalling Jinja2-3.1.5:\n",
      "      Successfully uninstalled Jinja2-3.1.5\n",
      "  Attempting uninstall: llama_cpp_python\n",
      "    Found existing installation: llama_cpp_python 0.3.6\n",
      "    Uninstalling llama_cpp_python-0.3.6:\n",
      "      Successfully uninstalled llama_cpp_python-0.3.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sam-2 1.0 requires torch>=2.5.1, but you have torch 2.0.0 which is incompatible.\n",
      "sam-2 1.0 requires torchvision>=0.20.1, but you have torchvision 0.15.1 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 diskcache-5.6.3 jinja2-3.1.5 llama_cpp_python-0.3.6 numpy-2.2.2 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DGGML_METAL=on\" FORCE_CMAKE=1 python3 -m pip install \"git+https://github.com/abetlen/llama-cpp-python.git@refs/pull/1901/head\" --force-reinstall --upgrade --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "llama_model_load_from_file_impl: using device Metal (Apple M3 Max) - 27647 MiB free\n",
      "llama_model_loader: loaded meta data with 41 key-value pairs and 339 tensors from models/Qwen2.5-7B-Instruct-Uncensored.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct Uncensored\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct-Uncensored\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   6:                            general.license str              = gpl-3.0\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Qwen2.5 7B Instruct\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Qwen\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7...\n",
      "llama_model_loader: - kv  11:                               general.tags arr[str,3]       = [\"qwen\", \"uncensored\", \"text-generati...\n",
      "llama_model_loader: - kv  12:                          general.languages arr[str,2]       = [\"zh\", \"en\"]\n",
      "llama_model_loader: - kv  13:                           general.datasets arr[str,5]       = [\"NobodyExistsOnTheInternet/ToxicQAFi...\n",
      "llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584\n",
      "llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944\n",
      "llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28\n",
      "llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  22:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {% set system_message = 'You are a he...\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  34:                                general.url str              = https://huggingface.co/mradermacher/Q...\n",
      "llama_model_loader: - kv  35:              mradermacher.quantize_version str              = 2\n",
      "llama_model_loader: - kv  36:                  mradermacher.quantized_by str              = mradermacher\n",
      "llama_model_loader: - kv  37:                  mradermacher.quantized_at str              = 2024-10-11T20:15:47+02:00\n",
      "llama_model_loader: - kv  38:                  mradermacher.quantized_on str              = db3\n",
      "llama_model_loader: - kv  39:                         general.source.url str              = https://huggingface.co/Orion-zhen/Qwe...\n",
      "llama_model_loader: - kv  40:                  mradermacher.convert_type str              = hf\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type q4_K:  169 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.36 GiB (4.91 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 3584\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 28\n",
      "print_info: n_head_kv        = 4\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 7\n",
      "print_info: n_embd_k_gqa     = 512\n",
      "print_info: n_embd_v_gqa     = 512\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 18944\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.62 B\n",
      "print_info: general.name     = Qwen2.5 7B Instruct Uncensored\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 152064\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 148848 'ÄĬ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 338 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/29 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  4460.45 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 28991.03 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x17fe061a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x17fe36b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x17fe39c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x17dc6dbe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x17dc6e770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x17dc6d3d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x17dc6f580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x17dc6ffd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x17dc70700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x17dc709c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x17fa99a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x137f2fd70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x137f30700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x1356849c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x135685630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x17fa9cfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x17fe38550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x135685da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x135686660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x135687d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x1356885b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x135688ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x16951cf60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x135689b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x135689f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x13568b8f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13568a340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x17fe3a420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x17fe390c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x17fa9e3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x17fa9eaa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x17fe3b0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x1691c0790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11157bf40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x169311eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x169317a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x17fa9d360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x17fe3bdd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x17fe3c4f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x17fe3cca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x17dc6fb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x17fe3d0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x17fa9f8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x17dc71c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x17dc716f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x17dc70eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x17faa03e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x17faa1300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x17fe3dc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x17fe3e190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x17fe3e8b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x17fe3efb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x17fe3f6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x17faa09a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x17faa1d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x17faa2450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x17faa2b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x17faa31d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x17faa3980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x17faa3ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x17dc72b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x17dc72f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x17dc73b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x17dc74300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13568ad90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x17faa4180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x17faa4440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x17fe400c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x17fe3f270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x17fe40b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13568c5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13568bd80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x17fe414b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13568d590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x17fe42060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13568da10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x17faa4ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x17fe42770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x17faa5710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x17fe42ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x17faa6430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x17faa5ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x17fe42a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13568d150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13568e9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x17fe43e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x17dc732f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x17faa71f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x17fe44130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x17faa7970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13568e140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x17fe44ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x17faa81e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13568e400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x17faa8db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x17dc74fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1356909d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x135690480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x17faa6a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x17faa90a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x17faaa150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x17dc746f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x135691850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x17fe44860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x17faa9c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x17fe45090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x135690e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x17faaad80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x135691cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x17faab7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x135691300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137f2f500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x17dc75da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x17fe45f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x17faaa470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x135692f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x17fe46790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x17fe455c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x17faac210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137f2ed80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x17fe46ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x17faacbc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x17faacf10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x17faada50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x17faae210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x17faae9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x17faaf0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x135692ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x17faaf870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137f31d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x135693e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x17fe47190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x17fe47ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x169317d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x17fe480b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137f30c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137f32d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x17fe48920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x17faafd60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x17fab08c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x135694480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x17dc761d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x17dc76580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x17dc775b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x17dc77c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x17fe49a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137f335e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x17fe4a0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x169317fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x16951d330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x17fe4a500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x17fe4ad50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x135695350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x17fab0020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x135695760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x17fab1590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x17fab1d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x17fe4b410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x17fe4bc40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x135694ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x17fab0ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x17fab24e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x17fab2d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x135696bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x17dc780d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x17fab3c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x135697340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137f33fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x17fe4c310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137f33a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x135697c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137f34e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x17fab4570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x17fd618b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x169318280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x169318540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x16951d840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x16951db00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x17fd61b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x17fd61fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x17fd62430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x16951e060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x17fd62c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1691b3ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x16951e320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x169318800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x169318ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x16951e5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x16951e8a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x16951eb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x16951f8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x16951ffc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1695216a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x17fd63430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x169318d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x169319040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x169319330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1693197c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x169520f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x169520b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x169521260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1695220c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x17fe4c5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x17fd636f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x17fd639b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x16931a2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x169522560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x16931bc90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x169523640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x169522820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1691bc290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x169523d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x169524a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x17fd63c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x169525440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1695257c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x17fd64a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x16931c3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x111592060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1691b6fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x16931cc90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x16931d640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x17fd64dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x169526310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1695266a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x16931d9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x169527070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x17fd66350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1695278a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x16931ddc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x169528620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x16931e260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x17fd66770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x17dfedd50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x16931f5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x17fd67350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x169528ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1691bee80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1691bf760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x17fab4c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x16931e600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1695293c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x17fd676e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x17fd67db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x17dfee010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x110768ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1693205f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x17fd68960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x169529750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x169529a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x17fd683b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x169529cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x17fd68cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x16952a770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x17dc78390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x17fab4f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x135698380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1107695c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1691e69a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x17fd69aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1691e6c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1691e6f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x16952a090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x1691f7300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x1691f75c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x1691f7880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x17fd69ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x17fd6a7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x17fd6b0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x169320c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x17dfee2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x17fab5210 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init:        CPU KV buffer size =   112.00 MiB\n",
      "llama_init_from_model: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   304.00 MiB\n",
      "llama_init_from_model: graph nodes  = 986\n",
      "llama_init_from_model: graph splits = 450 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/Orion-zhen/Qwen2.5-7B-Instruct-Uncensored', 'mradermacher.quantized_at': '2024-10-11T20:15:47+02:00', 'mradermacher.quantize_version': '2', 'general.url': 'https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF', 'tokenizer.chat_template': \"{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% endif %}{% if system_message is defined %}{{ '<|im_start|>system\\n' + system_message + '<|im_end|>\\n' }}{% endif %}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + content + '<|im_end|>\\n<|im_start|>assistant\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\n' }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.basename': 'Qwen2.5', 'qwen2.attention.head_count_kv': '4', 'general.size_label': '7B', 'general.base_model.0.name': 'Qwen2.5 7B Instruct', 'qwen2.embedding_length': '3584', 'qwen2.context_length': '32768', 'qwen2.block_count': '28', 'general.base_model.0.organization': 'Qwen', 'tokenizer.ggml.pre': 'qwen2', 'general.base_model.count': '1', 'qwen2.rope.freq_base': '1000000.000000', 'general.quantization_version': '2', 'general.license': 'gpl-3.0', 'general.base_model.0.repo_url': 'https://huggingface.co/Qwen/Qwen2.5-7B-Instruct', 'general.file_type': '15', 'general.finetune': 'Instruct-Uncensored', 'general.name': 'Qwen2.5 7B Instruct Uncensored', 'qwen2.feed_forward_length': '18944', 'mradermacher.quantized_by': 'mradermacher', 'general.architecture': 'qwen2', 'mradermacher.quantized_on': 'db3', 'qwen2.attention.head_count': '28', 'tokenizer.ggml.bos_token_id': '151643', 'general.type': 'model', 'tokenizer.ggml.model': 'gpt2'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 1 (Token count: 886, Sentences: 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    9224.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   945 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /  1102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   57398.60 ms /  2047 tokens\n",
      "Llama.generate: 96 prefix-match hit, remaining 891 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 2 (Token count: 936, Sentences: 40)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 199\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknowledge.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for the extracted valuable knowledge.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 199\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 190\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Token count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;241m.\u001b[39mtoken_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Sentences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunk\u001b[38;5;241m.\u001b[39msentences)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Extract valuable knowledge from the chunk text\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m extracted_knowledge \u001b[38;5;241m=\u001b[39m \u001b[43mchatbot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_valuable_knowledge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Save extracted knowledge (if any) to knowledge.json and update FAISS index\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extracted_knowledge:\n",
      "Cell \u001b[0;32mIn[1], line 62\u001b[0m, in \u001b[0;36mChatbot.extract_valuable_knowledge\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_valuable_knowledge\u001b[39m(\u001b[38;5;28mself\u001b[39m, message):\n\u001b[0;32m---> 62\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a knowledge extractor. Try to Extract any knowledge from the user.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReturn ONLY JSON with the following schema:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43mvaluable_knowledge\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m: [\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m      \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43msubject\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m...\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m      \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43mpredicate\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m...\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m      \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m...\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m      \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m...\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m  # ISO8601\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m    }\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m  ]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     78\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m}\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     79\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIf no knowledge can be extracted, return:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     80\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43mvaluable_knowledge\u001b[39;49m\u001b[38;5;130;43;01m\\\"\u001b[39;49;00m\u001b[38;5;124;43m: []}\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mschema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproperties\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvaluable_knowledge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitems\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproperties\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredicate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate-time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrequired\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredicate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrequired\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvaluable_knowledge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m         knowledge_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama.py:2012\u001b[0m, in \u001b[0;36mLlama.create_chat_completion\u001b[0;34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs)\u001b[0m\n\u001b[1;32m   1974\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate a chat completion from a list of messages.\u001b[39;00m\n\u001b[1;32m   1975\u001b[0m \n\u001b[1;32m   1976\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;124;03m    Generated chat completion or a stream of chat completion chunks.\u001b[39;00m\n\u001b[1;32m   2006\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2007\u001b[0m handler \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2008\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_handler\n\u001b[1;32m   2009\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_handlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   2010\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m llama_chat_format\u001b[38;5;241m.\u001b[39mget_chat_completion_handler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   2011\u001b[0m )\n\u001b[0;32m-> 2012\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllama\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2039\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2042\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama_chat_format.py:663\u001b[0m, in \u001b[0;36mchat_formatter_to_chat_completion_handler.<locals>.chat_completion_handler\u001b[0;34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e), file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m    659\u001b[0m         grammar \u001b[38;5;241m=\u001b[39m llama_grammar\u001b[38;5;241m.\u001b[39mLlamaGrammar\u001b[38;5;241m.\u001b[39mfrom_string(\n\u001b[1;32m    660\u001b[0m             llama_grammar\u001b[38;5;241m.\u001b[39mJSON_GBNF, verbose\u001b[38;5;241m=\u001b[39mllama\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m    661\u001b[0m         )\n\u001b[0;32m--> 663\u001b[0m completion_or_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     tool_name \u001b[38;5;241m=\u001b[39m tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama.py:1846\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1846\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama.py:1321\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1319\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1320\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1321\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_vocab_is_eog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompletion_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama.py:912\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 912\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    914\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    915\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    916\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    930\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    931\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama.py:646\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    642\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    644\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    645\u001b[0m )\n\u001b[0;32m--> 646\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/_internals.py:300\u001b[0m, in \u001b[0;36mLlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[0;32m--> 300\u001b[0m     return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import threading\n",
    "from datetime import datetime\n",
    "\n",
    "from chonkie import SemanticChunker\n",
    "from llama_cpp import Llama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from json import JSONDecodeError\n",
    "from chonkie import SentenceChunker\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# The Chatbot and LlamaSingleton\n",
    "# -------------------------------\n",
    "\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/Qwen2.5-7B-Instruct-Uncensored.Q4_K_M.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=2048)\n",
    "            return cls._instance\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, \n",
    "                 messages_file='messages.json', \n",
    "                 knowledge_file='knowledge.json', \n",
    "                 faiss_index_file='faiss_index.pkl',\n",
    "                 model_name='all-MiniLM-L6-v2'):\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "        self.llm = LlamaSingleton().llm\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.knowledge_data = []\n",
    "        self.initialize_files()\n",
    "        self.load_faiss_index()\n",
    "\n",
    "    def initialize_files(self):\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def save_json_data(self, file_path, data):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "    def extract_valuable_knowledge(self, message):\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are a knowledge extractor. Try to Extract any knowledge from the user.\\n\"\n",
    "                        \"Return ONLY JSON with the following schema:\\n\"\n",
    "                        \"{\\n\"\n",
    "                        \"  \\\"valuable_knowledge\\\": [\\n\"\n",
    "                        \"    {\\n\"\n",
    "                        \"      \\\"subject\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"predicate\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"object\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"timestamp\\\": \\\"...\\\"  # ISO8601\\n\"\n",
    "                        \"    }\\n\"\n",
    "                        \"  ]\\n\"\n",
    "                        \"}\\n\"\n",
    "                        \"If no knowledge can be extracted, return:\\n\"\n",
    "                        \"{\\\"valuable_knowledge\\\": []}\"\n",
    "                    )\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"valuable_knowledge\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"subject\": {\"type\": \"string\"},\n",
    "                                    \"predicate\": {\"type\": \"string\"},\n",
    "                                    \"object\": {\"type\": \"string\"},\n",
    "                                    \"timestamp\": {\"type\": \"string\", \"format\": \"date-time\"}\n",
    "                                },\n",
    "                                \"required\": [\"subject\", \"predicate\", \"object\", \"timestamp\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"valuable_knowledge\"],\n",
    "                },\n",
    "            },\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        try:\n",
    "            knowledge_data = json.loads(response['choices'][0]['message']['content'])\n",
    "            print(\"Extracted knowledge from a chunk:\", knowledge_data)\n",
    "            if \"valuable_knowledge\" not in knowledge_data:\n",
    "                knowledge_data[\"valuable_knowledge\"] = []\n",
    "            return knowledge_data[\"valuable_knowledge\"]\n",
    "        except (JSONDecodeError, KeyError):\n",
    "            return []\n",
    "\n",
    "    def save_knowledge(self, triplets):\n",
    "        if not triplets:\n",
    "            return\n",
    "        knowledge = self.load_json_data(self.knowledge_file)\n",
    "        existing_set = {(t['subject'], t['predicate'], t['object']) for t in knowledge}\n",
    "        new_triplets = []\n",
    "        for triplet in triplets:\n",
    "            # Use the current timestamp for each new triplet\n",
    "            triplet['timestamp'] = datetime.utcnow().isoformat()\n",
    "            key = (triplet['subject'], triplet['predicate'], triplet['object'])\n",
    "            if key not in existing_set:\n",
    "                knowledge.append(triplet)\n",
    "                new_triplets.append(triplet)\n",
    "                existing_set.add(key)\n",
    "        self.save_json_data(self.knowledge_file, knowledge)\n",
    "        if new_triplets:\n",
    "            self.update_faiss_index(new_triplets)\n",
    "\n",
    "    def update_faiss_index(self, triplets):\n",
    "        texts = [f\"{t['subject']} {t['predicate']} {t['object']}\" for t in triplets]\n",
    "        embeddings = self.model.encode(texts)\n",
    "        if self.index is None:\n",
    "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(np.array(embeddings, dtype=np.float32))\n",
    "        self.knowledge_data.extend(triplets)\n",
    "        self.save_faiss_index()\n",
    "\n",
    "    def save_faiss_index(self):\n",
    "        with open(self.faiss_index_file, 'wb') as f:\n",
    "            pickle.dump((self.index, self.knowledge_data), f)\n",
    "\n",
    "    def load_faiss_index(self):\n",
    "        if os.path.exists(self.faiss_index_file):\n",
    "            with open(self.faiss_index_file, 'rb') as f:\n",
    "                self.index, self.knowledge_data = pickle.load(f)\n",
    "        else:\n",
    "            self.index = None\n",
    "            self.knowledge_data = []\n",
    "\n",
    "# -------------------------------\n",
    "# Main Processing: Chunk and Extract Knowledge\n",
    "# -------------------------------\n",
    "\n",
    "def main():\n",
    "    # ----------------------------\n",
    "    # Part 1: Chunk the input file\n",
    "    # ----------------------------\n",
    "    # Initialize the SemanticChunker with desired parameters\n",
    "    chunker = SentenceChunker(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=128,\n",
    "        min_sentences_per_chunk=40 #20 voor kort/ 40 voor medium/ 80 voor lang\n",
    "    )\n",
    "\n",
    "    # Read the contents of input.txt\n",
    "    with open('input.txt', 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Get the list of chunks from the input text\n",
    "    chunks = chunker.chunk(text)\n",
    "    print(f\"Total chunks created: {len(chunks)}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Part 2: Extract Knowledge from Each Chunk\n",
    "    # ----------------------------\n",
    "    # Instantiate the Chatbot (for its knowledge extraction capability)\n",
    "    chatbot = Chatbot()\n",
    "\n",
    "    # Process each chunk\n",
    "    for i, chunk in enumerate(chunks, start=1):\n",
    "        print(f\"\\nProcessing chunk {i} (Token count: {chunk.token_count}, Sentences: {len(chunk.sentences)})\")\n",
    "        # Extract valuable knowledge from the chunk text\n",
    "        extracted_knowledge = chatbot.extract_valuable_knowledge(chunk.text)\n",
    "        # Save extracted knowledge (if any) to knowledge.json and update FAISS index\n",
    "        if extracted_knowledge:\n",
    "            chatbot.save_knowledge(extracted_knowledge)\n",
    "\n",
    "    print(\"\\nKnowledge extraction complete.\")\n",
    "    print(\"Please check 'knowledge.json' for the extracted valuable knowledge.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIMECODES ZIJN NU START EIND TIJD VAN CHUNKS EN CHUNKS GELADEN UIOT JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M3 Max) - 26559 MiB free\n",
      "llama_model_loader: loaded meta data with 41 key-value pairs and 339 tensors from models/Qwen2.5-7B-Instruct-Uncensored.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct Uncensored\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct-Uncensored\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   6:                            general.license str              = gpl-3.0\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Qwen2.5 7B Instruct\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Qwen\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7...\n",
      "llama_model_loader: - kv  11:                               general.tags arr[str,3]       = [\"qwen\", \"uncensored\", \"text-generati...\n",
      "llama_model_loader: - kv  12:                          general.languages arr[str,2]       = [\"zh\", \"en\"]\n",
      "llama_model_loader: - kv  13:                           general.datasets arr[str,5]       = [\"NobodyExistsOnTheInternet/ToxicQAFi...\n",
      "llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584\n",
      "llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944\n",
      "llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28\n",
      "llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  22:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {% set system_message = 'You are a he...\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  34:                                general.url str              = https://huggingface.co/mradermacher/Q...\n",
      "llama_model_loader: - kv  35:              mradermacher.quantize_version str              = 2\n",
      "llama_model_loader: - kv  36:                  mradermacher.quantized_by str              = mradermacher\n",
      "llama_model_loader: - kv  37:                  mradermacher.quantized_at str              = 2024-10-11T20:15:47+02:00\n",
      "llama_model_loader: - kv  38:                  mradermacher.quantized_on str              = db3\n",
      "llama_model_loader: - kv  39:                         general.source.url str              = https://huggingface.co/Orion-zhen/Qwe...\n",
      "llama_model_loader: - kv  40:                  mradermacher.convert_type str              = hf\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type q4_K:  169 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.36 GiB (4.91 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 3584\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 28\n",
      "print_info: n_head_kv        = 4\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 7\n",
      "print_info: n_embd_k_gqa     = 512\n",
      "print_info: n_embd_v_gqa     = 512\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 18944\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.62 B\n",
      "print_info: general.name     = Qwen2.5 7B Instruct Uncensored\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 152064\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 148848 'ÄĬ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 338 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/29 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  4460.45 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks loaded: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 28991.03 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x33ad5a710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x33a18eac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x33a1903a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x33b104080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x33a18f860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x33a18f080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x33a190d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x33a191310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x33a8ffd10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x33a34da00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x33a191940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x33b1056d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x33b00dbd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x33a34e340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x33b00f7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x33a192530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x33b1050c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x33b00f190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x33b00ff90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x33b105e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x33a34e6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x33b106610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x33a1920b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x33a192840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x339ed8ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x3467dc850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x339ed9b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x347621bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x3475298a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x3461ddbc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x33a34f6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x339ed9f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x3461ca1a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x33b010410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x33ad5be40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x33ad5c980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x33b0107f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x33b011f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x33a193dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x339ed8da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x33b012560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x347529d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x347621750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x33b0128b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x33b012cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x34752a010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x3461ca460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x34752a2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x339eda3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x339edac80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x339edb150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x3475265d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x347526cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x339edb6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x3471f4bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x347656080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x339edb980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x347656c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x33b106b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x33ad5aee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x33a193670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x347656fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x347657560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x33b0136b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x33b013a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x339edc8f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x347657b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x347657dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x33b013f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x33b107690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x33a34ffd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x33ad5b570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x347526890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x33ad5d830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x33b014710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x33ad5dfc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x33b014c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x33a351f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x33a352250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x33ad5e4a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x33a194740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x33a195850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x33ad5f280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x33ad5f8a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x347658670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x347523420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x33a195b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x3461de500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x347658f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x347523cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x33ad5fb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x339edcee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x33ad60610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x33b0153a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x3471f4e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x3475288e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x3471f5750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x347523f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x3461dea70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x347529260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x33a352510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x339edbc40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x3471f5a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x347659580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x339edd9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x339ede7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x339edeaa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x3471f5fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x339edf0c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x3461dfd50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x3471f6730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x347659b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x34752af80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x33b107fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x3471f7260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x3471f6a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1122042e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x34752b340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x34765a400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x339edf9b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x3461e0210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x33a352d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x33b0157f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x33b107950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x33a195f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x33b015f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x33a196c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x33b0165c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x33ad608d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x33ad613a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x3461e09f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x339ee0280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x3471f8fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x33a3532e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x33ad61660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x33a197300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x33a353b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x33a3541c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x33a354480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x33a354740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x33b016d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x34765a6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x339ee0540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x33ad61c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x33ad62260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x34765ab30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x34765b190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x33a1975c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x33a197b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x33a198110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x33a354e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x33ad62840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x33a3553e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x33b108ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x33b017750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x33a355c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x33ad630d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x33a1986d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x33a199460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x33a3562f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x33a3565b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x33a199ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x33a19a100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x34765b660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x33b108750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x33a356f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x339ee0800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x33ad63390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x33b109d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x33ad63720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x33a356870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x33a19a3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x33ad63e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x33b10aa50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x33b018440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x34765b920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x33ad64b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x33ad65240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x33b10b100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x33b018af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x33b019140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x339ee1030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x33b10b6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x339ee12f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x33a357960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x33b019500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x33a19a750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x33b10bc40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x33ad65600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x33ad658c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x34752b600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x34765c470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x34765ccc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x33b01a030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x347528c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x33a357c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x33b10bf00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x339ee1b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x33b01a510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x33ad66530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x33b01aa10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x33b10c360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x339ee2110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x33ad65f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x34752c9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x33ad66b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x339ee23d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x33a19aed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x33a19ba90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x33b01b080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x33b10c870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x33b10cb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x339ee2a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x33a358990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x33b10d590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x33b01b720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x34752cc60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x3461e13f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x34752cf20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x33a359010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x33a3595e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x33b10d130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x33a35a050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x33a35a650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x34765d200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x34752d1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x33ad677e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x3461e0d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x347626ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x17a507310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x339ee35a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x34752d4a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x346411af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x339ee3930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x3461e1df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x346412790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x34765d6f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x3461e2350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x3461e2650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x3461e2910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x346412a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x3476244f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x34752dc90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x347624ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x3461e36a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x3461e44f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x339ee3bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x34752e4b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1122047d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x34752ee00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x339ee4250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x3461e48b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x33a35a9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x34752f1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x34752f550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x339ee4d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x3476255d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x33b01c090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x33b01c420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x33b01c9b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x33a19bff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x33a19c570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x33ad67310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x33ad68290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x33ad67df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x33a19d1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x33b01d5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x33ad69830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x347625890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x33ad69fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x33a19d500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x33ad6a500 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init:        CPU KV buffer size =   112.00 MiB\n",
      "llama_init_from_model: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   304.00 MiB\n",
      "llama_init_from_model: graph nodes  = 986\n",
      "llama_init_from_model: graph splits = 450 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/Orion-zhen/Qwen2.5-7B-Instruct-Uncensored', 'mradermacher.quantized_at': '2024-10-11T20:15:47+02:00', 'mradermacher.quantize_version': '2', 'general.url': 'https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF', 'tokenizer.chat_template': \"{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% endif %}{% if system_message is defined %}{{ '<|im_start|>system\\n' + system_message + '<|im_end|>\\n' }}{% endif %}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + content + '<|im_end|>\\n<|im_start|>assistant\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\n' }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.basename': 'Qwen2.5', 'qwen2.attention.head_count_kv': '4', 'general.size_label': '7B', 'general.base_model.0.name': 'Qwen2.5 7B Instruct', 'qwen2.embedding_length': '3584', 'qwen2.context_length': '32768', 'qwen2.block_count': '28', 'general.base_model.0.organization': 'Qwen', 'tokenizer.ggml.pre': 'qwen2', 'general.base_model.count': '1', 'qwen2.rope.freq_base': '1000000.000000', 'general.quantization_version': '2', 'general.license': 'gpl-3.0', 'general.base_model.0.repo_url': 'https://huggingface.co/Qwen/Qwen2.5-7B-Instruct', 'general.file_type': '15', 'general.finetune': 'Instruct-Uncensored', 'general.name': 'Qwen2.5 7B Instruct Uncensored', 'qwen2.feed_forward_length': '18944', 'mradermacher.quantized_by': 'mradermacher', 'general.architecture': 'qwen2', 'mradermacher.quantized_on': 'db3', 'qwen2.attention.head_count': '28', 'tokenizer.ggml.bos_token_id': '151643', 'general.type': 'model', 'tokenizer.ggml.model': 'gpt2'}\n",
      "Available chat formats from metadata: chat_template.default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 1 (Start: 1.448, End: 15.18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   173 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   116 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6567.60 ms /   289 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 65 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'fuel and EVAP system', 'predicate': 'purpose', 'object': 'prevent harmful vapors from entering the atmosphere causing smog', 'timestamp': '0:00-0:10'}, {'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side', 'timestamp': '0:11-0:18'}]}\n",
      "\n",
      "Processing chunk 2 (Start: 15.2, End: 28.755)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   172 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8187.45 ms /   237 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 35 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'canister', 'predicate': 'is_held_in_by', 'object': 'two 12mm pieces of rust', 'timestamp': 'chunk start:0 chunk end:15'}, {'subject': 'vacuum hoses and electrical connectors', 'predicate': 'have_been_disconnected', 'object': 'from this side of the EVAP canister', 'timestamp': 'chunk start:28 chunk end:51'}, {'subject': 'the canister', 'predicate': 'is_being_removed_from', 'object': 'above the subframe', 'timestamp': 'chunk start:54 chunk end:77'}]}\n",
      "\n",
      "Processing chunk 3 (Start: 29.356, End: 35.579)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   141 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6738.24 ms /   176 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 35 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'gas tank', 'predicate': 'location', 'object': 'rear seat of the vehicle', 'timestamp': '0:00-0:07'}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'steel', 'timestamp': '0:11-0:13'}, {'subject': 'gas tank', 'predicate': 'attachment method', 'object': 'two straps', 'timestamp': '0:15-0:19'}]}\n",
      "\n",
      "Processing chunk 4 (Start: 35.679, End: 41.803)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3479.08 ms /    95 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 468 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'the strap', 'predicate': 'is held by', 'object': 'two 14mm bolts', 'timestamp': '0:31-0:38'}]}\n",
      "\n",
      "Processing chunk 5 (Start: 43.564, End: 148.419)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   671 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   32149.26 ms /  1139 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 147 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'fuel tank', 'predicate': 'contains', 'object': 'harmful vapors', 'timestamp': '0,33'}, {'subject': 'evap canister', 'predicate': 'absorbs', 'object': 'harmful vapors', 'timestamp': '0,33'}, {'subject': 'evap canister', 'predicate': 'vent', 'object': 'harmful vapors', 'timestamp': '0,33'}, {'subject': 'engine', 'predicate': 'burns', 'object': 'harmful vapors', 'timestamp': '0,33'}, {'subject': 'fuel filler neck', 'predicate': 'ventilates', 'object': 'gas tank', 'timestamp': '0,67'}, {'subject': 'fuel filler neck', 'predicate': 'ventilates', 'object': 'EVAP canister', 'timestamp': '0,67'}, {'subject': 'fill check valve', 'predicate': 'ventilates', 'object': 'gas tank', 'timestamp': '0,67'}, {'subject': 'fill check valve', 'predicate': 'ventilates', 'object': 'EVAP canister', 'timestamp': '0,67'}, {'subject': 'vapor pressure sensor', 'predicate': 'takes reading', 'object': 'atmospheric pressure', 'timestamp': '0,67'}, {'subject': 'vapor pressure sensor', 'predicate': 'gives reading', 'object': 'voltage', 'timestamp': '0,67'}, {'subject': 'fuel filler neck', 'predicate': 'ventilates', 'object': 'gas tank', 'timestamp': '1,00'}, {'subject': 'fuel filler neck', 'predicate': 'ventilates', 'object': 'EVAP canister', 'timestamp': '1,00'}, {'subject': 'fuel filler neck', 'predicate': 'ventilates', 'object': 'gas tank', 'timestamp': '1,00'}, {'subject': 'fuel filler neck', 'predicate': 'ventilates', 'object': 'EVAP canister', 'timestamp': '1,00'}, {'subject': 'fuel filler neck', 'predicate': 'ventilates', 'object': 'gas tank', 'timestamp': '1,00'}, {'subject': 'fuel filler neck', 'predicate': 'ventilates', 'object': 'EVAP canister', 'timestamp': '1,00'}]}\n",
      "\n",
      "Processing chunk 6 (Start: 148.639, End: 180.042)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   147 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   13037.93 ms /   426 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 22 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'fill check valve', 'predicate': 'functions as', 'object': 'allowing excess vapor to escape from the fuel tank into a canister where it gets filtered', 'timestamp': '0:00-0:15'}, {'subject': 'excess air pressure', 'predicate': 'drained out through', 'object': 'the air drain valve at the bottom', 'timestamp': '0:15-0:25'}, {'subject': 'vapor pressure sensor', 'predicate': 'senses', 'object': 'the pressure in the tank', 'timestamp': '0:25-0:35'}, {'subject': 'vacuum switching valve', 'predicate': 'used by ECU', 'object': 'to draw a vacuum on the entire system and monitor the pressure to test for leaks', 'timestamp': '0:35-0:55'}, {'subject': 'check engine light', 'predicate': 'indicating', 'object': 'problems in the system', 'timestamp': '0:55-1:15'}]}\n",
      "\n",
      "Processing chunk 7 (Start: 180.282, End: 183.305)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    35 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1759.72 ms /    57 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 51 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'ventilation hoses', 'predicate': 'disconnect', 'object': 'tank side', 'timestamp': 'over here'}]}\n",
      "\n",
      "Processing chunk 8 (Start: 185.836, End: 194.882)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    51 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   131 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6387.15 ms /   182 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 118 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'EVAP line', 'predicate': 'is held on by', 'object': 'four pieces of rust', 'timestamp': '0'}, {'subject': 'EVAP check valve', 'predicate': 'has an EVAP line', 'object': 'attached to it', 'timestamp': '0'}, {'subject': 'screws', 'predicate': 'are used to secure', 'object': 'lid', 'timestamp': '29'}]}\n",
      "\n",
      "Processing chunk 9 (Start: 196.161, End: 222.294)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   118 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    43 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3208.23 ms /   161 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 16 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'check valve', 'predicate': 'remove', 'object': 'six pieces of rust', 'timestamp': '0:21-0:28'}]}\n",
      "\n",
      "Processing chunk 10 (Start: 222.314, End: 223.915)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    40 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1902.42 ms /    56 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 179 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'grinding', 'predicate': 'need', 'object': 'off', 'timestamp': '0:00-0:04'}]}\n",
      "\n",
      "Processing chunk 11 (Start: 224.015, End: 264.245)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   179 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   183 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9248.44 ms /   362 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 15 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'fill check valve', 'predicate': 'allows escape of vapor', 'object': 'through valve into pipe', 'timestamp': '0:26'}, {'subject': 'vapor', 'predicate': 'escapes through valve', 'object': 'into EVAP canister', 'timestamp': '0:31'}, {'subject': 'remaining pressure', 'predicate': 'exits out here', 'object': 'after vapor enters canister', 'timestamp': '0:33'}, {'subject': 'harmful gases', 'predicate': 'are left inside of', 'object': 'charcoal', 'timestamp': '0:35'}]}\n",
      "\n",
      "Processing chunk 12 (Start: 264.265, End: 266.767)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     833.85 ms /    23 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 64 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': []}\n",
      "\n",
      "Processing chunk 13 (Start: 266.787, End: 280.936)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   240 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11151.78 ms /   304 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 107 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'float', 'predicate': 'rise', 'object': 'when fuel tank is full', 'timestamp': '0:00-0:06'}, {'subject': 'float', 'predicate': 'seal off', 'object': 'vapor ventilation', 'timestamp': '0:06-0:11'}, {'subject': 'float', 'predicate': 'prevent', 'object': 'liquid from going inside charcoal canister', 'timestamp': '0:11-0:18'}, {'subject': 'charcoal canister', 'predicate': 'be harmed by', 'object': 'liquid', 'timestamp': '0:18-0:22'}, {'subject': 'speaker', 'predicate': 'intend to', 'object': \"open the float to see what's inside\", 'timestamp': '0:22-0:29'}]}\n",
      "\n",
      "Processing chunk 14 (Start: 281.796, End: 303.949)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   107 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   327 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   15748.31 ms /   434 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 81 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'fuel check valve', 'predicate': 'contains', 'object': 'lid and diaphragm', 'timestamp': '0:00-0:06'}, {'subject': 'diaphragm', 'predicate': 'can move', 'object': 'up and down', 'timestamp': '0:06-0:10'}, {'subject': 'atmospheric pressure', 'predicate': 'allows', 'object': 'vapor movement', 'timestamp': '0:10-0:17'}, {'subject': 'fuel cap', 'predicate': 'seals', 'object': 'vapors', 'timestamp': '0:17-0:23'}, {'subject': 'diaphragm', 'predicate': 'seals against', 'object': 'lid', 'timestamp': '0:23-0:28'}, {'subject': 'vapors', 'predicate': 'cannot escape', 'object': 'if fuel cap is sealed', 'timestamp': '0:28-0:35'}, {'subject': 'gas tank', 'predicate': 'will be opened', 'object': 'for inspection', 'timestamp': '0:35-0:40'}]}\n",
      "\n",
      "Processing chunk 15 (Start: 308.171, End: 325.335)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   162 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8945.76 ms /   243 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 185 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'gas tank', 'predicate': 'material', 'object': 'sheet metal', 'timestamp': '0:0'}, {'subject': 'gas tank', 'predicate': 'construction', 'object': 'two halves', 'timestamp': '0:1'}, {'subject': 'gas tank', 'predicate': 'joining method', 'object': 'welded', 'timestamp': '0:7'}, {'subject': 'gas tank', 'predicate': 'baffles', 'object': 'run along the perimeter', 'timestamp': '0:15'}]}\n",
      "\n",
      "Processing chunk 16 (Start: 325.575, End: 365.911)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   288 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   14813.69 ms /   473 tokens\n",
      "Llama.generate: 102 prefix-match hit, remaining 50 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'baffle', 'predicate': 'prevents', 'object': 'fuel sloshing', 'timestamp': '0:00-0:06'}, {'subject': 'fuel pump', 'predicate': 'location', 'object': 'lowest part of the tank', 'timestamp': '0:07-0:11'}, {'subject': 'evap check valve', 'predicate': 'location', 'object': 'highest part of the tank', 'timestamp': '0:12-0:19'}, {'subject': 'check valve', 'predicate': 'function', 'object': 'allows fluid entry and prevents escape', 'timestamp': '0:20-0:32'}, {'subject': 'maximum gasoline level', 'predicate': 'determined by', 'object': 'height of fuel fill check valve', 'timestamp': '0:33-0:40'}, {'subject': 'remaining tank area', 'predicate': 'purpose', 'object': 'fuel vapor storage', 'timestamp': '0:41-0:47'}]}\n",
      "\n",
      "Processing chunk 17 (Start: 367.711, End: 378.521)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    50 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   151 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7757.94 ms /   201 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 39 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'gas caps', 'predicate': 'are sealed', 'object': 'on newer OBD2 cars', 'timestamp': '0:00-0:06'}, {'subject': 'gas caps', 'predicate': 'are part of a sealed system', 'object': 'including the gas tank', 'timestamp': '0:07-0:16'}, {'subject': 'gas cap', 'predicate': 'is', 'object': 'just a cap', 'timestamp': '0:17-0:23'}]}\n",
      "\n",
      "Processing chunk 18 (Start: 378.841, End: 385.745)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    39 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   248 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11515.27 ms /   287 tokens\n",
      "Llama.generate: 103 prefix-match hit, remaining 11 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'check valve', 'predicate': 'is inside', 'object': 'here', 'timestamp': '0:00:00,000-0:00:02,000'}, {'subject': 'check valve', 'predicate': 'can be removed', 'object': 'by removing', 'timestamp': '0:00:02,000-0:00:04,000'}, {'subject': 'filter', 'predicate': 'is present', 'object': 'inside the device after removing check valve', 'timestamp': '0:00:05,000-0:00:08,000'}, {'subject': 'spring', 'predicate': 'is present', 'object': 'inside the device after removing check valve', 'timestamp': '0:00:08,000-0:00:10,000'}]}\n",
      "\n",
      "Processing chunk 19 (Start: 386.586, End: 387.967)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    49 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2172.01 ms /    60 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 11 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'valve', 'predicate': 'is', 'object': 'itself', 'timestamp': '0 11'}]}\n",
      "\n",
      "Processing chunk 20 (Start: 388.627, End: 389.808)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2393.72 ms /    65 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 145 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'spring', 'predicate': 'followed by', 'object': 'spring', 'timestamp': '0:12-0:18'}]}\n",
      "\n",
      "Processing chunk 21 (Start: 390.148, End: 420.44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   145 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   180 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8960.98 ms /   325 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 78 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'Gas cap', 'predicate': 'usually sealed', 'object': 'in normal situations', 'timestamp': '0,13'}, {'subject': 'Safety valve', 'predicate': 'exists', 'object': 'to vent pressure', 'timestamp': '13,23'}, {'subject': 'Fuel tank vapor pressure sensor', 'predicate': 'reads pressure', 'object': 'in the tank', 'timestamp': '53,60'}, {'subject': 'Fuel tank vapor pressure sensor', 'predicate': 'measures differential', 'object': 'between tank pressure and atmospheric pressure', 'timestamp': '60,74'}]}\n",
      "\n",
      "Processing chunk 22 (Start: 420.46, End: 435.548)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5541.82 ms /   181 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 164 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'ECU', 'predicate': 'uses', 'object': 'voltage from middle terminal to determine EVAP system leaks', 'timestamp': '15-20'}, {'subject': 'vacuum switching valve', 'predicate': 'needs to be', 'object': 'cleaned from rust', 'timestamp': '25-30'}]}\n",
      "\n",
      "Processing chunk 23 (Start: 435.808, End: 468.877)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   164 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   419 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19959.59 ms /   583 tokens\n",
      "Llama.generate: 104 prefix-match hit, remaining 59 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'vacuum switching valve', 'predicate': 'functions as', 'object': 'redirecting air in the EVAP system', 'timestamp': '0,108'}, {'subject': 'vacuum switching valve', 'predicate': 'takes inlet air from', 'object': 'gas tank', 'timestamp': '1,23'}, {'subject': 'vacuum switching valve', 'predicate': 'vents air to', 'object': 'charcoal canister', 'timestamp': '2,25'}, {'subject': 'vacuum switching valve', 'predicate': 'is', 'object': 'a very simple solenoid', 'timestamp': '3,26'}, {'subject': 'vacuum switching valve', 'predicate': 'applies 12 volts to', 'object': 'move plunger outward or inward', 'timestamp': '5,27'}, {'subject': 'vacuum switching valve', 'predicate': 'allows or blocks', 'object': 'flow of vapors going into canister', 'timestamp': '6,28'}, {'subject': 'vacuum switching valve', 'predicate': 'clicking sound indicates', 'object': 'redirecting air from inlet to outlet', 'timestamp': '8,29'}, {'subject': 'vacuum switching valve', 'predicate': 'no air coming out indicates', 'object': 'airflow blocked', 'timestamp': '9,30'}, {'subject': 'vacuum switching valve', 'predicate': '12 volts applied indicates', 'object': 'airflow allowed', 'timestamp': '10,31'}]}\n",
      "\n",
      "Processing chunk 24 (Start: 469.037, End: 482.005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   201 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9526.70 ms /   260 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 42 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors', 'timestamp': '0:00-0:06'}, {'subject': 'EVAP charcoal canister', 'predicate': 'process', 'object': 'absorb gasoline vapors by charcoal', 'timestamp': '0:06-0:12'}, {'subject': 'EVAP charcoal canister', 'predicate': 'process', 'object': 'release gasoline vapors back into intake', 'timestamp': '0:12-0:19'}, {'subject': 'check valves', 'predicate': 'location', 'object': 'here', 'timestamp': '0:20-0:27'}]}\n",
      "\n",
      "Processing chunk 25 (Start: 482.045, End: 489.831)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    42 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   157 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7582.24 ms /   199 tokens\n",
      "Llama.generate: 101 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'one-way valves', 'predicate': 'allow', 'object': 'air to go in', 'timestamp': '0:00-0:08'}, {'subject': 'one-way valves', 'predicate': \"don't allow\", 'object': 'air to come out', 'timestamp': '0:08-0:12'}, {'subject': 'one-way valves', 'predicate': 'are essential for', 'object': 'running the self-diagnostic for the EVAP system', 'timestamp': '0:12-0:22'}]}\n",
      "\n",
      "Processing chunk 26 (Start: 489.851, End: 494.159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   111 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4906.14 ms /   140 tokens\n",
      "Llama.generate: 104 prefix-match hit, remaining 171 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'the user', 'predicate': 'pulls out', 'object': 'check valves', 'timestamp': '0:00:00-0:00:03'}, {'subject': 'the user', 'predicate': 'chops open', 'object': 'something', 'timestamp': '0:00:04-0:00:07'}]}\n",
      "\n",
      "Processing chunk 27 (Start: 496.587, End: 534.03)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   171 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   241 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11658.05 ms /   412 tokens\n",
      "Llama.generate: 102 prefix-match hit, remaining 97 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'check valve', 'predicate': 'acts as', 'object': 'ball check valve', 'timestamp': '0:00-0:08'}, {'subject': 'air pressure', 'predicate': 'causes', 'object': 'diaphragm to rise', 'timestamp': '0:09-0:16'}, {'subject': 'diaphragm', 'predicate': 'allows air to escape', 'object': 'through two ports', 'timestamp': '0:17-0:26'}, {'subject': 'air pressure', 'predicate': 'prevents air from entering', 'object': 'through the other side', 'timestamp': '0:27-0:36'}, {'subject': 'charcoal canister', 'predicate': 'contains', 'object': 'paper filter', 'timestamp': '0:37-0:41'}]}\n",
      "\n",
      "Processing chunk 28 (Start: 534.611, End: 554.608)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   272 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12757.09 ms /   369 tokens\n",
      "Llama.generate: 102 prefix-match hit, remaining 126 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'activated charcoal', 'predicate': 'absorbs', 'object': 'gasoline vapors', 'timestamp': '00:00:00-00:00:10'}, {'subject': 'activated charcoal', 'predicate': 'smells like', 'object': 'gasoline', 'timestamp': '00:00:10-00:00:15'}, {'subject': 'activated charcoal', 'predicate': 'looks like', 'object': 'rat shit', 'timestamp': '00:00:15-00:00:25'}, {'subject': 'charcoal', 'predicate': 'used for', 'object': 'barbecue', 'timestamp': '00:00:35-00:00:40'}, {'subject': 'foam things', 'predicate': 'sit on', 'object': 'springs', 'timestamp': '00:00:45-00:00:55'}]}\n",
      "\n",
      "Processing chunk 29 (Start: 554.708, End: 579.948)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1834.51 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   126 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   277 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12823.97 ms /   403 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'spring with the pad', 'predicate': 'applies pressure to', 'object': 'charcoal', 'timestamp': '0:00:00-0:00:04'}, {'subject': 'two chambers', 'predicate': 'are separated by', 'object': 'a wall', 'timestamp': '0:00:10-0:00:15'}, {'subject': 'vapors', 'predicate': 'enter from', 'object': 'one side', 'timestamp': '0:00:17-0:00:21'}, {'subject': 'vapors', 'predicate': 'exit from', 'object': 'the other side', 'timestamp': '0:00:24-0:00:28'}, {'subject': 'EVAP system', 'predicate': 'works by', 'object': 'allowing maximum surface area of vapors to be absorbed through charcoal', 'timestamp': '0:00:33-0:00:40'}]}\n",
      "\n",
      "Knowledge extraction complete.\n",
      "Please check 'knowledge.json' for the extracted valuable knowledge.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import threading\n",
    "from llama_cpp import Llama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from json import JSONDecodeError\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------------------\n",
    "# The Chatbot and LlamaSingleton\n",
    "# -------------------------------\n",
    "\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/Qwen2.5-7B-Instruct-Uncensored.Q4_K_M.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=2048)\n",
    "            return cls._instance\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, \n",
    "                 messages_file='messages.json', \n",
    "                 knowledge_file='knowledge.json', \n",
    "                 faiss_index_file='faiss_index.pkl',\n",
    "                 model_name='all-MiniLM-L6-v2'):\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "        self.llm = LlamaSingleton().llm\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.knowledge_data = []\n",
    "        self.initialize_files()\n",
    "        self.load_faiss_index()\n",
    "\n",
    "    def initialize_files(self):\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def save_json_data(self, file_path, data):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "    def extract_valuable_knowledge(self, message):\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are a knowledge extractor. Try to extract any knowledge from the user.\\n\"\n",
    "                        \"Return ONLY JSON with the following schema:\\n\"\n",
    "                        \"{\\n\"\n",
    "                        \"  \\\"valuable_knowledge\\\": [\\n\"\n",
    "                        \"    {\\n\"\n",
    "                        \"      \\\"subject\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"predicate\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"object\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"timestamp\\\": \\\"...\\\"  # This field will be replaced with chunk start and end\\n\"\n",
    "                        \"    }\\n\"\n",
    "                        \"  ]\\n\"\n",
    "                        \"}\\n\"\n",
    "                        \"If no knowledge can be extracted, return:\\n\"\n",
    "                        \"{\\\"valuable_knowledge\\\": []}\"\n",
    "                    )\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"valuable_knowledge\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"subject\": {\"type\": \"string\"},\n",
    "                                    \"predicate\": {\"type\": \"string\"},\n",
    "                                    \"object\": {\"type\": \"string\"},\n",
    "                                    \"timestamp\": {\"type\": \"string\", \"format\": \"date-time\"}\n",
    "                                },\n",
    "                                \"required\": [\"subject\", \"predicate\", \"object\", \"timestamp\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"valuable_knowledge\"],\n",
    "                },\n",
    "            },\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        try:\n",
    "            knowledge_data = json.loads(response['choices'][0]['message']['content'])\n",
    "            print(\"Extracted knowledge from a chunk:\", knowledge_data)\n",
    "            if \"valuable_knowledge\" not in knowledge_data:\n",
    "                knowledge_data[\"valuable_knowledge\"] = []\n",
    "            return knowledge_data[\"valuable_knowledge\"]\n",
    "        except (JSONDecodeError, KeyError):\n",
    "            return []\n",
    "\n",
    "    def save_knowledge(self, triplets):\n",
    "        if not triplets:\n",
    "            return\n",
    "        knowledge = self.load_json_data(self.knowledge_file)\n",
    "        existing_set = {(t['subject'], t['predicate'], t['object']) for t in knowledge}\n",
    "        new_triplets = []\n",
    "        for triplet in triplets:\n",
    "            # Here we assume the triplet already includes \"start\" and \"end\" keys.\n",
    "            key = (triplet['subject'], triplet['predicate'], triplet['object'])\n",
    "            if key not in existing_set:\n",
    "                knowledge.append(triplet)\n",
    "                new_triplets.append(triplet)\n",
    "                existing_set.add(key)\n",
    "        self.save_json_data(self.knowledge_file, knowledge)\n",
    "        if new_triplets:\n",
    "            self.update_faiss_index(new_triplets)\n",
    "\n",
    "    def update_faiss_index(self, triplets):\n",
    "        texts = [f\"{t['subject']} {t['predicate']} {t['object']}\" for t in triplets]\n",
    "        embeddings = self.model.encode(texts)\n",
    "        if self.index is None:\n",
    "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(np.array(embeddings, dtype=np.float32))\n",
    "        self.knowledge_data.extend(triplets)\n",
    "        self.save_faiss_index()\n",
    "\n",
    "    def save_faiss_index(self):\n",
    "        with open(self.faiss_index_file, 'wb') as f:\n",
    "            pickle.dump((self.index, self.knowledge_data), f)\n",
    "\n",
    "    def load_faiss_index(self):\n",
    "        if os.path.exists(self.faiss_index_file):\n",
    "            with open(self.faiss_index_file, 'rb') as f:\n",
    "                self.index, self.knowledge_data = pickle.load(f)\n",
    "        else:\n",
    "            self.index = None\n",
    "            self.knowledge_data = []\n",
    "\n",
    "# -------------------------------\n",
    "# Main Processing: Load Chunks and Extract Knowledge\n",
    "# -------------------------------\n",
    "\n",
    "def main():\n",
    "    # ----------------------------\n",
    "    # Part 1: Load the chunks from output_chunks.json\n",
    "    # ----------------------------\n",
    "    with open('output_chunks.json', 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    chunks = data.get(\"chunks\", [])\n",
    "    print(f\"Total chunks loaded: {len(chunks)}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Part 2: Extract Knowledge from Each Chunk\n",
    "    # ----------------------------\n",
    "    # Instantiate the Chatbot (for its knowledge extraction capability)\n",
    "    chatbot = Chatbot()\n",
    "\n",
    "    # Process each chunk from output_chunks.json\n",
    "    for i, chunk in enumerate(chunks, start=1):\n",
    "        text = chunk.get(\"text\", \"\")\n",
    "        start_time = chunk.get(\"start\")\n",
    "        end_time = chunk.get(\"end\")\n",
    "        print(f\"\\nProcessing chunk {i} (Start: {start_time}, End: {end_time})\")\n",
    "        # Extract valuable knowledge from the chunk text\n",
    "        extracted_knowledge = chatbot.extract_valuable_knowledge(text)\n",
    "        if extracted_knowledge:\n",
    "            # Replace the generated timestamp with the chunk's start and end times.\n",
    "            for triplet in extracted_knowledge:\n",
    "                triplet['start'] = start_time\n",
    "                triplet['end'] = end_time\n",
    "                if 'timestamp' in triplet:\n",
    "                    del triplet['timestamp']\n",
    "            chatbot.save_knowledge(extracted_knowledge)\n",
    "\n",
    "    print(\"\\nKnowledge extraction complete.\")\n",
    "    print(\"Please check 'knowledge.json' for the extracted valuable knowledge.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIMESTAMPS WEG NU ALLEEN START+EIND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M3 Max) - 26384 MiB free\n",
      "llama_model_loader: loaded meta data with 41 key-value pairs and 339 tensors from models/Qwen2.5-7B-Instruct-Uncensored.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct Uncensored\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct-Uncensored\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   6:                            general.license str              = gpl-3.0\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Qwen2.5 7B Instruct\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Qwen\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7...\n",
      "llama_model_loader: - kv  11:                               general.tags arr[str,3]       = [\"qwen\", \"uncensored\", \"text-generati...\n",
      "llama_model_loader: - kv  12:                          general.languages arr[str,2]       = [\"zh\", \"en\"]\n",
      "llama_model_loader: - kv  13:                           general.datasets arr[str,5]       = [\"NobodyExistsOnTheInternet/ToxicQAFi...\n",
      "llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584\n",
      "llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944\n",
      "llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28\n",
      "llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  22:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {% set system_message = 'You are a he...\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  34:                                general.url str              = https://huggingface.co/mradermacher/Q...\n",
      "llama_model_loader: - kv  35:              mradermacher.quantize_version str              = 2\n",
      "llama_model_loader: - kv  36:                  mradermacher.quantized_by str              = mradermacher\n",
      "llama_model_loader: - kv  37:                  mradermacher.quantized_at str              = 2024-10-11T20:15:47+02:00\n",
      "llama_model_loader: - kv  38:                  mradermacher.quantized_on str              = db3\n",
      "llama_model_loader: - kv  39:                         general.source.url str              = https://huggingface.co/Orion-zhen/Qwe...\n",
      "llama_model_loader: - kv  40:                  mradermacher.convert_type str              = hf\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type q4_K:  169 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.36 GiB (4.91 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 3584\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 28\n",
      "print_info: n_head_kv        = 4\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 7\n",
      "print_info: n_embd_k_gqa     = 512\n",
      "print_info: n_embd_v_gqa     = 512\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 18944\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.62 B\n",
      "print_info: general.name     = Qwen2.5 7B Instruct Uncensored\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 152064\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 148848 'ÄĬ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks loaded: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 338 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/29 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  4460.45 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 28991.03 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x4c7482220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x63cc129e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x4c748aa30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x4c7651720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x377d43700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x377d43cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x4c7486000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x86e153ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x86e153530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x3e3267350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x63cb54280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x4c767eb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x112205dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x3463ac720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x112206520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x112209e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x376b8a340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x376b8b2b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x3463ac9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x63cb58870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x376b8a600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x4c7651a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x376b8c9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x376b8c570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x376b8d130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x376b8d5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x376b8dba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x4c76513d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x376b8ec90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x63cb58c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x3463ae680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x3e32663b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x3463aec80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x3e3266880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x86e154470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x86e155770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x63cc0e1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x3e3267a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x3e3267fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x4c747f430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x3e3268260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x3e3269830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x4c767f2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x63cb6b010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x63cb6b9b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x34639fac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x4c7680350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x376b8e080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x3e3269140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x4c767f7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x17a50b250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x63cb6bfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x4c767fdf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x63cb54c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x377d44880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x3e3268cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x63ce519c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x63cc0d9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x63cc0d180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x63cc073e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x63ce52220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x63cc0e5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x63ce52c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x4c7482630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x86e155a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x63ce536b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x63cc0e8b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x4c7489f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x377d44e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x3e326a540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x3e326b350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x3e326b610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x17a90b630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x3463a0610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x376b8e6e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x376b8fdd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x3463a0d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x376b8f9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x63ce52ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x86e155e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x3e326b9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x86e156140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x63cb54ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x63ce54540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x3463a1320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x3463a1ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x3463a20d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x4c7681070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x63cb535d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x86e156ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x3e326c110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x4c7681980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x86e157900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x86e157bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x376b905f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x63cb555f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x3e326c6f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x3e326ce80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x3463a2930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x4c7682e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x4c7683490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x4c7683ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x86e157e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x86e158960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x4c7684190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x376b90d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x63ce54e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x376b915d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x3e326d310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x4c7684450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x3e326d900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x86e158c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x63cb568f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x3e326dee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x3e326ed00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x4c7684d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x4c76853a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x4c7685760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x63cc11e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x377d455c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x63cb57210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x3e326f2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x86e1594c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x3e326f560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x33a009ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x377d45880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x63cb574d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x3e326f910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x63cb57860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x376b91890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x63ce55560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x33a00a360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x63ce559f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x63cb58120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x3e3270070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x3e324a4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x376b91f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x86e159b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x376b92dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x376b93450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x376b93b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x63cbb3860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x4c7685a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x4c7685f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x4c7686590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x4c7687010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x376b93ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x376b94480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x33a00a9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x3e324a7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x63cbb3c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x86e159e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x63ce56590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x86e15a110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x376b94740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x63ce56ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x377d460c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x376b94ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x3e324aa60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x63ce56e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x4c7687590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x86e15a3d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x33a00b250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x63cbb4860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x376b95240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x377d46860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x4c7687c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x63cbb4b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x63cbb4f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x63cbb54e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x3e324aff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x377d46f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x63ce57120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x63cc134f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x63ce57430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x112212270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x112212d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x377d475e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x63cc12560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x377d478a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x377d47df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x86e15abb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x63ce576f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x377d48c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x377d48f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x377d491f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x17a40eb20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x86e15af90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x377d495f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x86e15b320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x4c74828f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x63ce57e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x63cc10ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x63cc0ff50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x63cc0d5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x377d49bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x86e15cdd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x63cbb5c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x63cbb6910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x4c7688360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x4c7688620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x63cbb6bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x63ce58b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x4c748a2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x377d4a4f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x377d4ac90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x4c7481730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x4c7487c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x377d4b9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x376b95500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x3e324bf20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x4c7689140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x3e3249980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x86e15c170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x376b957c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x376b95fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x4c7689900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x376b97680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x4c7689f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x86e15d5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x112213b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x4c7487590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x3e324ba90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x4c7487f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x376b98710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x3e324c3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x63cbb73c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x376b98d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x4c768a640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x376b98240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x3e3245160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x63cbb7a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x4c768acd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x63ce59760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x4c768b350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x3e3245830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x4c768ba20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x376b999e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x3e3245b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x63cbb7d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x377d4b4c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x376b9a0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x33a00baf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x376b9a960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x63cbb8210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x4c768bd50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x33a00c3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x33a00ce70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x3e3246160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x3e3246f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x4c768cb10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x3e32472c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x63cbb8bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x4c768ced0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x63cbb91f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x4c768d190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x63cbb9790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x63cbb9d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x3e3247580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x4c768d520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x4c768d950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x376b96800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x4c768de40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x3e3247b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x63cbba020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x3e3248720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x63cbba9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x63cbba560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x63cbbac60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x376b96ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x33a00d880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x4c768e460 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init:        CPU KV buffer size =   112.00 MiB\n",
      "llama_init_from_model: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   304.00 MiB\n",
      "llama_init_from_model: graph nodes  = 986\n",
      "llama_init_from_model: graph splits = 450 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/Orion-zhen/Qwen2.5-7B-Instruct-Uncensored', 'mradermacher.quantized_at': '2024-10-11T20:15:47+02:00', 'mradermacher.quantize_version': '2', 'general.url': 'https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF', 'tokenizer.chat_template': \"{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% endif %}{% if system_message is defined %}{{ '<|im_start|>system\\n' + system_message + '<|im_end|>\\n' }}{% endif %}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + content + '<|im_end|>\\n<|im_start|>assistant\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\n' }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.basename': 'Qwen2.5', 'qwen2.attention.head_count_kv': '4', 'general.size_label': '7B', 'general.base_model.0.name': 'Qwen2.5 7B Instruct', 'qwen2.embedding_length': '3584', 'qwen2.context_length': '32768', 'qwen2.block_count': '28', 'general.base_model.0.organization': 'Qwen', 'tokenizer.ggml.pre': 'qwen2', 'general.base_model.count': '1', 'qwen2.rope.freq_base': '1000000.000000', 'general.quantization_version': '2', 'general.license': 'gpl-3.0', 'general.base_model.0.repo_url': 'https://huggingface.co/Qwen/Qwen2.5-7B-Instruct', 'general.file_type': '15', 'general.finetune': 'Instruct-Uncensored', 'general.name': 'Qwen2.5 7B Instruct Uncensored', 'qwen2.feed_forward_length': '18944', 'mradermacher.quantized_by': 'mradermacher', 'general.architecture': 'qwen2', 'mradermacher.quantized_on': 'db3', 'qwen2.attention.head_count': '28', 'tokenizer.ggml.bos_token_id': '151643', 'general.type': 'model', 'tokenizer.ggml.model': 'gpt2'}\n",
      "Available chat formats from metadata: chat_template.default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 1 (Start: 1.448, End: 15.18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   154 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5247.00 ms /   240 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 65 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'fuel and EVAP system', 'predicate': 'purpose', 'object': 'prevent harmful vapors from entering the atmosphere causing smog'}, {'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side'}]}\n",
      "\n",
      "Processing chunk 2 (Start: 15.2, End: 28.755)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   163 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7965.35 ms /   228 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 35 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'canister', 'predicate': 'is_held_in_by', 'object': 'two 12mm pieces of rust'}, {'subject': 'vacuum_hoses', 'predicate': 'have_been_disconnected', 'object': 'from this side of the EVAP canister'}, {'subject': 'electrical_connectors', 'predicate': 'have_been_disconnected', 'object': 'from this side of the EVAP canister'}, {'subject': 'EVAP_canister', 'predicate': 'can_be_removed', 'object': 'from above the subframe'}]}\n",
      "\n",
      "Processing chunk 3 (Start: 29.356, End: 35.579)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5190.01 ms /   135 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 35 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'gas tank', 'predicate': 'location', 'object': 'underneath the rear seat of the vehicle'}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'big steel'}, {'subject': 'gas tank', 'predicate': 'fixing', 'object': 'two straps'}]}\n",
      "\n",
      "Processing chunk 4 (Start: 35.679, End: 41.803)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    35 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4096.83 ms /   110 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 468 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'thing', 'predicate': 'expected_to_snap', 'object': 'soon'}, {'subject': 'two_14mm_bolts', 'predicate': 'hold', 'object': 'strap_to_gas_tank'}]}\n",
      "\n",
      "Processing chunk 5 (Start: 43.564, End: 148.419)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12886.34 ms /   697 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 147 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'fuel filler neck', 'predicate': 'held in by', 'object': '14mm bolt'}, {'subject': 'purge line', 'predicate': 'vent harmful fumes', 'object': 'to engine'}, {'subject': 'air inlet line', 'predicate': 'comes from', 'object': 'air filter'}, {'subject': 'fuel tank', 'predicate': 'has', 'object': 'evap canister'}, {'subject': 'evap canister', 'predicate': 'has', 'object': 'two lines'}, {'subject': 'onboard recovery valve', 'predicate': 'ventilating gas tank', 'object': 'to EVAP canister'}, {'subject': 'vapor pressure sensor', 'predicate': 'take reading from', 'object': 'fuel tank'}]}\n",
      "\n",
      "Processing chunk 6 (Start: 148.639, End: 180.042)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   147 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   247 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11750.70 ms /   394 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 22 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'fill check valve', 'predicate': 'allows', 'object': 'excess vapor to pass into the canister'}, {'subject': 'air drain valve', 'predicate': 'drains', 'object': 'excess air pressure'}, {'subject': 'vapor pressure sensor', 'predicate': 'senses', 'object': 'pressure in the tank'}, {'subject': 'vacuum switching valve', 'predicate': 'works with', 'object': 'air inlet to draw a vacuum on the entire system'}, {'subject': 'ECU', 'predicate': 'uses', 'object': 'vacuum switching valve and air inlet to monitor pressure in the system'}, {'subject': 'ECU', 'predicate': 'tests', 'object': 'for leaks in the system'}, {'subject': 'check engine light', 'predicate': 'indicates', 'object': 'problems in the system'}]}\n",
      "\n",
      "Processing chunk 7 (Start: 180.282, End: 183.305)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    22 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    28 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1563.96 ms /    50 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 51 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'tank side', 'predicate': 'disconnect', 'object': 'two ventilation hoses'}]}\n",
      "\n",
      "Processing chunk 8 (Start: 185.836, End: 194.882)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    51 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3520.88 ms /   110 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 118 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'EVAP line', 'predicate': 'is held on by', 'object': 'four pieces of rust'}, {'subject': 'EVAP check valve', 'predicate': 'has EVAP line removed from', 'object': 'it'}]}\n",
      "\n",
      "Processing chunk 9 (Start: 196.161, End: 222.294)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   118 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5595.69 ms /   218 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 16 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'check valve', 'predicate': 'needs removal', 'object': 'six pieces of rust'}, {'subject': 'evap float', 'predicate': 'prevents', 'object': 'liquid gasoline'}, {'subject': 'evap float', 'predicate': 'locks off', 'object': 'middle valve'}]}\n",
      "\n",
      "Processing chunk 10 (Start: 222.314, End: 223.915)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    16 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     771.34 ms /    24 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 179 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': []}\n",
      "\n",
      "Processing chunk 11 (Start: 224.015, End: 264.245)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   179 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   193 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10015.41 ms /   372 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 15 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'fill check valve', 'predicate': 'allows escape of vapor', 'object': 'when gas tank is rapidly filling up with vapor'}, {'subject': 'fill check valve', 'predicate': 'opens valve', 'object': 'inside canister'}, {'subject': 'vapor', 'predicate': 'enters', 'object': 'canister'}, {'subject': 'remaining pressure', 'predicate': 'exits', 'object': 'out here'}, {'subject': 'canister', 'predicate': 'removes', 'object': 'harmful gases'}, {'subject': 'canister', 'predicate': 'contains', 'object': 'charcoal'}]}\n",
      "\n",
      "Processing chunk 12 (Start: 264.265, End: 266.767)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    15 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     794.02 ms /    23 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 64 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': []}\n",
      "\n",
      "Processing chunk 13 (Start: 266.787, End: 280.936)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5525.07 ms /   166 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 107 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'float', 'predicate': 'prevents', 'object': 'liquid from going inside your charcoal canister'}, {'subject': 'fuel tank', 'predicate': 'full', 'object': 'float will rise'}, {'subject': 'float', 'predicate': 'seals off', 'object': 'vapor ventilation'}]}\n",
      "\n",
      "Processing chunk 14 (Start: 281.796, End: 303.949)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   107 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   222 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10923.73 ms /   329 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 81 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'fuel check valve', 'predicate': 'contains', 'object': 'lid'}, {'subject': 'lid', 'predicate': 'exposes', 'object': 'diaphragm'}, {'subject': 'diaphragm', 'predicate': 'can move', 'object': 'up and down'}, {'subject': 'atmospheric pressure', 'predicate': 'allows', 'object': 'vapor to move'}, {'subject': 'vapor', 'predicate': 'moves into', 'object': 'charcoal canister'}, {'subject': 'sealed fuel cap', 'predicate': 'prevents', 'object': 'vapors from escaping'}, {'subject': 'gas tank', 'predicate': 'will be opened', 'object': \"to see what's inside\"}]}\n",
      "\n",
      "Processing chunk 15 (Start: 308.171, End: 325.335)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   184 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9387.58 ms /   265 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 185 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'gas tank', 'predicate': 'construction material', 'object': 'sheet metal'}, {'subject': 'gas tank', 'predicate': 'construction method', 'object': 'stamping'}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves'}, {'subject': 'gas tank', 'predicate': 'joining method', 'object': 'welding'}, {'subject': 'gas tank', 'predicate': 'internal feature', 'object': 'baffles'}, {'subject': 'baffles', 'predicate': 'location', 'object': 'perimeter of the tank'}]}\n",
      "\n",
      "Processing chunk 16 (Start: 325.575, End: 365.911)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   270 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   13254.05 ms /   455 tokens\n",
      "Llama.generate: 83 prefix-match hit, remaining 50 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'baffle', 'predicate': 'prevents', 'object': 'fuel sloshing'}, {'subject': 'fuel pump', 'predicate': 'is located', 'object': 'at the lowest part of the tank'}, {'subject': 'evap check valve', 'predicate': 'is located', 'object': 'at the highest part of the tank'}, {'subject': 'fuel filler neck', 'predicate': 'contains', 'object': 'a one-way check valve'}, {'subject': 'check valve', 'predicate': 'allows fluid entry', 'object': 'into the tank'}, {'subject': 'check valve', 'predicate': 'prevents fluid exit', 'object': 'from the filler neck'}, {'subject': 'maximum fuel level', 'predicate': 'is limited by', 'object': 'the height of the fuel fill check valve'}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation'}]}\n",
      "\n",
      "Processing chunk 17 (Start: 367.711, End: 378.521)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    50 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   130 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6295.48 ms /   180 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 39 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'gas filler neck', 'predicate': 'is part of', 'object': 'fuel system'}, {'subject': 'gas cap', 'predicate': 'is sealed', 'object': 'on newer OBD2 cars'}, {'subject': 'gas cap', 'predicate': 'forms part of', 'object': 'sealed system'}, {'subject': 'sealed system', 'predicate': 'includes', 'object': 'gas tank'}]}\n",
      "\n",
      "Processing chunk 18 (Start: 378.841, End: 385.745)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    39 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3896.96 ms /   107 tokens\n",
      "Llama.generate: 84 prefix-match hit, remaining 11 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'check valve', 'predicate': 'is present in', 'object': 'here'}, {'subject': 'filter', 'predicate': 'is present in', 'object': 'here'}, {'subject': 'spring', 'predicate': 'is present in', 'object': 'here'}]}\n",
      "\n",
      "Processing chunk 19 (Start: 386.586, End: 387.967)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    27 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1328.32 ms /    38 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 11 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'valve', 'predicate': 'is', 'object': 'itself'}]}\n",
      "\n",
      "Processing chunk 20 (Start: 388.627, End: 389.808)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    11 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     503.72 ms /    19 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 145 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': []}\n",
      "\n",
      "Processing chunk 21 (Start: 390.148, End: 420.44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   145 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   198 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9749.41 ms /   343 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 78 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'Gas cap', 'predicate': 'usually sealed', 'object': 'in normal situations'}, {'subject': 'Safety valve', 'predicate': 'exists', 'object': 'in case of high pressure'}, {'subject': 'Filler neck', 'predicate': 'contains', 'object': 'hollow tube'}, {'subject': 'Filler port', 'predicate': 'has', 'object': 'plastic collar with a flap'}, {'subject': 'Fuel tank vapor pressure sensor', 'predicate': 'measures', 'object': 'pressure differential between tank and atmospheric pressure'}, {'subject': 'Fuel tank vapor pressure sensor', 'predicate': 'has', 'object': '3 terminals'}]}\n",
      "\n",
      "Processing chunk 22 (Start: 420.46, End: 435.548)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5557.11 ms /   185 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 164 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'ECU', 'predicate': 'uses', 'object': 'voltage from middle terminal'}, {'subject': 'voltage from middle terminal', 'predicate': 'determines', 'object': 'EVAP system leaks'}, {'subject': 'vacuum switching valve', 'predicate': 'is being', 'object': 'broken rust off'}]}\n",
      "\n",
      "Processing chunk 23 (Start: 435.808, End: 468.877)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   164 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   224 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10730.82 ms /   388 tokens\n",
      "Llama.generate: 85 prefix-match hit, remaining 59 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'vacuum switching valve', 'predicate': 'functions as', 'object': 'redirecting air flow between gas tank and charcoal canister'}, {'subject': 'vacuum switching valve', 'predicate': 'activated by', 'object': 'application of 12 volts'}, {'subject': 'vacuum switching valve', 'predicate': 'operates through', 'object': 'a plunger moving outward or inward'}, {'subject': 'vacuum switching valve', 'predicate': 'allows', 'object': 'air flow from gas tank to charcoal canister'}, {'subject': 'vacuum switching valve', 'predicate': 'blocks', 'object': 'air flow to charcoal canister'}, {'subject': 'vacuum switching valve', 'predicate': 'clicks when', 'object': 'air flow direction is changed'}]}\n",
      "\n",
      "Processing chunk 24 (Start: 469.037, End: 482.005)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6781.64 ms /   201 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 42 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors'}, {'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 'absorb gasoline vapors'}, {'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 're-release gasoline vapors'}, {'subject': 'check valves', 'predicate': 'function', 'object': 'control flow of gasoline vapors'}]}\n",
      "\n",
      "Processing chunk 25 (Start: 482.045, End: 489.831)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    42 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5628.41 ms /   152 tokens\n",
      "Llama.generate: 82 prefix-match hit, remaining 29 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'one-way valves', 'predicate': 'allow', 'object': 'air to go in'}, {'subject': 'one-way valves', 'predicate': 'not allow', 'object': 'air to come out'}, {'subject': 'one-way valves', 'predicate': 'essential for', 'object': 'running the self-diagnostic for the EVAP system'}]}\n",
      "\n",
      "Processing chunk 26 (Start: 489.851, End: 494.159)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    28 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1609.65 ms /    57 tokens\n",
      "Llama.generate: 85 prefix-match hit, remaining 171 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'pulling out', 'predicate': 'action', 'object': 'check valves'}]}\n",
      "\n",
      "Processing chunk 27 (Start: 496.587, End: 534.03)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   171 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   239 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11582.22 ms /   410 tokens\n",
      "Llama.generate: 83 prefix-match hit, remaining 97 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'Check valve', 'predicate': 'acts as', 'object': 'a valve that releases air when a certain pressure is reached'}, {'subject': 'Diaphragm', 'predicate': 'functions as', 'object': 'a component that rises when air pressure builds up'}, {'subject': 'Ball check valve', 'predicate': 'is part of', 'object': 'the check valve'}, {'subject': 'Air pressure', 'predicate': 'triggers', 'object': 'the release of air'}, {'subject': 'Air', 'predicate': 'can escape', 'object': 'through two ports on the side'}, {'subject': 'Air', 'predicate': 'cannot enter', 'object': 'the canister from this side'}, {'subject': 'Paper filter', 'predicate': 'is part of', 'object': 'the charcoal canister'}]}\n",
      "\n",
      "Processing chunk 28 (Start: 534.611, End: 554.608)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   130 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6684.32 ms /   227 tokens\n",
      "Llama.generate: 83 prefix-match hit, remaining 126 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'Activated charcoal', 'predicate': 'Absorbs', 'object': 'Gasoline vapors'}, {'subject': 'Activated charcoal', 'predicate': 'Smells like', 'object': 'Gasoline'}, {'subject': 'Activated charcoal', 'predicate': 'Looks like', 'object': 'Small pieces of rat shit'}, {'subject': 'Activated charcoal', 'predicate': 'Amount', 'object': 'A lot'}]}\n",
      "\n",
      "Processing chunk 29 (Start: 554.708, End: 579.948)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1721.96 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   126 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   175 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9012.90 ms /   301 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted knowledge from a chunk: {'valuable_knowledge': [{'subject': 'spring with the pad', 'predicate': 'applies pressure', 'object': 'charcoal'}, {'subject': 'charcoal', 'predicate': 'becomes compact', 'object': 'due to pressure'}, {'subject': 'EVAP system', 'predicate': 'has two chambers', 'object': 'with a wall separating them'}, {'subject': 'vapors', 'predicate': 'enter from one side', 'object': 'and exit from the other side'}, {'subject': 'vapors', 'predicate': 'have maximum surface area', 'object': 'absorbed by charcoal'}]}\n",
      "\n",
      "Knowledge extraction complete.\n",
      "Please check 'knowledge.json' for the extracted valuable knowledge.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import threading\n",
    "from llama_cpp import Llama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from json import JSONDecodeError\n",
    "\n",
    "# -------------------------------\n",
    "# The Chatbot and LlamaSingleton\n",
    "# -------------------------------\n",
    "\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/Qwen2.5-7B-Instruct-Uncensored.Q4_K_M.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=2048)\n",
    "            return cls._instance\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, \n",
    "                 messages_file='messages.json', \n",
    "                 knowledge_file='knowledge.json', \n",
    "                 faiss_index_file='faiss_index.pkl',\n",
    "                 model_name='all-MiniLM-L6-v2'):\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "        self.llm = LlamaSingleton().llm\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.knowledge_data = []\n",
    "        self.initialize_files()\n",
    "        self.load_faiss_index()\n",
    "\n",
    "    def initialize_files(self):\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def save_json_data(self, file_path, data):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "    def extract_valuable_knowledge(self, message):\n",
    "        \"\"\"\n",
    "        Sends the chunk text to the model and asks it to return JSON with\n",
    "        subject/predicate/object. No timestamps are generated by the model.\n",
    "        \"\"\"\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are a knowledge extractor. Try to extract any knowledge from the user.\\n\"\n",
    "                        \"Return ONLY JSON with the following schema:\\n\"\n",
    "                        \"{\\n\"\n",
    "                        \"  \\\"valuable_knowledge\\\": [\\n\"\n",
    "                        \"    {\\n\"\n",
    "                        \"      \\\"subject\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"predicate\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"object\\\": \\\"...\\\"\\n\"\n",
    "                        \"    }\\n\"\n",
    "                        \"  ]\\n\"\n",
    "                        \"}\\n\"\n",
    "                        \"If no knowledge can be extracted, return:\\n\"\n",
    "                        \"{\\\"valuable_knowledge\\\": []}\"\n",
    "                    )\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"valuable_knowledge\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"subject\": {\"type\": \"string\"},\n",
    "                                    \"predicate\": {\"type\": \"string\"},\n",
    "                                    \"object\": {\"type\": \"string\"}\n",
    "                                },\n",
    "                                \"required\": [\"subject\", \"predicate\", \"object\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"valuable_knowledge\"],\n",
    "                },\n",
    "            },\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        try:\n",
    "            knowledge_data = json.loads(response['choices'][0]['message']['content'])\n",
    "            print(\"Extracted knowledge from a chunk:\", knowledge_data)\n",
    "            if \"valuable_knowledge\" not in knowledge_data:\n",
    "                knowledge_data[\"valuable_knowledge\"] = []\n",
    "            return knowledge_data[\"valuable_knowledge\"]\n",
    "        except (JSONDecodeError, KeyError):\n",
    "            return []\n",
    "\n",
    "    def save_knowledge(self, triplets):\n",
    "        \"\"\"\n",
    "        Persists triplets to `knowledge.json` and updates FAISS index if new triplets\n",
    "        are found. We do not add any timestamps here.\n",
    "        \"\"\"\n",
    "        if not triplets:\n",
    "            return\n",
    "        knowledge = self.load_json_data(self.knowledge_file)\n",
    "        existing_set = {(t['subject'], t['predicate'], t['object']) for t in knowledge}\n",
    "        new_triplets = []\n",
    "        for triplet in triplets:\n",
    "            key = (triplet['subject'], triplet['predicate'], triplet['object'])\n",
    "            if key not in existing_set:\n",
    "                knowledge.append(triplet)\n",
    "                new_triplets.append(triplet)\n",
    "                existing_set.add(key)\n",
    "        self.save_json_data(self.knowledge_file, knowledge)\n",
    "        if new_triplets:\n",
    "            self.update_faiss_index(new_triplets)\n",
    "\n",
    "    def update_faiss_index(self, triplets):\n",
    "        texts = [f\"{t['subject']} {t['predicate']} {t['object']}\" for t in triplets]\n",
    "        embeddings = self.model.encode(texts)\n",
    "        if self.index is None:\n",
    "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(np.array(embeddings, dtype=np.float32))\n",
    "        self.knowledge_data.extend(triplets)\n",
    "        self.save_faiss_index()\n",
    "\n",
    "    def save_faiss_index(self):\n",
    "        with open(self.faiss_index_file, 'wb') as f:\n",
    "            pickle.dump((self.index, self.knowledge_data), f)\n",
    "\n",
    "    def load_faiss_index(self):\n",
    "        if os.path.exists(self.faiss_index_file):\n",
    "            with open(self.faiss_index_file, 'rb') as f:\n",
    "                self.index, self.knowledge_data = pickle.load(f)\n",
    "        else:\n",
    "            self.index = None\n",
    "            self.knowledge_data = []\n",
    "\n",
    "# -------------------------------\n",
    "# Main Processing: Load Chunks and Extract Knowledge\n",
    "# -------------------------------\n",
    "\n",
    "def main():\n",
    "    # 1. Load the chunks from output_chunks.json\n",
    "    with open('output_chunks.json', 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    chunks = data.get(\"chunks\", [])\n",
    "    print(f\"Total chunks loaded: {len(chunks)}\")\n",
    "\n",
    "    # 2. Extract Knowledge from Each Chunk\n",
    "    chatbot = Chatbot()\n",
    "\n",
    "    for i, chunk in enumerate(chunks, start=1):\n",
    "        text = chunk.get(\"text\", \"\")\n",
    "        start_time = chunk.get(\"start\")\n",
    "        end_time = chunk.get(\"end\")\n",
    "\n",
    "        print(f\"\\nProcessing chunk {i} (Start: {start_time}, End: {end_time})\")\n",
    "\n",
    "        # Extract valuable knowledge from the chunk text\n",
    "        extracted_knowledge = chatbot.extract_valuable_knowledge(text)\n",
    "\n",
    "        if extracted_knowledge:\n",
    "            # Attach the chunk's start/end to each extracted item\n",
    "            for triplet in extracted_knowledge:\n",
    "                triplet['start'] = start_time\n",
    "                triplet['end'] = end_time\n",
    "\n",
    "            # Save the extracted knowledge\n",
    "            chatbot.save_knowledge(extracted_knowledge)\n",
    "\n",
    "    print(\"\\nKnowledge extraction complete.\")\n",
    "    print(\"Please check 'knowledge.json' for the extracted valuable knowledge.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST door te chatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M3 Max) - 25280 MiB free\n",
      "llama_model_loader: loaded meta data with 41 key-value pairs and 339 tensors from models/Qwen2.5-7B-Instruct-Uncensored.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct Uncensored\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct-Uncensored\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   6:                            general.license str              = gpl-3.0\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Qwen2.5 7B Instruct\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Qwen\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7...\n",
      "llama_model_loader: - kv  11:                               general.tags arr[str,3]       = [\"qwen\", \"uncensored\", \"text-generati...\n",
      "llama_model_loader: - kv  12:                          general.languages arr[str,2]       = [\"zh\", \"en\"]\n",
      "llama_model_loader: - kv  13:                           general.datasets arr[str,5]       = [\"NobodyExistsOnTheInternet/ToxicQAFi...\n",
      "llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584\n",
      "llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944\n",
      "llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28\n",
      "llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  22:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {% set system_message = 'You are a he...\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  34:                                general.url str              = https://huggingface.co/mradermacher/Q...\n",
      "llama_model_loader: - kv  35:              mradermacher.quantize_version str              = 2\n",
      "llama_model_loader: - kv  36:                  mradermacher.quantized_by str              = mradermacher\n",
      "llama_model_loader: - kv  37:                  mradermacher.quantized_at str              = 2024-10-11T20:15:47+02:00\n",
      "llama_model_loader: - kv  38:                  mradermacher.quantized_on str              = db3\n",
      "llama_model_loader: - kv  39:                         general.source.url str              = https://huggingface.co/Orion-zhen/Qwe...\n",
      "llama_model_loader: - kv  40:                  mradermacher.convert_type str              = hf\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type q4_K:  169 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.36 GiB (4.91 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 3584\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 28\n",
      "print_info: n_head_kv        = 4\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 7\n",
      "print_info: n_embd_k_gqa     = 512\n",
      "print_info: n_embd_v_gqa     = 512\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 18944\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.62 B\n",
      "print_info: general.name     = Qwen2.5 7B Instruct Uncensored\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 152064\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 148848 'ÄĬ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 338 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/29 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  4460.45 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 28991.03 MB\n",
      "ggml_metal_init: loaded kernel_add                                   0x7921e49a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                               0x791fd88550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                   0x767fa2fc50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                               0x791fd88b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                   0x728d088c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                               0x767fa2f6f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                   0x767ee64cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                               0x728d2f35d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                            0x7921e4b330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                            0x73d6a07120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                            0x7921e4b920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                            0x73d604ec90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                 0x767ee65ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                               0x73d6a076f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                 0x73d604f0f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                  0x767ee66630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                  0x767ee670c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                               0x73d604f7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                  0x791fba0200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                0x73d6a079b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                            0x767ee67930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                          0x7921e4a1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                  0x791fd88ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                0x7921e4a4a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                   0x791fb9e380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                          0x791fd88020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                        0x791fba1070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                          0x728d2f39a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                        0x767ee67bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                         0x728d2f2b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                       0x791fd878d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                          0x767fa2eb80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                          0x767fa300a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                         0x767fa310c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                         0x767ee68220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                         0x73d6a064e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                         0x791fba08d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                         0x728d2f3d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                         0x791fd8ac90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                         0x728d2f5630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                         0x728d2f4b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                         0x728d2f5c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                         0x767ee68e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                      0x728d2f6a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                       0x728d2f62a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                      0x767fa31550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                        0x767fa31de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                        0x73d60508f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                        0x767ee69450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                        0x767ee69a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                       0x728d2f7070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                       0x73d6a08510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                          0x73d604fb70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                              0x767ee6a0c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                            0x767fa30a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                  0x7921e4cc70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                          0x791fba1980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                          0x791fba1d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                        0x73d6051600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                        0x728d2f75e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                   0x728d2f7cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                     0x767ee6abc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                        0x73d6a08a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                       0x73d6a069e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                       0x73d6051030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                       0x767fa324f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                       0x7921e4cf30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                       0x7921e4dde0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2               0x767fa331d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3               0x73d6a08f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4               0x767fa33f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5               0x791fd89800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2              0x767fa345f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3              0x7921e4e0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4              0x767ee6ae80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5              0x767fa34c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2              0x728d2f8500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3              0x791fd8be20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4              0x728d2f9260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5              0x728d2f9910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2              0x73d60523f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3              0x728d2f9f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4              0x728d2fa610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5              0x728ce50c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2              0x728ce50f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3              0x767ee6b140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4              0x728d2fa8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5              0x73d6a09c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2              0x73d6a094a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3              0x73d6a0a980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4              0x73d6051b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5              0x728ce51210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2              0x73d6a0ac40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3              0x73d6a0b110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4              0x73d6a0ba40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5              0x791fba2dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2              0x73d6a0c110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3              0x728ce51c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4              0x728ce514d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5              0x73d6793110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2              0x73d6053210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3              0x728d2fab90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4              0x73d6a0c9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5              0x791fd8c680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2            0x791fd8d3d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3            0x791fd8d690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4            0x73d6a0d2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5            0x73d60537b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                       0x73d6053df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                       0x767fa34ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                       0x73d6793410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                       0x73d6054240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                       0x73d6a0d680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                    0x73d6054b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                     0x728ce51790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                    0x73d6794950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                      0x791fd8d950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                      0x73d6a0df20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                      0x73d6a0e1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                      0x73d6a0e4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                     0x73d6a0eb80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                     0x791fd8dc10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                     0x73d6a0ee40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                     0x73d6a0f100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                    0x791fba2660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                    0x7921e4e880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                    0x791fd8e830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                    0x73d6a0f3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                    0x73d6054e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                    0x767fa35190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                    0x767fa358c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                    0x791fd8edd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                    0x767ee6bc30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                    0x791fd8f630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                 0x728d2fb050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                  0x767fa36010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                 0x728d2fbed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                   0x767fa366c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                   0x728d2fc580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                   0x7921e4f7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                   0x791fd8fce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                  0x728d2fcbd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                  0x73d6a0fa60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                        0x728d2fcf90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                        0x791fd900a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                       0x728d2fd530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                       0x728d2fdad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                       0x7921e4ffa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x17a92c270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                       0x728d2fdd90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                       0x767ee6b440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                       0x728d2fe190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                       0x767fa36d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                       0x7921e50400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                       0x728d2fe6e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                    0x767fa37310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                     0x7921e51690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                    0x7921e51d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                      0x7921e52010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                      0x767fa380c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                      0x728d2fecf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                      0x7921e52650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                     0x767ee6c890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                     0x767ee6d660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                     0x767ee6d920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                     0x767fa38740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                    0x767fa38a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                    0x767ee6dd10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                    0x791fd90db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                    0x767fa38fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                    0x73d6055550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                    0x791fd903e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                    0x728ce52870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                    0x73d6055ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                    0x767fa39840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                    0x767fa39ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                 0x7921e53060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                  0x73d6056360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                 0x7921e50e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                   0x73d6056620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                   0x767ee6e490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                   0x767fa3a590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                   0x791fd91ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                  0x767ee6ec20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                  0x767fa3ac10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                         0x728d2ff450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                         0x767ee6f5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                         0x7921e53640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                         0x728d2ff710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                            0x791fd91da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                            0x791fd927a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                        0x767ee6f890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                        0x7921e53be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32             0x728d2ffaa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32             0x728d2d3d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                           0x7921e54c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                               0x791fd92c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                    0x73d6a101d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                0x73d6a10df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                            0x73d6a11370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                   0x767ee70280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                  0x7921e55160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                        0x791fd93820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                0x767ee70ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                0x767ee70f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                0x7921e558b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112               0x791fd93bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128               0x767fa3b3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256               0x728d2d42c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64               0x728d2d4950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80               0x767ee71770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96               0x791fd94320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112              0x791fd94c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128              0x7921e56440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256              0x728d2d4c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64               0x791fba2020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80               0x791fd952f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96               0x7921e56770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112              0x728d2d4ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128              0x791fd95970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256              0x791fd96010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64               0x767ee71b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80               0x767ee725a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96               0x73d6a11630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112              0x7921e56e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128              0x73d6a11a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256              0x791fd966a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64               0x791fd96d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80               0x7921e57220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96               0x791fd973b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112              0x767ee72c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128              0x767ee732d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256              0x73d6a12580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64               0x767ee73680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80               0x791fd97b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96               0x791fd97fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112              0x767fa3b680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128              0x791fd92270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256              0x7921e583d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128           0x73d6a12900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128          0x791fd988d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128          0x7921e58a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128          0x767fa3bfb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128          0x767ee73a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128          0x7921e59040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256           0x767ee74840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256          0x728ce535c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256          0x791fd98f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256          0x7921e596e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256          0x7921e599a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256          0x7921e5a3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                               0x7921e4f040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                               0x7921e544f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                           0x73d6a12bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                           0x791fd99320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                           0x73d60568e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                           0x7921e5aa20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                          0x767fa3c370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                          0x728d2d5690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                          0x791fd99720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1122c81c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                          0x791fba37b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                        0x73d6794c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                0x73d60570c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                   0x73d6a13640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                  0x73d6057510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                   0x728ce52f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                   0x791fba4e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                              0x728ce54680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                0x73d60577d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                       0x767ee74e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                       0x767fa3c630 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init:        CPU KV buffer size =   112.00 MiB\n",
      "llama_init_from_model: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   304.00 MiB\n",
      "llama_init_from_model: graph nodes  = 986\n",
      "llama_init_from_model: graph splits = 450 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/Orion-zhen/Qwen2.5-7B-Instruct-Uncensored', 'mradermacher.quantized_at': '2024-10-11T20:15:47+02:00', 'mradermacher.quantize_version': '2', 'general.url': 'https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF', 'tokenizer.chat_template': \"{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% endif %}{% if system_message is defined %}{{ '<|im_start|>system\\n' + system_message + '<|im_end|>\\n' }}{% endif %}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + content + '<|im_end|>\\n<|im_start|>assistant\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\n' }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.basename': 'Qwen2.5', 'qwen2.attention.head_count_kv': '4', 'general.size_label': '7B', 'general.base_model.0.name': 'Qwen2.5 7B Instruct', 'qwen2.embedding_length': '3584', 'qwen2.context_length': '32768', 'qwen2.block_count': '28', 'general.base_model.0.organization': 'Qwen', 'tokenizer.ggml.pre': 'qwen2', 'general.base_model.count': '1', 'qwen2.rope.freq_base': '1000000.000000', 'general.quantization_version': '2', 'general.license': 'gpl-3.0', 'general.base_model.0.repo_url': 'https://huggingface.co/Qwen/Qwen2.5-7B-Instruct', 'general.file_type': '15', 'general.finetune': 'Instruct-Uncensored', 'general.name': 'Qwen2.5 7B Instruct Uncensored', 'qwen2.feed_forward_length': '18944', 'mradermacher.quantized_by': 'mradermacher', 'general.architecture': 'qwen2', 'mradermacher.quantized_on': 'db3', 'qwen2.attention.head_count': '28', 'tokenizer.ggml.bos_token_id': '151643', 'general.type': 'model', 'tokenizer.ggml.model': 'gpt2'}\n",
      "Available chat formats from metadata: chat_template.default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/76/kx23hgz16fdcd_cjrpy0jnlm0000gn/T/ipykernel_96649/224684272.py:112: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  message = {\"role\": role, \"content\": content, \"timestamp\": datetime.utcnow().isoformat()}\n",
      "/var/folders/76/kx23hgz16fdcd_cjrpy0jnlm0000gn/T/ipykernel_96649/224684272.py:168: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  current_time = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T13:00:41.143422\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': '\"question\": \"Which component is responsible for sensing the fuel tank\\'s pressure and comparing it with atmospheric pressure?\",       \"options\": [         \"Fuel filler neck\",         \"Vapor pressure sensor\",         \"Vacuum switching valve\",         \"Purge line\"       ],       \"answer\": \"Vapor pressure sensor”'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   130 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   379 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   17312.43 ms /   509 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The component responsible for sensing the fuel tank's pressure and comparing it with atmospheric pressure is the vapor pressure sensor. Here's a detailed explanation:\n",
      "\n",
      "1. **Vapor Pressure Sensor**:\n",
      "   - The vapor pressure sensor is specifically designed to monitor the pressure inside the fuel tank. It can measure the pressure difference between the fuel tank and the atmosphere.\n",
      "   - This sensor is usually integrated into the fuel filler neck, which is the part of the fuel system that connects the fuel pump to the fuel tank. \n",
      "   - The vapor pressure sensor sends information about the fuel tank's pressure to the vehicle's electronic control unit (ECU), which uses this data to adjust the fuel pressure as needed.\n",
      "\n",
      "2. **Fuel Filler Neck**:\n",
      "   - The fuel filler neck is a component of the fuel system that provides access to the fuel tank. It contains the vapor pressure sensor but is not the sensor itself.\n",
      "   - The fuel filler neck is involved in fueling the vehicle and venting the fuel tank but is not the primary component for pressure sensing.\n",
      "\n",
      "3. **Vacuum Switching Valve**:\n",
      "   - The vacuum switching valve is a component that helps control the vacuum in the fuel system. It switches between different vacuum sources as needed.\n",
      "   - While it plays a crucial role in the fuel system's operation, it is not primarily responsible for sensing the fuel tank's pressure.\n",
      "\n",
      "4. **Purge Line**:\n",
      "   - The purge line is a component of the fuel vapor recovery system that allows fuel vapors to be drawn from the fuel tank and sent to the engine for combustion.\n",
      "   - It is not involved in pressure sensing.\n",
      "\n",
      "In summary, the vapor pressure sensor is the component responsible for sensing the fuel tank's pressure and comparing it with atmospheric pressure. It is part of the fuel filler neck, but it is distinct from other components like the vacuum switching valve and the purge line.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T13:04:48.658676\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': '\"question\": \"Which component is responsible for sensing the fuel tank\\'s pressure and comparing it with atmospheric pressure?\",       \"options\": [         \"Fuel filler neck\",         \"Vapor pressure sensor\",         \"Vacuum switching valve\",         \"Purge line\"       ],'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 31 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   224 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10565.49 ms /   313 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The component responsible for sensing the fuel tank's pressure and comparing it with atmospheric pressure is the vapor pressure sensor.\n",
      "\n",
      "Here's an explanation of each option:\n",
      "\n",
      "1. **Fuel filler neck**: This is the part of the fuel system where fuel is added to the vehicle. It is not involved in the sensing of fuel tank pressure.\n",
      "\n",
      "2. **Vapor pressure sensor**: This component is specifically designed to sense the pressure inside the fuel tank and compare it with atmospheric pressure. It is typically part of the fuel vapor recovery system and is essential for maintaining the correct pressure balance in the fuel tank.\n",
      "\n",
      "3. **Vacuum switching valve**: This component is used to manage the flow of fuel vapors between the fuel tank and the charcoal canister. It does not directly sense the pressure of the fuel tank.\n",
      "\n",
      "4. **Purge line**: This is a part of the fuel vapor recovery system that allows the fuel vapor from the charcoal canister to be vented to the fuel tank when the engine is running. It does not directly measure the pressure of the fuel tank.\n",
      "\n",
      "Therefore, the correct answer is \"Vapor pressure sensor\".\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T13:06:53.452672\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': ' \"question\": \"What role does the vacuum switching valve play in the EVAP system?\",       \"options\": [         \"It directly filters fuel vapors\",         \"It directs air flow within the system when 12 volts is applied\",         \"It secures the EVAP canister in place\",         \"It measures the temperature of the fuel\"       ],'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 31 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   108 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   408 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18074.80 ms /   516 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The vacuum switching valve plays a key role in the EVAP (Evaporative Emission Control) system. The correct answer is: \"It directs air flow within the system when 12 volts is applied\". Here's a detailed explanation:\n",
      "\n",
      "1. **Function of the EVAP System**: The purpose of the EVAP system is to contain and recover fuel vapors that are generated in the fuel tank and fuel lines. It does this by sealing the fuel vapor collection system from the atmosphere and recycling the vapors back to the fuel tank to be burned.\n",
      "\n",
      "2. **Role of the Vacuum Switching Valve**: The vacuum switching valve is part of the purge control system. When the engine is running, it directs air flow into the charcoal canister (which holds the fuel vapors), allowing the vapors to be drawn into the engine and burned. The valve switches between the canister and the atmosphere, directing air flow into the canister when the engine is running and into the atmosphere when it's not.\n",
      "\n",
      "3. **Operation**: The valve's operation is controlled by a vacuum signal. When the engine is running, a vacuum line from the engine manifold provides a vacuum signal to the valve, which then directs the air flow into the canister. When the engine is not running, the vacuum signal is removed, and the valve switches the air flow to the atmosphere. \n",
      "\n",
      "4. **Why the Other Options are Incorrect**:\n",
      "   - \"It directly filters fuel vapors\" - The EVAP system doesn't filter the vapors; it contains and recycles them.\n",
      "   - \"It secures the EVAP canister in place\" - The canister is typically secured by bolts or clips, not the vacuum switching valve.\n",
      "   - \"It measures the temperature of the fuel\" - This is not a function of the vacuum switching valve; temperature measurement is typically done by a separate sensor. \n",
      "\n",
      "This detailed explanation clarifies the role of the vacuum switching valve in the EVAP system and why the other options are not correct.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T13:16:16.758717\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': ' \"question\": \"Which component in the system uses activated charcoal?\",       \"options\": [         \"The fuel filler neck\",         \"The gas tank\",         \"The EVAP canister\",         \"The vapor pressure sensor\"       ],'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 30 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   13845.45 ms /   386 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The component in the system that uses activated charcoal is the EVAP (Evaporative Emission Control) canister.\n",
      "\n",
      "Here's a detailed explanation:\n",
      "\n",
      "1. **Fuel Filler Neck**: This is the opening where you fill the vehicle with fuel. It does not contain activated charcoal. It is primarily designed to allow fuel to enter the tank and maintain a secure seal.\n",
      "\n",
      "2. **Gas Tank**: While the fuel tank is essential for storing fuel, it does not incorporate activated charcoal. The tank's primary function is to contain the fuel and prevent physical damage.\n",
      "\n",
      "3. **EVAP Canister**: This component is specifically designed to use activated charcoal. Its main purpose is to filter and store fuel vapors that might escape from the fuel tank. Activated charcoal is an excellent material for adsorption, which means it can attract and hold onto fuel vapors. The canister is connected to the fuel tank and the intake manifold. When the engine is running, the vacuum from the intake manifold draws fuel vapors from the canister back into the engine for combustion, reducing harmful emissions.\n",
      "\n",
      "4. **Vapor Pressure Sensor**: This sensor is used to monitor the pressure of fuel vapors in the fuel tank and connected lines. It does not contain activated charcoal. Its function is to provide the engine control module (ECM) with data to manage fuel vapor emissions effectively. \n",
      "\n",
      "In summary, the correct answer is the \"EVAP canister,\" which utilizes activated charcoal to manage fuel vapors and enhance emission control.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T13:23:27.577454\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': '   \"question\": \"What is one of the purposes of the baffles inside the gas tank?\",       \"options\": [         \"To increase fuel capacity\",         \"To prevent fuel from sloshing as the vehicle takes corners\",         \"To cool the fuel\",         \"To filter fuel impurities\"       ],'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 30 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   215 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10262.47 ms /   314 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The correct answer is: \"To prevent fuel from sloshing as the vehicle takes corners.\"\n",
      "\n",
      "Explanation:\n",
      "Baffles inside a gas tank are designed to manage the movement of fuel within the tank, particularly as the vehicle accelerates, brakes, or turns. The primary purpose of baffles is to keep the fuel from sloshing, which can lead to uneven fuel distribution and potential fuel starvation in the fuel pump or carburetor.\n",
      "\n",
      "Let's briefly address the other options:\n",
      "\n",
      "1. \"To increase fuel capacity\": Baffles do not increase the total fuel storage capacity of the tank. They simply manage the fuel's movement within the tank.\n",
      "\n",
      "2. \"To cool the fuel\": Baffles are not designed to cool the fuel. The fuel temperature is more influenced by the ambient temperature, the fuel system's design, and the vehicle's overall thermal management.\n",
      "\n",
      "3. \"To filter fuel impurities\": Fuel filters are typically installed in the fuel line between the fuel pump and the engine. Baffles do not filter impurities from the fuel but manage its movement within the tank.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T13:25:13.543183\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': '[{\\'role\\': \\'system\\', \\'content\\': \\'You are a helpful assistent; Current date and time: 2025-02-24T13:23:27.577454\\\\nNo direct related knowledge found. Proceeding with general reasoning.\\\\n\\'}, {\\'role\\': \\'user\\', \\'content\\': \\'   \"question\": \"What is one of the purposes of the baffles inside the gas tank?\",       \"options\": [         \"To increase fuel capacity\",         \"To prevent fuel from sloshing as the vehicle takes corners\",         \"To cool the fuel\",         \"To filter fuel impurities\"       ],\\'}]'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 31 prefix-match hit, remaining 175 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   175 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6493.48 ms /   287 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The baffles inside a gas tank are designed to prevent fuel from sloshing as the vehicle takes corners. Baffles are typically flat or curved plates placed inside the fuel tank to keep the fuel from moving around too much when the vehicle is turning. This is important because if the fuel sloshed around too much, it could potentially cause the fuel pump to suck in air instead of fuel, which could lead to fuel starvation and engine failure. So the correct answer to your question is option B: To prevent fuel from sloshing as the vehicle takes corners.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T13:36:01.718198\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': '\"statement\": \"The EVAP system is designed to reduce harmful vapor emissions by capturing fuel vapors before they escape into the atmosphere.\",       \"answer\": true'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 30 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   390 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   17539.68 ms /   457 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The statement about the EVAP (Evaporative Emission Control) system is correct. The purpose of an EVAP system is indeed to minimize the emission of harmful gases into the air by capturing fuel vapors before they are released from the fuel tank. This system is a crucial part of modern vehicle emission control systems.\n",
      "\n",
      "Here's a more detailed explanation of how an EVAP system works:\n",
      "\n",
      "1. **Fuel Vapor Collection**: The fuel vapor control system collects fuel vapors that are generated in the fuel tank and in the fuel lines. These vapors are a byproduct of the fuel evaporation process.\n",
      "\n",
      "2. **Activated Charcoal Canister**: The fuel vapors are stored in an activated charcoal canister. Activated charcoal has a high adsorption capacity, meaning it can hold a large amount of fuel vapors without getting saturated.\n",
      "\n",
      "3. **Evaporation Emission Control Valve (EVAP Valve)**: This valve regulates the flow of fuel vapor from the canister to the intake manifold, where the vapors are burned along with the fuel-air mixture in the engine. The EVAP valve opens when the engine is running and closes when the engine is off, preventing the release of fuel vapors into the atmosphere.\n",
      "\n",
      "4. **Fuel Vapor Purge**: When the engine is running, the fuel vapor purge system uses a vacuum to draw the fuel vapors from the canister and into the engine for combustion. This process is timed to occur when the engine is operating under conditions where it can safely burn the fuel vapors.\n",
      "\n",
      "5. **Fuel Tank Pressure Regulation**: The system also includes a pressure relief valve that prevents the fuel tank from over-pressurizing. This valve allows excess pressure to be vented to the atmosphere through a charcoal canister.\n",
      "\n",
      "The EVAP system plays a vital role in reducing harmful emissions, contributing to the overall effort to improve air quality and reduce the environmental impact of vehicles. Therefore, the statement is accurate.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T13:36:26.850123\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': '\"statement\": \"The EVAP system is designed to reduce harmful vapor emissions by capturing fuel vapors before they escape into the atmosphere.\",'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 33 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   437 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   19174.18 ms /   496 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The statement is correct. The Evaporative Emission Control System, commonly known as the EVAP system, is a vital component of a vehicle's emission control system. Its primary function is to manage fuel vapors, which are a by-product of the evaporation of fuel in the fuel tank.\n",
      "\n",
      "Here's a more detailed breakdown of the EVAP system and its function:\n",
      "\n",
      "1. **Fuel Tank**: Fuel is stored in the tank, and as it evaporates, fuel vapors are produced. These vapors are less harmful than the actual fuel, but they're still pollutants that need to be controlled.\n",
      "\n",
      "2. **Charcoal Canister**: The EVAP system captures these fuel vapors in a canister filled with activated charcoal. The charcoal has a high adsorption capacity, which means it can hold a large amount of fuel vapors without breaking down.\n",
      "\n",
      "3. **Purge Valves and Control Module**: When the engine is running, the control module sends a signal to the purge valve. This valve opens, allowing the fuel vapors to be drawn from the canister into the engine intake manifold. The vapors are then burned along with the air-fuel mixture in the engine, effectively eliminating the harmful emissions.\n",
      "\n",
      "4. **Evaporative Purge Solenoid**: This component controls the flow of fuel vapors into the engine. It opens and closes as needed, based on the engine's operating conditions.\n",
      "\n",
      "5. **Fuel Tank Cap**: The cap is designed to minimize the amount of fuel vapor that escapes from the fuel tank. It includes a valve that opens when pressure inside the tank exceeds a certain level, allowing the vapors to be directed into the canister.\n",
      "\n",
      "6. **Vacuum Hose**: This hose connects the charcoal canister to the engine intake manifold. It allows the fuel vapors to flow from the canister to the engine during the purge process.\n",
      "\n",
      "In summary, the EVAP system is designed to minimize harmful vapor emissions by capturing fuel vapors and directing them into the engine for combustion. This system is a crucial part of modern vehicle emission control and helps in maintaining air quality by reducing the amount of fuel vapor that escapes into the atmosphere.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T13:41:05.720204\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': '\"statement\": \"The EVAP canister is located underneath the rear seat of the vehicle.\",'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 30 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   240 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11020.58 ms /   294 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The EVAP (Evaporative Emission Control) system is a crucial part of modern vehicles' exhaust systems. It's designed to capture fuel vapors that can escape from the fuel tank and prevent them from being emitted into the atmosphere.\n",
      "\n",
      "The location of the EVAP canister can vary depending on the make and model of the vehicle. However, it's common for the canister to be located near the fuel tank, often in the rear part of the vehicle. This is because the fuel vapors tend to rise and accumulate near the top of the fuel tank.\n",
      "\n",
      "In the case of the statement you've provided, it's accurate to say that the EVAP canister is often located underneath the rear seat of the vehicle. This placement allows for easy access when servicing the system. \n",
      "\n",
      "However, it's important to note that this is not a universal truth for every vehicle. Some cars may have the EVAP canister in a different location, such as near the fuel pump or even in the engine bay. Therefore, when performing maintenance on a vehicle, it's best to consult the vehicle's repair manual or speak with a professional mechanic to confirm the exact location of the EVAP canister.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T13:42:00.070860\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': 'true or false, [{\\'role\\': \\'system\\', \\'content\\': \\'You are a helpful assistent; Current date and time: 2025-02-24T13:41:05.720204\\\\nNo direct related knowledge found. Proceeding with general reasoning.\\\\n\\'}, {\\'role\\': \\'user\\', \\'content\\': \\'\"statement\": \"The EVAP canister is located underneath the rear seat of the vehicle.\",\\'}]'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 31 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   132 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   133 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7135.24 ms /   265 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: False.\n",
      "\n",
      "The statement \"The EVAP canister is located underneath the rear seat of the vehicle\" is not generally true for all vehicles. The exact location of the EVAP (Evaporative Emission Control) canister can vary depending on the vehicle's make, model, and engine configuration. Typically, the EVAP canister is found in the rear part of the vehicle's fuel tank, but it can also be located in other areas. Some vehicles have the EVAP canister in the engine compartment, while others might have it in a location that is not easily accessible. The rear seat area is not a common location for the EVAP canister.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T13:44:27.821261\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': '\"statement\": \"The fuel and EVAP system includes the fuel tank, EVAP canister, vacuum lines, and the fuel filler neck.\",       \"answer\": true'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 31 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   213 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9958.50 ms /   281 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: To determine if the given statement is correct, let's break down the components mentioned in the statement and verify each one:\n",
      "\n",
      "1. Fuel Tank: This is a component of the fuel and EVAP (Evaporative Emission Control) system. The fuel tank stores the fuel and is connected to the fuel lines and filler neck. \n",
      "\n",
      "2. EVAP Canister: The EVAP canister is a crucial component of the fuel and EVAP system. Its purpose is to capture and store fuel vapors that are produced during the fuel vaporization process. \n",
      "\n",
      "3. Vacuum Lines: These lines are part of the fuel system. They are used to regulate the pressure in the fuel system and ensure a smooth flow of fuel. \n",
      "\n",
      "4. Fuel Filler Neck: This is the part of the vehicle where the fuel is added. It is connected to the fuel tank and is an essential component of the fuel and EVAP system.\n",
      "\n",
      "Given the above information, the statement correctly identifies the components included in the fuel and EVAP system. Therefore, the answer is true.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T13:44:41.681921\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': ' \"statement\": \"The fuel and EVAP system includes the fuel tank, EVAP canister, vacuum lines, and the fuel filler neck.\",'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 33 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   426 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18688.73 ms /   487 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: This statement is partially correct. The fuel and EVAP (Emission Vapor) system does include the fuel tank, EVAP canister, vacuum lines, and the fuel filler neck. However, it is important to note that the system also includes other components such as the fuel pump, fuel pressure regulator, fuel injectors, and the fuel vapor recovery valve. Here's a more complete overview of the fuel and EVAP system:\n",
      "\n",
      "1. Fuel Tank: This is the main storage container for the fuel. It is typically made of metal and has a vapor-tight seal to prevent fuel vapor leaks.\n",
      "\n",
      "2. EVAP Canister: This is a sealed container that stores fuel vapors produced by the fuel in the tank. The canister is connected to the fuel tank by a vacuum line, allowing the fuel vapor to be transferred to the engine when needed.\n",
      "\n",
      "3. Vacuum Lines: These lines connect the fuel tank and the EVAP canister to the engine. They allow the fuel vapor to be drawn into the engine and burned off, reducing emissions.\n",
      "\n",
      "4. Fuel Filler Neck: This is the opening in the fuel tank where fuel is added. It is sealed by a cap that can be removed to add fuel.\n",
      "\n",
      "5. Fuel Pump: This component is responsible for pumping fuel from the fuel tank to the engine. It is typically located in the fuel tank or in the engine bay.\n",
      "\n",
      "6. Fuel Pressure Regulator: This component regulates the fuel pressure in the fuel system to ensure consistent fuel flow to the engine.\n",
      "\n",
      "7. Fuel Injectors: These components are responsible for injecting fuel into the engine. They are controlled by the engine's computer and are responsible for the precise amount of fuel delivered to the engine.\n",
      "\n",
      "8. Fuel Vapor Recovery Valve: This component is responsible for transferring fuel vapor from the fuel tank to the EVAP canister when the fuel tank is being refilled.\n",
      "\n",
      "In summary, the fuel and EVAP system is a crucial component of a vehicle's emission control system, and it is responsible for managing the fuel vapor produced by the fuel in the tank and ensuring that it is burned off in the engine.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T13:47:17.254196\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': '\"statement\": \"The vapor pressure sensor in the EVAP system compares the pressure in the fuel tank with atmospheric pressure.\",'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 31 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   407 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   17933.03 ms /   465 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The statement is accurate. The vapor pressure sensor in the Evaporative Emission Control (EVAP) system indeed compares the pressure in the fuel tank to atmospheric pressure. This comparison is crucial for determining whether there is a significant difference between the two pressures, which could indicate a problem with the EVAP system.\n",
      "\n",
      "To elaborate, the EVAP system is designed to prevent fuel vapors from escaping into the atmosphere. It does this by collecting the vapors in a charcoal canister and then periodically routing the vapors into the engine for combustion. The vapor pressure sensor is a key component in this system.\n",
      "\n",
      "Here's how the vapor pressure sensor works:\n",
      "\n",
      "1. **Pressure Comparison**: The sensor compares the pressure inside the fuel tank with the atmospheric pressure outside. This comparison is made by measuring the pressure difference between the two locations.\n",
      "\n",
      "2. **System Operation**: If the pressure in the fuel tank is higher than atmospheric pressure, it indicates that there is a positive pressure difference. This could be due to a malfunction in the EVAP system, allowing fuel vapors to escape. Conversely, if the pressure in the fuel tank is lower than atmospheric pressure, it could indicate a blockage in the system, preventing proper vapor collection.\n",
      "\n",
      "3. **Diagnostic Function**: The vapor pressure sensor sends this pressure difference information to the vehicle's On-Board Diagnostics (OBD) system. The OBD system then uses this information to diagnose potential issues with the EVAP system.\n",
      "\n",
      "4. **Maintenance and Repairs**: By monitoring the vapor pressure, the system can alert the driver to potential issues. If the pressure difference is outside the normal range, it might indicate a problem with the EVAP system, such as a clogged charcoal canister or a leaking hose. This information helps in diagnosing and repairing the system to ensure proper function and to meet emission regulations.\n",
      "\n",
      "In summary, the vapor pressure sensor in the EVAP system does indeed compare the pressure in the fuel tank to atmospheric pressure, serving as an essential diagnostic tool for maintaining the system's functionality.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T13:51:20.709923\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': '\"statement\": \"The onboard recovery valve is also known as the fill check valve and is responsible for venting excess vapor during refueling.\",'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 30 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   366 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   16335.89 ms /   429 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The statement provided is correct. The onboard recovery valve, also known as the fill check valve, plays a crucial role in the refueling process. It is designed to manage the flow of fuel vapor, ensuring that the fueling operation runs smoothly and safely.\n",
      "\n",
      "Here's a more detailed explanation:\n",
      "\n",
      "1. **Function**: The onboard recovery valve's primary function is to monitor and control the flow of vapor that is generated during the refueling process. When the fuel tank is nearly full, the vapor pressure inside the tank increases, and this excess vapor needs to be vented out to prevent overpressure and potential fuel spillage.\n",
      "\n",
      "2. **Valve Operation**: When the fuel tank is filling up, the onboard recovery valve opens, allowing the excess vapor to be vented out. It is designed to close when the fuel tank is full, preventing fuel from spilling out. This valve operates automatically and is essential in ensuring that the refueling operation is safe and efficient.\n",
      "\n",
      "3. **Fill Check Valve**: The term \"fill check valve\" is another name for the onboard recovery valve. This valve serves as a safeguard during the refueling process, ensuring that fuel only flows into the tank and vapor only flows out.\n",
      "\n",
      "4. **Safety**: The valve's ability to vent excess vapor during refueling is crucial for safety reasons. If this vapor were to build up inside the fuel tank, it could create an explosion hazard. By venting the vapor, the onboard recovery valve helps to prevent such accidents and ensures that the refueling process is as safe as possible.\n",
      "\n",
      "In summary, the onboard recovery valve, also known as the fill check valve, is an essential component in the refueling process. Its function is to monitor and control the flow of vapor, ensuring that the fuel tank remains at a safe pressure level and preventing fuel spillage.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T14:14:52.035962\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': 'true or false? \"statement\": \"The onboard recovery valve is also known as the fill check valve and is responsible for venting excess vapor during refueling.\",'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 28 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   157 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7795.51 ms /   226 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The statement \"The onboard recovery valve is also known as the fill check valve and is responsible for venting excess vapor during refueling\" is partially correct.\n",
      "\n",
      "The onboard recovery valve (also known as the vapor valve or recovery valve) is a component in aircraft fuel systems. Its primary function is to vent excess vapor during the refueling process. However, it's not accurate to say that the onboard recovery valve is also known as the fill check valve. The fill check valve is a separate component in the refueling system that prevents fuel from flowing back into the fuel truck or aircraft fueling hose during refueling.\n",
      "\n",
      "In summary, the onboard recovery valve and the fill check valve are related, but they serve different purposes in the refueling process. So, the statement is not entirely true.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T15:15:48.828164\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': 'how many pieces of rust must be removed in order to take off the fill check valve?'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 28 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   383 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18008.76 ms /   438 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: To answer this question, we need to understand the context of removing a fill check valve, which is typically found in the plumbing system of a building or a piece of equipment. The fill check valve is a component that prevents backflow of water in a system. It is often rusted or corroded, and may require some preparation before it can be removed.\n",
      "\n",
      "Here's a step-by-step process to determine how many pieces of rust must be removed to take off the fill check valve:\n",
      "\n",
      "1. **Identify the location and condition of the valve**: Locate the fill check valve in the system. Assess the extent of rust and corrosion that has accumulated on the valve.\n",
      "\n",
      "2. **Determine the severity of rust**: Rust and corrosion can be in various stages, ranging from light to heavy. If the rust is light, you might be able to remove it with a wire brush or a rust remover. If it is heavy, you might need to use a chisel or hammer to remove it.\n",
      "\n",
      "3. **Estimate the number of rust pieces**: Once you've assessed the severity of rust, you can estimate the number of pieces that need to be removed. If the rust is light, you might only need to remove a few pieces. If it is heavy, you might need to remove many pieces.\n",
      "\n",
      "4. **Prepare the valve**: Use a wire brush, rust remover, or a chisel to remove the rust and corrosion from the valve. Keep track of the number of pieces you remove.\n",
      "\n",
      "5. **Remove the fill check valve**: Once the rust and corrosion have been removed, you can use a wrench or a socket to remove the fill check valve from the system.\n",
      "\n",
      "In summary, the number of pieces of rust that must be removed to take off the fill check valve depends on the severity of the rust and corrosion. To determine the exact number, you would need to assess the valve and its condition.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T17:45:24.261279\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': 'How is an EVAP canister secured in place?'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 28 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    48 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   400 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   18173.68 ms /   448 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: An Evaporative Emission Control System (EVAP) canister is secured in place to ensure that it remains functional and doesn't become dislodged or damaged during the vehicle's operation. Here's an overview of how an EVAP canister is typically secured:\n",
      "\n",
      "1. Mounting: The EVAP canister is mounted to the vehicle's frame or chassis using one or more brackets. These brackets are usually attached to the canister via bolts or screws. The specific mounting location and number of brackets vary depending on the vehicle's design.\n",
      "\n",
      "2. Sealing: To prevent fuel vapors from leaking around the mounting points, a sealant or gasket is used between the canister and the mounting brackets. This sealant helps maintain a tight seal and prevents fuel vapor emissions.\n",
      "\n",
      "3. Reinforcement: In some cases, additional reinforcement may be added to the mounting points to ensure that the EVAP canister remains secure during operation. This can include additional brackets, straps, or other reinforcement components.\n",
      "\n",
      "4. Locking Mechanism: Some vehicles may use a locking mechanism to secure the EVAP canister in place. This can be a simple mechanical lock or a more advanced locking system that requires a specific tool to release the canister.\n",
      "\n",
      "5. Welding: In some instances, the EVAP canister may be welded to the vehicle's frame or chassis to provide additional security and prevent it from being dislodged.\n",
      "\n",
      "6. Anti-theft devices: To further secure the EVAP canister, some vehicles may use anti-theft devices that incorporate the canister into the overall security system.\n",
      "\n",
      "7. Regular inspections: To ensure that the EVAP canister remains properly secured, regular inspections should be conducted. This can include checking the mounting brackets, seals, and any locking mechanisms to ensure they are functioning correctly and not causing any issues.\n",
      "\n",
      "By securing the EVAP canister in these various ways, vehicles can minimize the risk of fuel vapor emissions and maintain their emissions control systems' effectiveness.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T17:46:23.326174\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': ' \"question\": \"How is an EVAP canister secured in place?”,       \"options\": [         \"With two 12mm pieces of rust\",         \"With four 14mm bolts\",         \"With two 14mm bolts\",         \"With six pieces of rust\"       ],'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 31 prefix-match hit, remaining 97 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   247 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11644.83 ms /   344 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: To answer the question about how an EVAP (Evaporative Emission Control System) canister is secured in place, let's analyze the provided options:\n",
      "\n",
      "1. \"With two 12mm pieces of rust\" - Rust is a result of metal corrosion and is not a method of securing. This option is incorrect.\n",
      "\n",
      "2. \"With four 14mm bolts\" - Bolts are a common method for securing components in vehicles. Four 14mm bolts could be a plausible method for securing the EVAP canister.\n",
      "\n",
      "3. \"With two 14mm bolts\" - Similar to the previous option, this suggests the use of bolts, which are indeed used for securing parts in vehicles. Two 14mm bolts could be a feasible method.\n",
      "\n",
      "4. \"With six pieces of rust\" - Again, rust is not a method of securing. This option is incorrect.\n",
      "\n",
      "While both options 2 and 3 suggest the use of bolts, it is more common for EVAP canisters to be secured with four bolts rather than just two. This is because four bolts provide a more secure and stable installation. \n",
      "\n",
      "Therefore, the most likely correct answer is: \"With four 14mm bolts\".\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T18:01:52.511729\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': 'What bolt size is used for securing a strap onto a gas tank?'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 28 prefix-match hit, remaining 51 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    51 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   168 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8113.95 ms /   219 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The bolt size used for securing a strap onto a gas tank can vary depending on the specific manufacturer, model, and intended application. However, a common size for securing straps onto gas tanks is a 5/16-inch bolt, which is often used in the automotive industry. \n",
      "\n",
      "To give you a better idea of the size, a 5/16-inch bolt has a diameter of about 7.95 millimeters. It's important to ensure that the bolt size used is appropriate for the strap and the gas tank, as using the wrong size can lead to safety concerns.\n",
      "\n",
      "In some cases, other bolt sizes may be used, such as 3/8-inch or 7/16-inch bolts. It's always best to consult the manufacturer's specifications or guidelines for the specific gas tank and strap being used.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T18:02:32.204775\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': '\"question\": \"What bolt size is used for securing a strap onto a gas tank?”,       \"options\": [         \"12mm\",         \"14mm\",         \"16mm\",         \"10mm\"       ],'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 31 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   139 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6942.48 ms /   223 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The bolt size used for securing a strap onto a gas tank can vary depending on the specific requirements of the application, but 16mm is a common size for securing straps to gas tanks. \n",
      "\n",
      "This size is large enough to provide adequate strength for securing the strap to the tank, while still being small enough to fit into the holes typically found on gas tanks. \n",
      "\n",
      "12mm, 14mm, and 10mm bolts are also common in various applications, but they are typically used for other types of fastening and may not be strong enough to secure a strap to a gas tank. \n",
      "\n",
      "Therefore, the most likely answer among the options provided is 16mm.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T18:04:40.206840\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': '\"question\": \"What component limits the maximum level of gasoline in the tank?”,       \"options\": [         \"The fuel pump\",         \"The fuel filler neck\",         \"The fuel fill check valve\",         \"The vapor pressure sensor\"       ],'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 31 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6696.04 ms /   220 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The component that limits the maximum level of gasoline in the tank is the fuel filler neck. The fuel filler neck is a tube that connects the fuel tank to the fuel filler port on the vehicle. It has a vent that allows air to flow in and out of the tank as fuel is added. At the bottom of the filler neck is a restrictor, which is a small hole that limits the amount of fuel that can enter the tank at any one time. This restrictor ensures that the tank cannot be overfilled, protecting the vehicle and its fuel system from potential damage. The other components listed are not directly responsible for limiting the maximum level of gasoline in the tank.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T18:06:32.270583\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': '\"question\": \"What is the primary job of the baffles inside the gas tank?”,       \"options\": [         \"To increase fuel capacity\",         \"To filter impurities from fuel\",         \"To prevent fuel from sloshing back and forth\",         \"To cool the fuel\"       ],'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 31 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10786.36 ms /   325 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The primary job of the baffles inside the gas tank is to prevent fuel from sloshing back and forth. Baffles are strategically placed inside the fuel tank to create compartments that restrict the movement of fuel, which prevents fuel from sloshing and spilling during vehicle acceleration, braking, or turning. This helps maintain the fuel level at a stable position and ensures a consistent fuel supply to the engine.\n",
      "\n",
      "The other options listed are not the primary job of baffles:\n",
      "\n",
      "1. Increasing fuel capacity: The baffles do not contribute to increasing the fuel capacity of the tank. In fact, they can slightly reduce the effective fuel storage space by creating additional compartments.\n",
      "\n",
      "2. Filtering impurities from fuel: While fuel filters are used to remove impurities from the fuel, the baffles in the fuel tank do not play a significant role in this process. Fuel filters are typically located in the fuel lines between the fuel tank and the engine.\n",
      "\n",
      "3. Cooling the fuel: Baffles are not designed to cool the fuel. Fuel temperature regulation is generally achieved by other means, such as fuel lines running through the vehicle's frame or an auxiliary fuel cooler.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T18:14:39.678752\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': 'and down?”,       \"options\": [         \"When the tank is pressurized above atmospheric pressure,         \"As long as that side is at atmospheric pressure\",         \"When the engine is running\",         \"Only during refueling\"       ],'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 30 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   305 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   14256.99 ms /   391 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The question provided does not directly relate to the given options. However, if the question is related to a scenario involving a tank and atmospheric pressure, the most likely correct answer would be:\n",
      "\n",
      "\"When the tank is pressurized above atmospheric pressure.\"\n",
      "\n",
      "This answer makes sense in the context of fluid dynamics and atmospheric pressure. Here's a breakdown:\n",
      "\n",
      "1. \"When the tank is pressurized above atmospheric pressure\": This suggests that the tank's pressure is greater than the surrounding atmospheric pressure. When this happens, the fluid inside the tank can be forced out through a valve or opening, regardless of the atmospheric pressure on the other side.\n",
      "\n",
      "2. \"As long as that side is at atmospheric pressure\": This implies that if one side of the tank is at atmospheric pressure, fluid can flow from the pressurized side to the atmospheric side. This is true, as fluid will flow from a region of high pressure to a region of low pressure.\n",
      "\n",
      "3. \"When the engine is running\": While the engine could be related to pressurizing the tank, it's not directly the cause for fluid flow.\n",
      "\n",
      "4. \"Only during refueling\": Refueling would involve pressurizing the tank, but this option limits the flow to only refueling times, which is not always the case.\n",
      "\n",
      "In summary, the most comprehensive and accurate answer is \"When the tank is pressurized above atmospheric pressure,\" as it encompasses the conditions under which fluid can flow from the pressurized side to the atmospheric side, regardless of the specific scenario.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T18:18:23.178186\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': '\"question\": \"What is the function of the check valves in the EVAP charcoal canister?”,       \"options\": [         \"Allow air in but block air out,         \"Allow air out but block air in\",         \"Regulate vapor temperature\",         \"Absorb gasoline vapors\"       ],'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 31 prefix-match hit, remaining 97 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   258 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11907.38 ms /   355 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The function of the check valves in the EVAP (Evaporative Emission Control System) charcoal canister is to \"Allow air in but block air out.\" This is necessary for maintaining the proper operation of the system.\n",
      "\n",
      "In the EVAP system, the charcoal canister stores gasoline vapors that are emitted from the fuel tank. The system's goal is to prevent these vapors from escaping into the atmosphere. To achieve this, the system uses a series of valves and sensors.\n",
      "\n",
      "The check valves are crucial in this process. When the engine is running, the intake manifold vacuum draws the vapor stored in the charcoal canister into the intake manifold and then into the engine for combustion. This is facilitated by check valves that allow air to enter the canister but prevent vapor from escaping back into the atmosphere.\n",
      "\n",
      "When the engine is not running, the check valves block the air flow, keeping the vapors trapped in the canister. This prevents them from escaping into the atmosphere until the next time the engine runs and the system is activated.\n",
      "\n",
      "In summary, the check valves in the EVAP system play a vital role in ensuring that gasoline vapors are stored and then combusted, preventing their release into the atmosphere. The correct answer to the question is \"Allow air in but block air out.\"\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-24T18:19:00.853793\\nNo direct related knowledge found. Proceeding with general reasoning.\\n'}, {'role': 'user', 'content': '\"question\": \"What is the function of the check valves in the EVAP charcoal canister?”,       \"options\": [         \"Allow air in but block air out,         \"Allow air out but block air in\",         \"Regulate vapor temperature\",         \"Absorb gasoline vapors\"       ],'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 31 prefix-match hit, remaining 97 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1613.15 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   296 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   13676.78 ms /   393 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The function of the check valves in the EVAP (Evaporative Emission Control System) charcoal canister is to \"Allow air in but block air out\". This is the correct option among the given choices.\n",
      "\n",
      "The EVAP system is designed to capture gasoline vapors that may escape from the fuel tank and fuel lines. It does this by using a charcoal canister to store the vapors. The canister is connected to the fuel tank and the engine intake manifold through a series of valves, including check valves.\n",
      "\n",
      "The purpose of the check valves is to allow a one-way flow of air and vapors. Specifically, they allow air to flow from the engine intake manifold into the charcoal canister to purge the stored vapors, but they prevent air from flowing directly from the atmosphere into the fuel tank. This is important because it maintains a vacuum in the fuel tank, which helps to keep the gasoline vapors contained.\n",
      "\n",
      "The other options listed are not correct for the check valves in the EVAP system:\n",
      "\n",
      "- \"Allow air out but block air in\" is incorrect because the check valves do not allow air to flow out of the fuel tank. They only allow air to flow in from the engine intake manifold.\n",
      "- \"Regulate vapor temperature\" is not the function of the check valves. This is more related to the temperature control solenoids in the system.\n",
      "- \"Absorb gasoline vapors\" is the function of the charcoal canister, not the check valves.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from llama_cpp import Llama\n",
    "import threading\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from json import JSONDecodeError\n",
    "\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/Qwen2.5-7B-Instruct-Uncensored.Q4_K_M.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=2048)\n",
    "            return cls._instance\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, \n",
    "                 messages_file='messages.json', \n",
    "                 knowledge_file='knowledge.json', \n",
    "                 faiss_index_file='faiss_index.pkl',\n",
    "                 model_name='all-MiniLM-L6-v2'):\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "        self.llm = LlamaSingleton().llm\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.knowledge_data = []\n",
    "        self.initialize_files()\n",
    "        self.load_faiss_index()\n",
    "\n",
    "    def initialize_files(self):\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def save_json_data(self, file_path, data):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "    def extract_valuable_knowledge(self, message):\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are a knowledge extractor. Try to Extract any knowledge from the user.\\n\"\n",
    "                        \"Return ONLY JSON with the following schema:\\n\"\n",
    "                        \"{\\n\"\n",
    "                        \"  \\\"valuable_knowledge\\\": [\\n\"\n",
    "                        \"    {\\n\"\n",
    "                        \"      \\\"subject\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"predicate\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"object\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"timestamp\\\": \\\"...\\\"  # ISO8601\\n\"\n",
    "                        \"    }\\n\"\n",
    "                        \"  ]\\n\"\n",
    "                        \"}\\n\"\n",
    "                        \"If no knowledge can be extracted, return:\\n\"\n",
    "                        \"{\\\"valuable_knowledge\\\": []}\"\n",
    "                    )\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"valuable_knowledge\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"subject\": {\"type\": \"string\"},\n",
    "                                    \"predicate\": {\"type\": \"string\"},\n",
    "                                    \"object\": {\"type\": \"string\"},\n",
    "                                    \"timestamp\": {\"type\": \"string\", \"format\": \"date-time\"}\n",
    "                                },\n",
    "                                \"required\": [\"subject\", \"predicate\", \"object\", \"timestamp\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"valuable_knowledge\"],\n",
    "                },\n",
    "            },\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        try:\n",
    "            knowledge_data = json.loads(response['choices'][0]['message']['content'])\n",
    "            print(knowledge_data)\n",
    "            if \"valuable_knowledge\" not in knowledge_data:\n",
    "                knowledge_data[\"valuable_knowledge\"] = []\n",
    "            return knowledge_data[\"valuable_knowledge\"]\n",
    "        except (JSONDecodeError, KeyError):\n",
    "            return []\n",
    "\n",
    "    def save_message(self, role, content):\n",
    "        messages = self.load_json_data(self.messages_file)\n",
    "        message = {\"role\": role, \"content\": content, \"timestamp\": datetime.utcnow().isoformat()}\n",
    "        messages.append(message)\n",
    "        self.save_json_data(self.messages_file, messages)\n",
    "\n",
    "    def save_knowledge(self, triplets):\n",
    "        if not triplets:\n",
    "            return\n",
    "        knowledge = self.load_json_data(self.knowledge_file)\n",
    "        existing_set = {(t['subject'], t['predicate'], t['object']) for t in knowledge}\n",
    "        new_triplets = []\n",
    "        for triplet in triplets:\n",
    "            triplet['timestamp'] = datetime.utcnow().isoformat()\n",
    "            key = (triplet['subject'], triplet['predicate'], triplet['object'])\n",
    "            if key not in existing_set:\n",
    "                knowledge.append(triplet)\n",
    "                new_triplets.append(triplet)\n",
    "                existing_set.add(key)\n",
    "        self.save_json_data(self.knowledge_file, knowledge)\n",
    "        if new_triplets:\n",
    "            self.update_faiss_index(new_triplets)\n",
    "\n",
    "    def update_faiss_index(self, triplets):\n",
    "        texts = [f\"{t['subject']} {t['predicate']} {t['object']}\" for t in triplets]\n",
    "        embeddings = self.model.encode(texts)\n",
    "        if self.index is None:\n",
    "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(np.array(embeddings, dtype=np.float32))\n",
    "        self.knowledge_data.extend(triplets)\n",
    "        self.save_faiss_index()\n",
    "\n",
    "    def save_faiss_index(self):\n",
    "        with open(self.faiss_index_file, 'wb') as f:\n",
    "            pickle.dump((self.index, self.knowledge_data), f)\n",
    "\n",
    "    def load_faiss_index(self):\n",
    "        if os.path.exists(self.faiss_index_file):\n",
    "            with open(self.faiss_index_file, 'rb') as f:\n",
    "                self.index, self.knowledge_data = pickle.load(f)\n",
    "        else:\n",
    "            self.index = None\n",
    "            self.knowledge_data = []\n",
    "\n",
    "    def search_knowledge(self, query, top_k=5):\n",
    "        if self.index is None or len(self.knowledge_data) == 0:\n",
    "            return []\n",
    "        query_embedding = self.model.encode([query])\n",
    "        distances, indices = self.index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            results.append(self.knowledge_data[idx])\n",
    "        return results\n",
    "\n",
    "    def generate_response(self, conversation_history, user_message):\n",
    "        knowledge_matches = None #self.search_knowledge(user_message, top_k=5)\n",
    "        current_time = datetime.utcnow().isoformat()\n",
    "        system_message = f\"Current date and time: {current_time}\\n\"\n",
    "        if knowledge_matches:\n",
    "            system_message += \"Answer based on retrieved knowledge:\\n\"\n",
    "            for t in knowledge_matches:\n",
    "                system_message += f\"- {t['subject']} {t['predicate']} {t['object']} (Videotimestamps: start: {t['start']}, end: {t['end']})\\n\"\n",
    "            \n",
    "        else:\n",
    "            system_message += \"No direct related knowledge found. Proceeding with general reasoning.\\n\"\n",
    "        enriched_history = [{\"role\": \"system\", \"content\": f\"You are a helpful assistent; {system_message}\"}] #+ conversation_history\n",
    "        enriched_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        print(enriched_history)\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=enriched_history,\n",
    "            temperature=0.7,\n",
    "        )['choices'][0]['message']['content']\n",
    "        return response\n",
    "\n",
    "    def chat(self):\n",
    "        print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "        while True:\n",
    "            user_message = input(\"You: \")\n",
    "            if user_message.lower().strip() in ['exit', 'quit']:\n",
    "                print(\"Chatbot: Goodbye!\")\n",
    "                break\n",
    "            self.save_message(role='user', content=user_message)\n",
    "            conversation = self.load_json_data(self.messages_file)[-3:]\n",
    "            assistant_response = self.generate_response(conversation, user_message)\n",
    "            print(f\"Assistant: {assistant_response}\")\n",
    "            #generate_speech(assistant_response)\n",
    "            self.save_message(role='assistant', content=assistant_response)\n",
    "            #user_knowledge_response = self.extract_valuable_knowledge(user_message)\n",
    "            #print(user_knowledge_response)  \n",
    "            #if user_knowledge_response:\n",
    "                #self.save_knowledge(user_knowledge_response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = Chatbot()\n",
    "    chatbot.chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUTION PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M3 Max) - 26578 MiB free\n",
      "llama_model_loader: loaded meta data with 41 key-value pairs and 339 tensors from models/Qwen2.5-7B-Instruct.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct Uncensored\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct-Uncensored\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   6:                            general.license str              = gpl-3.0\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Qwen2.5 7B Instruct\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Qwen\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7...\n",
      "llama_model_loader: - kv  11:                               general.tags arr[str,3]       = [\"qwen\", \"uncensored\", \"text-generati...\n",
      "llama_model_loader: - kv  12:                          general.languages arr[str,2]       = [\"zh\", \"en\"]\n",
      "llama_model_loader: - kv  13:                           general.datasets arr[str,5]       = [\"NobodyExistsOnTheInternet/ToxicQAFi...\n",
      "llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584\n",
      "llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944\n",
      "llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28\n",
      "llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  22:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {% set system_message = 'You are a he...\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  34:                                general.url str              = https://huggingface.co/mradermacher/Q...\n",
      "llama_model_loader: - kv  35:              mradermacher.quantize_version str              = 2\n",
      "llama_model_loader: - kv  36:                  mradermacher.quantized_by str              = mradermacher\n",
      "llama_model_loader: - kv  37:                  mradermacher.quantized_at str              = 2024-10-11T20:15:47+02:00\n",
      "llama_model_loader: - kv  38:                  mradermacher.quantized_on str              = db3\n",
      "llama_model_loader: - kv  39:                         general.source.url str              = https://huggingface.co/Orion-zhen/Qwe...\n",
      "llama_model_loader: - kv  40:                  mradermacher.convert_type str              = hf\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type q4_K:  169 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.36 GiB (4.91 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 3584\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 28\n",
      "print_info: n_head_kv        = 4\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 7\n",
      "print_info: n_embd_k_gqa     = 512\n",
      "print_info: n_embd_v_gqa     = 512\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 18944\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.62 B\n",
      "print_info: general.name     = Qwen2.5 7B Instruct Uncensored\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 152064\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 148848 'ÄĬ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 338 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/29 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  4460.45 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 28991.03 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x31310bfd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x177f957c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x45d60a9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x42c54e7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x42c54eae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x45d5206b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x45d51e5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x45d54c490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x177fdd6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x177f95500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x45d51aa00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x45d51e900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x42c54fbc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x42c54ee40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x177f95a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x1120fc6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x3139df030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x31310d130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x45d521ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x45d519e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x124a05800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x45d51c340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x313308320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x313307aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x31310c290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x31310d4b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x124a08050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x42c550ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x42c550940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x312ce98c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1114c0bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x3139de4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x42c5505e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x42c53d0f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x111476bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x42c540ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x3133094c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1769c87b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x45d51b930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x31310eb10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1120fca60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1120fce00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x45d523ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x3139deb60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x31310dfc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x42c538db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x3133098f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x31310e5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x313309e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x312cea090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x313308950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x3139dfd60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x3139df7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x3139e02d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x312cea350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x31310f090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x312cea6e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x312ceac90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x312c37ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x31310f510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x111478080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x42c541e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x312c38440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x31310fa80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x31330a850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x313110020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1120fd4b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1120fd770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x3139e0590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x312c38700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1120fdb10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1769c7770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x31330aef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1114be8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x16b84d7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11146a9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1114ccd90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1120fea40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x3139e0850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1120fed00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x313110600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x312c38f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x313111530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x42c54ff20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x31330c660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x31330c920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x31330cbe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x31330d710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x312c39330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x313111010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x3139e0b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x313112e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x3139e0ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1120fefc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x311d48b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1114bef70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x177fde190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x3139e1a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x312c39bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1120ff280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x42c551000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x177fde900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x3139e11e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1120ff540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1120ff800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x177fde450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x177f85080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1769c7420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x312c3a510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x177f85800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x176998f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1769c6c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1120ffbc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x177f86000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x31330d9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x177f8ad60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x4af3e6780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x177f8b020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x4af3e6130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x312c3acd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1769cfc70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x312c3b5c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x31330de10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x4af3e2040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x42c585230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x177f8b2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x31330e580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x312c3bce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x312c3c370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x31330eaa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1769b56e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x312c3ca20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x313113620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x31330f230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x312c3d190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x3139e15a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1114bf630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x313a11a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x31330f8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x312c3d880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x4af3e57e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x313a1f2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x110bab560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x311c047a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1114cd640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x31330fff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x3139e2ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x313a1c6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x4af3e3220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x4af3e4c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x311d44f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x124a04830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x110b9a1a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x311d47fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x313a1bf90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x313a1f920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x177fb5b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x110baa5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x103c770f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x313a1ea70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x110fa1990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x3133102b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x110fa1c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x110fa1f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x110fa2c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x110ba2410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x110fa2f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x4af3e0c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x3139e2300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1114ec220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x311d44b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x3131138e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x110ba9f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x3139e3940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x4af3e1120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x110babcd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x4af3df550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x110babf90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x3139e40d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x110fa31d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x4af3dde90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x313113ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x3139e30d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x3131143f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1114bfae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x177fb5e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x177fb60d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x1769c7c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x312c3e0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x45d607aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x312c3e700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x313310570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x312c3ed40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x4af3de150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x312c3f550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x1769cb580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x45d605dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x313311630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x313114e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x1114bebb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x312c401e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x313115150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x312c40a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x313311e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x177fb70f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1114c07a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x313312450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1114c0ef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x45d606080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x312c40d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x312c417f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x177fb7ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x313312e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x312c41e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x3133134d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x45d60b4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x313313ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x312c42520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x312c42870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1114ec680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x3133144d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x312c43040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x4af3dc4b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1114ecb70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x312c436d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1114ece30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x3139e47a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x3139e5230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x45d6088a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x312c43f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x3131155e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x45d608b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x45d608e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x4af3dbb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x4af3dd1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x313314850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x312c43a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x313315240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x3131160b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x312c44960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x313116740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x313116dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x45d60c9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x313315b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x312c458f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x313314d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x45d60c2a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x313117460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x45d605600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x3139e4c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1114ed780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x3139e5a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x3133165a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x3139e5e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x4af3dd470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x313316cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x313117ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x313117d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x3131183b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x313118950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x3139e6440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x177fb7f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x177fb8210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x312c45f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x3131192d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x177fb84d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x177fb8de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x313317740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x3131198d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x313317e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x177fb9190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x177fb9770 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 512, n_embd_v_gqa = 512\n",
      "llama_kv_cache_init:        CPU KV buffer size =   112.00 MiB\n",
      "llama_init_from_model: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   304.00 MiB\n",
      "llama_init_from_model: graph nodes  = 986\n",
      "llama_init_from_model: graph splits = 450 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/Orion-zhen/Qwen2.5-7B-Instruct-Uncensored', 'mradermacher.quantized_at': '2024-10-11T20:15:47+02:00', 'mradermacher.quantize_version': '2', 'general.url': 'https://huggingface.co/mradermacher/Qwen2.5-7B-Instruct-Uncensored-GGUF', 'tokenizer.chat_template': \"{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% endif %}{% if system_message is defined %}{{ '<|im_start|>system\\n' + system_message + '<|im_end|>\\n' }}{% endif %}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + content + '<|im_end|>\\n<|im_start|>assistant\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\n' }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.basename': 'Qwen2.5', 'qwen2.attention.head_count_kv': '4', 'general.size_label': '7B', 'general.base_model.0.name': 'Qwen2.5 7B Instruct', 'qwen2.embedding_length': '3584', 'qwen2.context_length': '32768', 'qwen2.block_count': '28', 'general.base_model.0.organization': 'Qwen', 'tokenizer.ggml.pre': 'qwen2', 'general.base_model.count': '1', 'qwen2.rope.freq_base': '1000000.000000', 'general.quantization_version': '2', 'general.license': 'gpl-3.0', 'general.base_model.0.repo_url': 'https://huggingface.co/Qwen/Qwen2.5-7B-Instruct', 'general.file_type': '15', 'general.finetune': 'Instruct-Uncensored', 'general.name': 'Qwen2.5 7B Instruct Uncensored', 'qwen2.feed_forward_length': '18944', 'mradermacher.quantized_by': 'mradermacher', 'general.architecture': 'qwen2', 'mradermacher.quantized_on': 'db3', 'qwen2.attention.head_count': '28', 'tokenizer.ggml.bos_token_id': '151643', 'general.type': 'model', 'tokenizer.ggml.model': 'gpt2'}\n",
      "Available chat formats from metadata: chat_template.default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/76/kx23hgz16fdcd_cjrpy0jnlm0000gn/T/ipykernel_4104/140284658.py:72: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  current_time = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-25T18:49:12.495904\\nAnswer based on retrieved knowledge:\\n- valve is itself (Videotimestamps: start: 386.586, end: 387.967)\\n- thing expected_to_snap soon (Videotimestamps: start: 35.679, end: 41.803)\\n- spring is present in here (Videotimestamps: start: 378.841, end: 385.745)\\n- check valve is present in here (Videotimestamps: start: 378.841, end: 385.745)\\n- filter is present in here (Videotimestamps: start: 378.841, end: 385.745)\\n'}, {'role': 'user', 'content': 'damn'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    2169.32 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   232 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   210 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10846.34 ms /   442 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Based on the timestamps provided, it seems like the context is related to a mechanical or engineering system, possibly a hydraulic or pneumatic system. Here's a summary of the key points:\n",
      "\n",
      "1. A valve is mentioned, and it's being described as itself (likely referring to a specific valve type or component).\n",
      "\n",
      "2. Something expected to snap soon is mentioned, which could refer to a part like a spring or a valve component that's nearing the end of its lifespan.\n",
      "\n",
      "3. A spring is present, which could be part of a valve mechanism or another component that operates by spring tension.\n",
      "\n",
      "4. Check valves, filters, and other components are also present, suggesting that the system being described is complex and has multiple parts with specific functions.\n",
      "\n",
      "Given this information, it seems like the system might be experiencing some wear and tear, with a part (possibly a spring) expected to fail soon. The presence of check valves, filters, and other components suggests that the system is designed to prevent backflow, filter contaminants, or perform some other specific function.\n",
      "[{'role': 'system', 'content': 'You are a helpful assistent; Current date and time: 2025-02-25T18:49:30.472556\\nAnswer based on retrieved knowledge:\\n- remaining pressure exits out here (Videotimestamps: start: 224.015, end: 264.245)\\n- vapors enter from one side and exit from the other side (Videotimestamps: start: 554.708, end: 579.948)\\n- check valve prevents fluid exit from the filler neck (Videotimestamps: start: 325.575, end: 365.911)\\n- vacuum_hoses have_been_disconnected from this side of the EVAP canister (Videotimestamps: start: 15.2, end: 28.755)\\n- pulling out action check valves (Videotimestamps: start: 489.851, end: 494.159)\\n'}, {'role': 'user', 'content': 'exitt'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 33 prefix-match hit, remaining 222 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2169.32 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   222 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5195.39 ms /   301 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Based on the provided timestamps from the video, it seems that the vapors enter from one side and exit from the other side of the EVAP canister. This is indicated in the video segment starting at 554.708 and ending at 579.948. Therefore, the exit point for the vapors is the opposite side from where they enter.\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from llama_cpp import Llama\n",
    "import threading\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from json import JSONDecodeError\n",
    "\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/Qwen2.5-7B-Instruct.Q4_K_M.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=2048)\n",
    "            return cls._instance\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, \n",
    "                 messages_file='messages.json', \n",
    "                 knowledge_file='knowledge.json', \n",
    "                 faiss_index_file='faiss_index.pkl',\n",
    "                 model_name='all-MiniLM-L6-v2'):\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "        self.llm = LlamaSingleton().llm\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.knowledge_data = []\n",
    "        self.initialize_files()\n",
    "        self.load_faiss_index()\n",
    "\n",
    "    def initialize_files(self):\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "\n",
    "    def load_faiss_index(self):\n",
    "        if os.path.exists(self.faiss_index_file):\n",
    "            with open(self.faiss_index_file, 'rb') as f:\n",
    "                self.index, self.knowledge_data = pickle.load(f)\n",
    "        else:\n",
    "            self.index = None\n",
    "            self.knowledge_data = []\n",
    "\n",
    "    def search_knowledge(self, query, top_k=5):\n",
    "        if self.index is None or len(self.knowledge_data) == 0:\n",
    "            return []\n",
    "        query_embedding = self.model.encode([query])\n",
    "        distances, indices = self.index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            results.append(self.knowledge_data[idx])\n",
    "        return results\n",
    "\n",
    "    def generate_response(self, user_message):\n",
    "        knowledge_matches = self.search_knowledge(user_message, top_k=5)\n",
    "        current_time = datetime.utcnow().isoformat()\n",
    "        system_message = f\"Current date and time: {current_time}\\n\"\n",
    "        if knowledge_matches:\n",
    "            system_message += \"Answer based on retrieved knowledge, but only if it relates to the question:\\n\"\n",
    "            for t in knowledge_matches:\n",
    "                system_message += f\"- {t['subject']} {t['predicate']} {t['object']} (Videotimestamps: start: {t['start']}, end: {t['end']})\\n\"\n",
    "            \n",
    "        else:\n",
    "            system_message += \"\\n\"\n",
    "        enriched_history = [{\"role\": \"system\", \"content\": f\"You are a helpful assistent; {system_message}\"}] \n",
    "        enriched_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        print(enriched_history)\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=enriched_history,\n",
    "            temperature=0.5,\n",
    "        )['choices'][0]['message']['content']\n",
    "        return response\n",
    "\n",
    "    def chat(self):\n",
    "        print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "        while True:\n",
    "            user_message = input(\"You: \")\n",
    "            if user_message.lower().strip() in ['exit', 'quit']:\n",
    "                print(\"Chatbot: Goodbye!\")\n",
    "                break\n",
    "            assistant_response = self.generate_response(user_message)\n",
    "            print(f\"Assistant: {assistant_response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = Chatbot()\n",
    "    chatbot.chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATIE SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M3 Max) - 26310 MiB free\n",
      "llama_model_loader: loaded meta data with 34 key-value pairs and 290 tensors from models/Qwen2.5-0.5B-Instruct-f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 0.5B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 0.5B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-0...\n",
      "llama_model_loader: - kv   8:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 0.5B\n",
      "llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen\n",
      "llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-0.5B\n",
      "llama_model_loader: - kv  12:                               general.tags arr[str,2]       = [\"chat\", \"text-generation\"]\n",
      "llama_model_loader: - kv  13:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  14:                          qwen2.block_count u32              = 24\n",
      "llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 896\n",
      "llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 4864\n",
      "llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 14\n",
      "llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  22:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type  f16:  169 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = F16\n",
      "print_info: file size   = 942.43 MiB (16.00 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 896\n",
      "print_info: n_layer          = 24\n",
      "print_info: n_head           = 14\n",
      "print_info: n_head_kv        = 2\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 7\n",
      "print_info: n_embd_k_gqa     = 128\n",
      "print_info: n_embd_v_gqa     = 128\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 4864\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 1B\n",
      "print_info: model params     = 494.03 M\n",
      "print_info: general.name     = Qwen2.5 0.5B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 148848 'ÄĬ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (f16) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/25 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =   942.43 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 28991.03 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x6430d8ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x6430d8860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x6bb3ea900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x760d6f700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x760d6fcd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x3367ea170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x5e23b41c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x6bb3e9570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x5e23b4b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x760d6e780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x760d6ea40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x5e23b5200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x432ef7b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x3367ea940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x760d70e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x432ee7b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x6bb3d01b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x3367eac00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x6bb3d6660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x760d705e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x3367cbe10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x6bb3da540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x6bb3d5810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x6bb3dbfc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x44a3e62c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x3367cc2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x6bb3efaa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x760d6ee90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x3367e7e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x760d6f150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x6430ee7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x6bb3eac10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x6bb3d3bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x6bb3e7790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x760d72820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x760d73400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x760d72de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x5e23b4480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x6bb3dc540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x3367d7760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x3367d7bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x760d71e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x3367d7e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x44a3f75c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x4abc43c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x5e2396550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x760d74820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x4abc43f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x5e23ac2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x760d74180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x760d753a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x760d74e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x5e23ac650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x4abc44970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x5e23acc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x3367d81e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x760d763d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x3367d84a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x4abc44450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x5e23ad290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x760d76c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x760d77380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x760d766e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x4abc34e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x4abc35130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x760d77990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x4abc35440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x4abc34560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x760d75f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x3367e8c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x5e23a0cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x4abc34820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x3367cd450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x5e23a0f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x760d78220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x760d785b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x44a3f7a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x5e23a1250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x5e239e080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x5e238c4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x44a3f7d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x3367cd8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x5e238c7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x5e238ca60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x5e238cd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x760d78c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x4abc1a000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x4abc13090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x5e238cfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x5e23ae860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x5e23aef10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x5e23af560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x760d79400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x5e23af820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x3389947a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x5e237e380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x5e2399990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x5e2399f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x760d7a180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x5e23a4ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x5e23a5770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x4abc12aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x4abc1d280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x760d7a830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x44a3f7fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x760d7ae80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x5e23a5a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x44a3f8280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x4abc3c9b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x5e23b13a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x44a3f8540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x760d7b240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x4abc13d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x4abc3cc70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x760d7b500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x5e23a5330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x760d7b890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x44a3fbaf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x338994b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x760d7c630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x338978c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x44a3fbdb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x760d7c8f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x44a3e4c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x760d7cbb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x4abc2f110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x760d7d920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x5e23b2d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x4abc2f3d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x5e23b3000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x760d7dec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x44a3f0980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x3389793a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x5e2398120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x338979660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x760d7e250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x4abc3dc10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x760d7e9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x5e23b1660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x760d7f8f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x44a3f0c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x338979920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x760d7ff70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x5e23b1da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x44a3e66a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x760d80230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x760d804f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x44a3e5330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x760d807b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x4abc3ded0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x6430e7640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x760d80bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x760d81230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x4abc3e570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x5e2399220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x5e23b2060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x760d817e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x338998d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x4abc3ee30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x760d81f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x5e238ab90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x760d82220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x5e238ae50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x760d82780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x5e23a1730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x6430e7900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x5e23a5e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x338997d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x5e23a60e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x760d82f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x6430e7bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x6430e7e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x5e23b5a10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x5e23a9910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x3389b40b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x6430e8140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x3389b4800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x5e2395030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x5e23a67f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x5e239ba20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x5e239c0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x6430e8400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x4abc3f0f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x4abc3f590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x760d83590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x3389b5270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x6430e86c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x6430e8980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x6430e8c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x6430e8f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x6430e94a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x3389b5530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x4abc27410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x760d843c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x6430eb050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x760d84680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x5e239c360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x3389b5c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x5e23b3a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x4abc28140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x6430ee260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x5e23a8a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x3389b6c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x3389b74e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x6430e6690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x5e239f510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x5e23a9f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x6430e4070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x6430d9020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x3389b77a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x5e23aa250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x5e23aa510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x760d84940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x6431e1be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x124a19fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x4ab939a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x6bb48a150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x6431e24a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x760d85260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x4abc18360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x4abc28500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x4abc317e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x4abc31aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x6430d94b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x642dd99f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x643189360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x642dfc840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x124a1a550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x6430d9a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x3389b7d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x3389b8b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x5e23aa7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x760d855f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x3389b8550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x4abc2b9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x4abc2bca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x5e23aaa90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x760d85d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x5e23aaea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x4abc3c010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x4abc41710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x6430da440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x5e260fef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x760d86b20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x642de7fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x6430da7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x6430daf60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x6430db910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x6430dc210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x6430db480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x760d87300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x5e23ab340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x5e23ab970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x5e2375920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x4abc0d0c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x4abc1db20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x6430dcbc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x4abc406f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x5e23a1db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x4abc29160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x4abc29420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x642de5070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x432aba7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x760d876c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x6bb489bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x760d87c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x5e2610cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x4abc2aa60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x5e2610840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x6430dd450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x5e23a2070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x432a69140 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 128, n_embd_v_gqa = 128\n",
      "llama_kv_cache_init:        CPU KV buffer size =    24.00 MiB\n",
      "llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   298.50 MiB\n",
      "llama_init_from_model: graph nodes  = 846\n",
      "llama_init_from_model: graph splits = 386 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'general.license.link': 'https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct/blob/main/LICENSE', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.basename': 'Qwen2.5', 'qwen2.attention.head_count_kv': '2', 'general.size_label': '0.5B', 'general.base_model.0.name': 'Qwen2.5 0.5B', 'qwen2.embedding_length': '896', 'qwen2.context_length': '32768', 'qwen2.block_count': '24', 'general.base_model.0.organization': 'Qwen', 'tokenizer.ggml.pre': 'qwen2', 'general.base_model.count': '1', 'qwen2.rope.freq_base': '1000000.000000', 'general.quantization_version': '2', 'general.base_model.0.repo_url': 'https://huggingface.co/Qwen/Qwen2.5-0.5B', 'general.license': 'apache-2.0', 'general.file_type': '1', 'general.finetune': 'Instruct', 'general.name': 'Qwen2.5 0.5B Instruct', 'qwen2.feed_forward_length': '4864', 'general.architecture': 'qwen2', 'qwen2.attention.head_count': '14', 'tokenizer.ggml.bos_token_id': '151643', 'general.type': 'model', 'tokenizer.ggml.model': 'gpt2'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "/var/folders/76/kx23hgz16fdcd_cjrpy0jnlm0000gn/T/ipykernel_4104/4187505112.py:78: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  current_time = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side', 'start': 1.448, 'end': 15.18}, {'subject': 'EVAP line', 'predicate': 'is held on by', 'object': 'four pieces of rust', 'start': 185.836, 'end': 194.882}, {'subject': 'canister', 'predicate': 'is_held_in_by', 'object': 'two 12mm pieces of rust', 'start': 15.2, 'end': 28.755}, {'subject': 'evap canister', 'predicate': 'has', 'object': 'two lines', 'start': 43.564, 'end': 148.419}, {'subject': 'EVAP_canister', 'predicate': 'can_be_removed', 'object': 'from above the subframe', 'start': 15.2, 'end': 28.755}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   314 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     432.15 ms /   329 tokens\n",
      "/var/folders/76/kx23hgz16fdcd_cjrpy0jnlm0000gn/T/ipykernel_4104/4187505112.py:96: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  current_time = datetime.utcnow().isoformat()\n",
      "Llama.generate: 9 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     267.60 ms /   108 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 299 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'location', 'object': 'underneath the rear seat of the vehicle', 'start': 29.356, 'end': 35.579}, {'subject': 'fuel pump', 'predicate': 'is located', 'object': 'at the lowest part of the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side', 'start': 1.448, 'end': 15.18}, {'subject': 'gas tank', 'predicate': 'will be opened', 'object': \"to see what's inside\", 'start': 281.796, 'end': 303.949}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   299 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    37 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     625.46 ms /   336 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    22 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     368.21 ms /   116 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 292 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'two_14mm_bolts', 'predicate': 'hold', 'object': 'strap_to_gas_tank', 'start': 35.679, 'end': 41.803}, {'subject': 'gas tank', 'predicate': 'fixing', 'object': 'two straps', 'start': 29.356, 'end': 35.579}, {'subject': 'fuel filler neck', 'predicate': 'held in by', 'object': '14mm bolt', 'start': 43.564, 'end': 148.419}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'big steel', 'start': 29.356, 'end': 35.579}, {'subject': 'gas tank', 'predicate': 'location', 'object': 'underneath the rear seat of the vehicle', 'start': 29.356, 'end': 35.579}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   292 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    25 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     506.28 ms /   317 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    20 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     347.77 ms /   115 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 306 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'fuel filler neck', 'predicate': 'held in by', 'object': '14mm bolt', 'start': 43.564, 'end': 148.419}, {'subject': 'gas filler neck', 'predicate': 'is part of', 'object': 'fuel system', 'start': 367.711, 'end': 378.521}, {'subject': 'fuel filler neck', 'predicate': 'contains', 'object': 'a one-way check valve', 'start': 325.575, 'end': 365.911}, {'subject': 'two_14mm_bolts', 'predicate': 'hold', 'object': 'strap_to_gas_tank', 'start': 35.679, 'end': 41.803}, {'subject': 'Filler neck', 'predicate': 'contains', 'object': 'hollow tube', 'start': 390.148, 'end': 420.44}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    38 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     628.20 ms /   344 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     784.62 ms /   166 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 283 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'tank side', 'predicate': 'disconnect', 'object': 'two ventilation hoses', 'start': 180.282, 'end': 183.305}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'vacuum_hoses', 'predicate': 'have_been_disconnected', 'object': 'from this side of the EVAP canister', 'start': 15.2, 'end': 28.755}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'Fuel tank vapor pressure sensor', 'predicate': 'has', 'object': '3 terminals', 'start': 390.148, 'end': 420.44}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   283 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    34 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     587.28 ms /   317 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     374.08 ms /    96 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 287 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'check valve', 'predicate': 'needs removal', 'object': 'six pieces of rust', 'start': 196.161, 'end': 222.294}, {'subject': 'pulling out', 'predicate': 'action', 'object': 'check valves', 'start': 489.851, 'end': 494.159}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'check valve', 'predicate': 'is present in', 'object': 'here', 'start': 378.841, 'end': 385.745}, {'subject': 'Ball check valve', 'predicate': 'is part of', 'object': 'the check valve', 'start': 496.587, 'end': 534.03}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    20 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     449.83 ms /   307 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   203 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2148.19 ms /   295 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 311 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'fill check valve', 'predicate': 'opens valve', 'object': 'inside canister', 'start': 224.015, 'end': 264.245}, {'subject': 'fill check valve', 'predicate': 'allows', 'object': 'excess vapor to pass into the canister', 'start': 148.639, 'end': 180.042}, {'subject': 'fill check valve', 'predicate': 'allows escape of vapor', 'object': 'when gas tank is rapidly filling up with vapor', 'start': 224.015, 'end': 264.245}, {'subject': 'check valve', 'predicate': 'allows fluid entry', 'object': 'into the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'check valve', 'predicate': 'is present in', 'object': 'here', 'start': 378.841, 'end': 385.745}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   311 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     369.76 ms /   320 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1024.13 ms /   190 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 306 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'evap float', 'predicate': 'prevents', 'object': 'liquid gasoline', 'start': 196.161, 'end': 222.294}, {'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'float', 'predicate': 'prevents', 'object': 'liquid from going inside your charcoal canister', 'start': 266.787, 'end': 280.936}, {'subject': 'fuel tank', 'predicate': 'has', 'object': 'evap canister', 'start': 43.564, 'end': 148.419}, {'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 'absorb gasoline vapors', 'start': 469.037, 'end': 482.005}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     471.10 ms /   324 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     876.40 ms /   176 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 287 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'construction material', 'object': 'sheet metal', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'big steel', 'start': 29.356, 'end': 35.579}, {'subject': 'gas tank', 'predicate': 'construction method', 'object': 'stamping', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'joining method', 'object': 'welding', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     865.03 ms /   349 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1068.34 ms /   189 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 304 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'maximum fuel level', 'predicate': 'is limited by', 'object': 'the height of the fuel fill check valve', 'start': 325.575, 'end': 365.911}, {'subject': 'fuel pump', 'predicate': 'is located', 'object': 'at the lowest part of the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'check valves', 'predicate': 'function', 'object': 'control flow of gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'gas filler neck', 'predicate': 'is part of', 'object': 'fuel system', 'start': 367.711, 'end': 378.521}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   304 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    14 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     424.85 ms /   318 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1055.95 ms /   183 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 294 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'internal feature', 'object': 'baffles', 'start': 308.171, 'end': 325.335}, {'subject': 'baffle', 'predicate': 'prevents', 'object': 'fuel sloshing', 'start': 325.575, 'end': 365.911}, {'subject': 'baffles', 'predicate': 'location', 'object': 'perimeter of the tank', 'start': 308.171, 'end': 325.335}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'gas tank', 'predicate': 'construction method', 'object': 'stamping', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   294 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.51 ms /   305 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    41 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     854.62 ms /   139 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 320 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'check valve', 'predicate': 'allows fluid entry', 'object': 'into the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'diaphragm', 'predicate': 'can move', 'object': 'up and down', 'start': 281.796, 'end': 303.949}, {'subject': 'Check valve', 'predicate': 'acts as', 'object': 'a valve that releases air when a certain pressure is reached', 'start': 496.587, 'end': 534.03}, {'subject': 'fuel check valve', 'predicate': 'contains', 'object': 'lid', 'start': 281.796, 'end': 303.949}, {'subject': 'maximum fuel level', 'predicate': 'is limited by', 'object': 'the height of the fuel fill check valve', 'start': 325.575, 'end': 365.911}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   320 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     949.22 ms /   393 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   110 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   111 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1403.21 ms /   221 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 322 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 'absorb gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'check valves', 'predicate': 'function', 'object': 'control flow of gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'vacuum switching valve', 'predicate': 'allows', 'object': 'air flow from gas tank to charcoal canister', 'start': 435.808, 'end': 468.877}, {'subject': 'vacuum switching valve', 'predicate': 'functions as', 'object': 'redirecting air flow between gas tank and charcoal canister', 'start': 435.808, 'end': 468.877}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   322 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     618.55 ms /   345 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    39 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     713.11 ms /   142 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results saved to result_eval_Qwen2.5-0.5B-Instruct-f16.json\n",
      "[{'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side', 'start': 1.448, 'end': 15.18}, {'subject': 'EVAP line', 'predicate': 'is held on by', 'object': 'four pieces of rust', 'start': 185.836, 'end': 194.882}, {'subject': 'canister', 'predicate': 'is_held_in_by', 'object': 'two 12mm pieces of rust', 'start': 15.2, 'end': 28.755}, {'subject': 'evap canister', 'predicate': 'has', 'object': 'two lines', 'start': 43.564, 'end': 148.419}, {'subject': 'EVAP_canister', 'predicate': 'can_be_removed', 'object': 'from above the subframe', 'start': 15.2, 'end': 28.755}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 9 prefix-match hit, remaining 305 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   305 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    39 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     889.52 ms /   344 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     931.19 ms /   173 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 299 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'location', 'object': 'underneath the rear seat of the vehicle', 'start': 29.356, 'end': 35.579}, {'subject': 'fuel pump', 'predicate': 'is located', 'object': 'at the lowest part of the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side', 'start': 1.448, 'end': 15.18}, {'subject': 'gas tank', 'predicate': 'will be opened', 'object': \"to see what's inside\", 'start': 281.796, 'end': 303.949}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   299 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.97 ms /   312 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    42 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     539.24 ms /   136 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 292 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'two_14mm_bolts', 'predicate': 'hold', 'object': 'strap_to_gas_tank', 'start': 35.679, 'end': 41.803}, {'subject': 'gas tank', 'predicate': 'fixing', 'object': 'two straps', 'start': 29.356, 'end': 35.579}, {'subject': 'fuel filler neck', 'predicate': 'held in by', 'object': '14mm bolt', 'start': 43.564, 'end': 148.419}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'big steel', 'start': 29.356, 'end': 35.579}, {'subject': 'gas tank', 'predicate': 'location', 'object': 'underneath the rear seat of the vehicle', 'start': 29.356, 'end': 35.579}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   292 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    25 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     521.21 ms /   317 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     351.22 ms /   114 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 306 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'fuel filler neck', 'predicate': 'held in by', 'object': '14mm bolt', 'start': 43.564, 'end': 148.419}, {'subject': 'gas filler neck', 'predicate': 'is part of', 'object': 'fuel system', 'start': 367.711, 'end': 378.521}, {'subject': 'fuel filler neck', 'predicate': 'contains', 'object': 'a one-way check valve', 'start': 325.575, 'end': 365.911}, {'subject': 'two_14mm_bolts', 'predicate': 'hold', 'object': 'strap_to_gas_tank', 'start': 35.679, 'end': 41.803}, {'subject': 'Filler neck', 'predicate': 'contains', 'object': 'hollow tube', 'start': 390.148, 'end': 420.44}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    40 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     630.00 ms /   346 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     230.95 ms /   110 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 283 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'tank side', 'predicate': 'disconnect', 'object': 'two ventilation hoses', 'start': 180.282, 'end': 183.305}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'vacuum_hoses', 'predicate': 'have_been_disconnected', 'object': 'from this side of the EVAP canister', 'start': 15.2, 'end': 28.755}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'Fuel tank vapor pressure sensor', 'predicate': 'has', 'object': '3 terminals', 'start': 390.148, 'end': 420.44}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   283 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    20 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     451.01 ms /   303 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    30 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     458.21 ms /   109 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 287 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'check valve', 'predicate': 'needs removal', 'object': 'six pieces of rust', 'start': 196.161, 'end': 222.294}, {'subject': 'pulling out', 'predicate': 'action', 'object': 'check valves', 'start': 489.851, 'end': 494.159}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'check valve', 'predicate': 'is present in', 'object': 'here', 'start': 378.841, 'end': 385.745}, {'subject': 'Ball check valve', 'predicate': 'is part of', 'object': 'the check valve', 'start': 496.587, 'end': 534.03}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1423.04 ms /   397 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1117.20 ms /   162 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 311 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'fill check valve', 'predicate': 'opens valve', 'object': 'inside canister', 'start': 224.015, 'end': 264.245}, {'subject': 'fill check valve', 'predicate': 'allows', 'object': 'excess vapor to pass into the canister', 'start': 148.639, 'end': 180.042}, {'subject': 'fill check valve', 'predicate': 'allows escape of vapor', 'object': 'when gas tank is rapidly filling up with vapor', 'start': 224.015, 'end': 264.245}, {'subject': 'check valve', 'predicate': 'allows fluid entry', 'object': 'into the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'check valve', 'predicate': 'is present in', 'object': 'here', 'start': 378.841, 'end': 385.745}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   311 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     316.98 ms /   320 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     877.69 ms /   174 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 306 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'evap float', 'predicate': 'prevents', 'object': 'liquid gasoline', 'start': 196.161, 'end': 222.294}, {'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'float', 'predicate': 'prevents', 'object': 'liquid from going inside your charcoal canister', 'start': 266.787, 'end': 280.936}, {'subject': 'fuel tank', 'predicate': 'has', 'object': 'evap canister', 'start': 43.564, 'end': 148.419}, {'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 'absorb gasoline vapors', 'start': 469.037, 'end': 482.005}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     597.84 ms /   324 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.52 ms /   115 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 287 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'construction material', 'object': 'sheet metal', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'big steel', 'start': 29.356, 'end': 35.579}, {'subject': 'gas tank', 'predicate': 'construction method', 'object': 'stamping', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'joining method', 'object': 'welding', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     880.83 ms /   341 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     486.60 ms /   109 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 304 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'maximum fuel level', 'predicate': 'is limited by', 'object': 'the height of the fuel fill check valve', 'start': 325.575, 'end': 365.911}, {'subject': 'fuel pump', 'predicate': 'is located', 'object': 'at the lowest part of the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'check valves', 'predicate': 'function', 'object': 'control flow of gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'gas filler neck', 'predicate': 'is part of', 'object': 'fuel system', 'start': 367.711, 'end': 378.521}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   304 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     459.02 ms /   322 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1120.70 ms /   191 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 294 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'internal feature', 'object': 'baffles', 'start': 308.171, 'end': 325.335}, {'subject': 'baffle', 'predicate': 'prevents', 'object': 'fuel sloshing', 'start': 325.575, 'end': 365.911}, {'subject': 'baffles', 'predicate': 'location', 'object': 'perimeter of the tank', 'start': 308.171, 'end': 325.335}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'gas tank', 'predicate': 'construction method', 'object': 'stamping', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   294 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     326.12 ms /   305 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    22 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.27 ms /   120 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 320 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'check valve', 'predicate': 'allows fluid entry', 'object': 'into the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'diaphragm', 'predicate': 'can move', 'object': 'up and down', 'start': 281.796, 'end': 303.949}, {'subject': 'Check valve', 'predicate': 'acts as', 'object': 'a valve that releases air when a certain pressure is reached', 'start': 496.587, 'end': 534.03}, {'subject': 'fuel check valve', 'predicate': 'contains', 'object': 'lid', 'start': 281.796, 'end': 303.949}, {'subject': 'maximum fuel level', 'predicate': 'is limited by', 'object': 'the height of the fuel fill check valve', 'start': 325.575, 'end': 365.911}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   320 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     919.12 ms /   388 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   110 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1512.50 ms /   195 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 322 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 'absorb gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'check valves', 'predicate': 'function', 'object': 'control flow of gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'vacuum switching valve', 'predicate': 'allows', 'object': 'air flow from gas tank to charcoal canister', 'start': 435.808, 'end': 468.877}, {'subject': 'vacuum switching valve', 'predicate': 'functions as', 'object': 'redirecting air flow between gas tank and charcoal canister', 'start': 435.808, 'end': 468.877}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   322 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    39 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     906.84 ms /   361 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    39 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1046.85 ms /   142 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results saved to result_eval_Llama-3.2-1B-Instruct-f16.json\n",
      "[{'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side', 'start': 1.448, 'end': 15.18}, {'subject': 'EVAP line', 'predicate': 'is held on by', 'object': 'four pieces of rust', 'start': 185.836, 'end': 194.882}, {'subject': 'canister', 'predicate': 'is_held_in_by', 'object': 'two 12mm pieces of rust', 'start': 15.2, 'end': 28.755}, {'subject': 'evap canister', 'predicate': 'has', 'object': 'two lines', 'start': 43.564, 'end': 148.419}, {'subject': 'EVAP_canister', 'predicate': 'can_be_removed', 'object': 'from above the subframe', 'start': 15.2, 'end': 28.755}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 9 prefix-match hit, remaining 305 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   305 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    45 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1458.80 ms /   350 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     702.09 ms /   106 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 299 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'location', 'object': 'underneath the rear seat of the vehicle', 'start': 29.356, 'end': 35.579}, {'subject': 'fuel pump', 'predicate': 'is located', 'object': 'at the lowest part of the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side', 'start': 1.448, 'end': 15.18}, {'subject': 'gas tank', 'predicate': 'will be opened', 'object': \"to see what's inside\", 'start': 281.796, 'end': 303.949}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   299 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     558.77 ms /   318 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     851.91 ms /   160 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 292 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'two_14mm_bolts', 'predicate': 'hold', 'object': 'strap_to_gas_tank', 'start': 35.679, 'end': 41.803}, {'subject': 'gas tank', 'predicate': 'fixing', 'object': 'two straps', 'start': 29.356, 'end': 35.579}, {'subject': 'fuel filler neck', 'predicate': 'held in by', 'object': '14mm bolt', 'start': 43.564, 'end': 148.419}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'big steel', 'start': 29.356, 'end': 35.579}, {'subject': 'gas tank', 'predicate': 'location', 'object': 'underneath the rear seat of the vehicle', 'start': 29.356, 'end': 35.579}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   292 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     600.83 ms /   311 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     343.03 ms /   114 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 306 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'fuel filler neck', 'predicate': 'held in by', 'object': '14mm bolt', 'start': 43.564, 'end': 148.419}, {'subject': 'gas filler neck', 'predicate': 'is part of', 'object': 'fuel system', 'start': 367.711, 'end': 378.521}, {'subject': 'fuel filler neck', 'predicate': 'contains', 'object': 'a one-way check valve', 'start': 325.575, 'end': 365.911}, {'subject': 'two_14mm_bolts', 'predicate': 'hold', 'object': 'strap_to_gas_tank', 'start': 35.679, 'end': 41.803}, {'subject': 'Filler neck', 'predicate': 'contains', 'object': 'hollow tube', 'start': 390.148, 'end': 420.44}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1143.04 ms /   385 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     685.90 ms /   153 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 283 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'tank side', 'predicate': 'disconnect', 'object': 'two ventilation hoses', 'start': 180.282, 'end': 183.305}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'vacuum_hoses', 'predicate': 'have_been_disconnected', 'object': 'from this side of the EVAP canister', 'start': 15.2, 'end': 28.755}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'Fuel tank vapor pressure sensor', 'predicate': 'has', 'object': '3 terminals', 'start': 390.148, 'end': 420.44}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   283 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    16 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     378.92 ms /   299 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    28 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.90 ms /   107 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 287 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'check valve', 'predicate': 'needs removal', 'object': 'six pieces of rust', 'start': 196.161, 'end': 222.294}, {'subject': 'pulling out', 'predicate': 'action', 'object': 'check valves', 'start': 489.851, 'end': 494.159}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'check valve', 'predicate': 'is present in', 'object': 'here', 'start': 378.841, 'end': 385.745}, {'subject': 'Ball check valve', 'predicate': 'is part of', 'object': 'the check valve', 'start': 496.587, 'end': 534.03}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   159 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1759.81 ms /   446 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   234 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3437.77 ms /   326 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 311 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'fill check valve', 'predicate': 'opens valve', 'object': 'inside canister', 'start': 224.015, 'end': 264.245}, {'subject': 'fill check valve', 'predicate': 'allows', 'object': 'excess vapor to pass into the canister', 'start': 148.639, 'end': 180.042}, {'subject': 'fill check valve', 'predicate': 'allows escape of vapor', 'object': 'when gas tank is rapidly filling up with vapor', 'start': 224.015, 'end': 264.245}, {'subject': 'check valve', 'predicate': 'allows fluid entry', 'object': 'into the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'check valve', 'predicate': 'is present in', 'object': 'here', 'start': 378.841, 'end': 385.745}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   311 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     360.86 ms /   320 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     341.93 ms /   106 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 306 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'evap float', 'predicate': 'prevents', 'object': 'liquid gasoline', 'start': 196.161, 'end': 222.294}, {'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'float', 'predicate': 'prevents', 'object': 'liquid from going inside your charcoal canister', 'start': 266.787, 'end': 280.936}, {'subject': 'fuel tank', 'predicate': 'has', 'object': 'evap canister', 'start': 43.564, 'end': 148.419}, {'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 'absorb gasoline vapors', 'start': 469.037, 'end': 482.005}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.88 ms /   324 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1161.09 ms /   207 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 287 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'construction material', 'object': 'sheet metal', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'big steel', 'start': 29.356, 'end': 35.579}, {'subject': 'gas tank', 'predicate': 'construction method', 'object': 'stamping', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'joining method', 'object': 'welding', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     937.35 ms /   360 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     962.94 ms /   181 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 304 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'maximum fuel level', 'predicate': 'is limited by', 'object': 'the height of the fuel fill check valve', 'start': 325.575, 'end': 365.911}, {'subject': 'fuel pump', 'predicate': 'is located', 'object': 'at the lowest part of the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'check valves', 'predicate': 'function', 'object': 'control flow of gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'gas filler neck', 'predicate': 'is part of', 'object': 'fuel system', 'start': 367.711, 'end': 378.521}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   304 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     637.07 ms /   319 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    29 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     453.59 ms /   121 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 294 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'internal feature', 'object': 'baffles', 'start': 308.171, 'end': 325.335}, {'subject': 'baffle', 'predicate': 'prevents', 'object': 'fuel sloshing', 'start': 325.575, 'end': 365.911}, {'subject': 'baffles', 'predicate': 'location', 'object': 'perimeter of the tank', 'start': 308.171, 'end': 325.335}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'gas tank', 'predicate': 'construction method', 'object': 'stamping', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   294 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     366.24 ms /   305 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    25 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     384.07 ms /   123 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 320 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'check valve', 'predicate': 'allows fluid entry', 'object': 'into the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'diaphragm', 'predicate': 'can move', 'object': 'up and down', 'start': 281.796, 'end': 303.949}, {'subject': 'Check valve', 'predicate': 'acts as', 'object': 'a valve that releases air when a certain pressure is reached', 'start': 496.587, 'end': 534.03}, {'subject': 'fuel check valve', 'predicate': 'contains', 'object': 'lid', 'start': 281.796, 'end': 303.949}, {'subject': 'maximum fuel level', 'predicate': 'is limited by', 'object': 'the height of the fuel fill check valve', 'start': 325.575, 'end': 365.911}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   320 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1008.85 ms /   394 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   110 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1065.46 ms /   198 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 322 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 'absorb gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'check valves', 'predicate': 'function', 'object': 'control flow of gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'vacuum switching valve', 'predicate': 'allows', 'object': 'air flow from gas tank to charcoal canister', 'start': 435.808, 'end': 468.877}, {'subject': 'vacuum switching valve', 'predicate': 'functions as', 'object': 'redirecting air flow between gas tank and charcoal canister', 'start': 435.808, 'end': 468.877}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   322 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     504.13 ms /   345 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    39 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     661.23 ms /   142 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results saved to result_eval_gemma-2-2b-it.F16.json\n",
      "[{'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side', 'start': 1.448, 'end': 15.18}, {'subject': 'EVAP line', 'predicate': 'is held on by', 'object': 'four pieces of rust', 'start': 185.836, 'end': 194.882}, {'subject': 'canister', 'predicate': 'is_held_in_by', 'object': 'two 12mm pieces of rust', 'start': 15.2, 'end': 28.755}, {'subject': 'evap canister', 'predicate': 'has', 'object': 'two lines', 'start': 43.564, 'end': 148.419}, {'subject': 'EVAP_canister', 'predicate': 'can_be_removed', 'object': 'from above the subframe', 'start': 15.2, 'end': 28.755}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 9 prefix-match hit, remaining 305 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   305 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    39 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     807.55 ms /   344 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     8 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     257.88 ms /   108 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 299 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'location', 'object': 'underneath the rear seat of the vehicle', 'start': 29.356, 'end': 35.579}, {'subject': 'fuel pump', 'predicate': 'is located', 'object': 'at the lowest part of the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side', 'start': 1.448, 'end': 15.18}, {'subject': 'gas tank', 'predicate': 'will be opened', 'object': \"to see what's inside\", 'start': 281.796, 'end': 303.949}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   299 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    36 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     703.10 ms /   335 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     318.07 ms /   107 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 292 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'two_14mm_bolts', 'predicate': 'hold', 'object': 'strap_to_gas_tank', 'start': 35.679, 'end': 41.803}, {'subject': 'gas tank', 'predicate': 'fixing', 'object': 'two straps', 'start': 29.356, 'end': 35.579}, {'subject': 'fuel filler neck', 'predicate': 'held in by', 'object': '14mm bolt', 'start': 43.564, 'end': 148.419}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'big steel', 'start': 29.356, 'end': 35.579}, {'subject': 'gas tank', 'predicate': 'location', 'object': 'underneath the rear seat of the vehicle', 'start': 29.356, 'end': 35.579}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   292 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    25 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     504.81 ms /   317 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     451.22 ms /   114 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 306 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'fuel filler neck', 'predicate': 'held in by', 'object': '14mm bolt', 'start': 43.564, 'end': 148.419}, {'subject': 'gas filler neck', 'predicate': 'is part of', 'object': 'fuel system', 'start': 367.711, 'end': 378.521}, {'subject': 'fuel filler neck', 'predicate': 'contains', 'object': 'a one-way check valve', 'start': 325.575, 'end': 365.911}, {'subject': 'two_14mm_bolts', 'predicate': 'hold', 'object': 'strap_to_gas_tank', 'start': 35.679, 'end': 41.803}, {'subject': 'Filler neck', 'predicate': 'contains', 'object': 'hollow tube', 'start': 390.148, 'end': 420.44}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    45 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     785.27 ms /   351 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     741.24 ms /   158 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 283 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'tank side', 'predicate': 'disconnect', 'object': 'two ventilation hoses', 'start': 180.282, 'end': 183.305}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'vacuum_hoses', 'predicate': 'have_been_disconnected', 'object': 'from this side of the EVAP canister', 'start': 15.2, 'end': 28.755}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'Fuel tank vapor pressure sensor', 'predicate': 'has', 'object': '3 terminals', 'start': 390.148, 'end': 420.44}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   283 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    39 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     587.47 ms /   322 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    31 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     434.11 ms /   110 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 287 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'check valve', 'predicate': 'needs removal', 'object': 'six pieces of rust', 'start': 196.161, 'end': 222.294}, {'subject': 'pulling out', 'predicate': 'action', 'object': 'check valves', 'start': 489.851, 'end': 494.159}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'check valve', 'predicate': 'is present in', 'object': 'here', 'start': 378.841, 'end': 385.745}, {'subject': 'Ball check valve', 'predicate': 'is part of', 'object': 'the check valve', 'start': 496.587, 'end': 534.03}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   123 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1387.58 ms /   410 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   234 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2462.28 ms /   326 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 311 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'fill check valve', 'predicate': 'opens valve', 'object': 'inside canister', 'start': 224.015, 'end': 264.245}, {'subject': 'fill check valve', 'predicate': 'allows', 'object': 'excess vapor to pass into the canister', 'start': 148.639, 'end': 180.042}, {'subject': 'fill check valve', 'predicate': 'allows escape of vapor', 'object': 'when gas tank is rapidly filling up with vapor', 'start': 224.015, 'end': 264.245}, {'subject': 'check valve', 'predicate': 'allows fluid entry', 'object': 'into the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'check valve', 'predicate': 'is present in', 'object': 'here', 'start': 378.841, 'end': 385.745}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   311 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     342.26 ms /   320 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     337.73 ms /   109 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 306 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'evap float', 'predicate': 'prevents', 'object': 'liquid gasoline', 'start': 196.161, 'end': 222.294}, {'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'float', 'predicate': 'prevents', 'object': 'liquid from going inside your charcoal canister', 'start': 266.787, 'end': 280.936}, {'subject': 'fuel tank', 'predicate': 'has', 'object': 'evap canister', 'start': 43.564, 'end': 148.419}, {'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 'absorb gasoline vapors', 'start': 469.037, 'end': 482.005}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    20 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     638.83 ms /   326 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     918.58 ms /   172 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 287 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'construction material', 'object': 'sheet metal', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'big steel', 'start': 29.356, 'end': 35.579}, {'subject': 'gas tank', 'predicate': 'construction method', 'object': 'stamping', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'joining method', 'object': 'welding', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     958.83 ms /   353 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     487.90 ms /   109 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 304 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'maximum fuel level', 'predicate': 'is limited by', 'object': 'the height of the fuel fill check valve', 'start': 325.575, 'end': 365.911}, {'subject': 'fuel pump', 'predicate': 'is located', 'object': 'at the lowest part of the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'check valves', 'predicate': 'function', 'object': 'control flow of gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'gas filler neck', 'predicate': 'is part of', 'object': 'fuel system', 'start': 367.711, 'end': 378.521}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   304 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.01 ms /   319 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    87 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1010.11 ms /   179 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 294 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'internal feature', 'object': 'baffles', 'start': 308.171, 'end': 325.335}, {'subject': 'baffle', 'predicate': 'prevents', 'object': 'fuel sloshing', 'start': 325.575, 'end': 365.911}, {'subject': 'baffles', 'predicate': 'location', 'object': 'perimeter of the tank', 'start': 308.171, 'end': 325.335}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'gas tank', 'predicate': 'construction method', 'object': 'stamping', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   294 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     869.51 ms /   349 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     374.99 ms /   109 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 320 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'check valve', 'predicate': 'allows fluid entry', 'object': 'into the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'diaphragm', 'predicate': 'can move', 'object': 'up and down', 'start': 281.796, 'end': 303.949}, {'subject': 'Check valve', 'predicate': 'acts as', 'object': 'a valve that releases air when a certain pressure is reached', 'start': 496.587, 'end': 534.03}, {'subject': 'fuel check valve', 'predicate': 'contains', 'object': 'lid', 'start': 281.796, 'end': 303.949}, {'subject': 'maximum fuel level', 'predicate': 'is limited by', 'object': 'the height of the fuel fill check valve', 'start': 325.575, 'end': 365.911}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   320 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1171.17 ms /   387 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   110 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1063.31 ms /   194 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 322 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 'absorb gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'check valves', 'predicate': 'function', 'object': 'control flow of gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'vacuum switching valve', 'predicate': 'allows', 'object': 'air flow from gas tank to charcoal canister', 'start': 435.808, 'end': 468.877}, {'subject': 'vacuum switching valve', 'predicate': 'functions as', 'object': 'redirecting air flow between gas tank and charcoal canister', 'start': 435.808, 'end': 468.877}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   322 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    29 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     623.58 ms /   351 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    39 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     559.45 ms /   142 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results saved to result_eval_Llama-3.2-3B-Instruct-f16.json\n",
      "[{'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side', 'start': 1.448, 'end': 15.18}, {'subject': 'EVAP line', 'predicate': 'is held on by', 'object': 'four pieces of rust', 'start': 185.836, 'end': 194.882}, {'subject': 'canister', 'predicate': 'is_held_in_by', 'object': 'two 12mm pieces of rust', 'start': 15.2, 'end': 28.755}, {'subject': 'evap canister', 'predicate': 'has', 'object': 'two lines', 'start': 43.564, 'end': 148.419}, {'subject': 'EVAP_canister', 'predicate': 'can_be_removed', 'object': 'from above the subframe', 'start': 15.2, 'end': 28.755}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 9 prefix-match hit, remaining 305 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   305 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     978.20 ms /   375 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     412.65 ms /   109 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 299 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'location', 'object': 'underneath the rear seat of the vehicle', 'start': 29.356, 'end': 35.579}, {'subject': 'fuel pump', 'predicate': 'is located', 'object': 'at the lowest part of the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side', 'start': 1.448, 'end': 15.18}, {'subject': 'gas tank', 'predicate': 'will be opened', 'object': \"to see what's inside\", 'start': 281.796, 'end': 303.949}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   299 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    13 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.82 ms /   312 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    46 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     733.62 ms /   140 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 292 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'two_14mm_bolts', 'predicate': 'hold', 'object': 'strap_to_gas_tank', 'start': 35.679, 'end': 41.803}, {'subject': 'gas tank', 'predicate': 'fixing', 'object': 'two straps', 'start': 29.356, 'end': 35.579}, {'subject': 'fuel filler neck', 'predicate': 'held in by', 'object': '14mm bolt', 'start': 43.564, 'end': 148.419}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'big steel', 'start': 29.356, 'end': 35.579}, {'subject': 'gas tank', 'predicate': 'location', 'object': 'underneath the rear seat of the vehicle', 'start': 29.356, 'end': 35.579}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   292 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    47 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     804.63 ms /   339 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     471.10 ms /   114 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 306 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'fuel filler neck', 'predicate': 'held in by', 'object': '14mm bolt', 'start': 43.564, 'end': 148.419}, {'subject': 'gas filler neck', 'predicate': 'is part of', 'object': 'fuel system', 'start': 367.711, 'end': 378.521}, {'subject': 'fuel filler neck', 'predicate': 'contains', 'object': 'a one-way check valve', 'start': 325.575, 'end': 365.911}, {'subject': 'two_14mm_bolts', 'predicate': 'hold', 'object': 'strap_to_gas_tank', 'start': 35.679, 'end': 41.803}, {'subject': 'Filler neck', 'predicate': 'contains', 'object': 'hollow tube', 'start': 390.148, 'end': 420.44}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    38 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     750.80 ms /   344 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     243.34 ms /   110 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 283 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'tank side', 'predicate': 'disconnect', 'object': 'two ventilation hoses', 'start': 180.282, 'end': 183.305}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'vacuum_hoses', 'predicate': 'have_been_disconnected', 'object': 'from this side of the EVAP canister', 'start': 15.2, 'end': 28.755}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'Fuel tank vapor pressure sensor', 'predicate': 'has', 'object': '3 terminals', 'start': 390.148, 'end': 420.44}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   283 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     922.97 ms /   350 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    32 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     511.95 ms /   111 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 287 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'check valve', 'predicate': 'needs removal', 'object': 'six pieces of rust', 'start': 196.161, 'end': 222.294}, {'subject': 'pulling out', 'predicate': 'action', 'object': 'check valves', 'start': 489.851, 'end': 494.159}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'check valve', 'predicate': 'is present in', 'object': 'here', 'start': 378.841, 'end': 385.745}, {'subject': 'Ball check valve', 'predicate': 'is part of', 'object': 'the check valve', 'start': 496.587, 'end': 534.03}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    26 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     478.99 ms /   313 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     841.10 ms /   164 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 311 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'fill check valve', 'predicate': 'opens valve', 'object': 'inside canister', 'start': 224.015, 'end': 264.245}, {'subject': 'fill check valve', 'predicate': 'allows', 'object': 'excess vapor to pass into the canister', 'start': 148.639, 'end': 180.042}, {'subject': 'fill check valve', 'predicate': 'allows escape of vapor', 'object': 'when gas tank is rapidly filling up with vapor', 'start': 224.015, 'end': 264.245}, {'subject': 'check valve', 'predicate': 'allows fluid entry', 'object': 'into the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'check valve', 'predicate': 'is present in', 'object': 'here', 'start': 378.841, 'end': 385.745}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   311 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     360.83 ms /   320 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     931.98 ms /   179 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 306 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'evap float', 'predicate': 'prevents', 'object': 'liquid gasoline', 'start': 196.161, 'end': 222.294}, {'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'float', 'predicate': 'prevents', 'object': 'liquid from going inside your charcoal canister', 'start': 266.787, 'end': 280.936}, {'subject': 'fuel tank', 'predicate': 'has', 'object': 'evap canister', 'start': 43.564, 'end': 148.419}, {'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 'absorb gasoline vapors', 'start': 469.037, 'end': 482.005}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     430.37 ms /   318 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     895.77 ms /   115 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 287 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'construction material', 'object': 'sheet metal', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'big steel', 'start': 29.356, 'end': 35.579}, {'subject': 'gas tank', 'predicate': 'construction method', 'object': 'stamping', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'joining method', 'object': 'welding', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    41 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     647.56 ms /   328 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     796.96 ms /   153 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 304 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'maximum fuel level', 'predicate': 'is limited by', 'object': 'the height of the fuel fill check valve', 'start': 325.575, 'end': 365.911}, {'subject': 'fuel pump', 'predicate': 'is located', 'object': 'at the lowest part of the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'check valves', 'predicate': 'function', 'object': 'control flow of gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'gas filler neck', 'predicate': 'is part of', 'object': 'fuel system', 'start': 367.711, 'end': 378.521}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   304 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.08 ms /   319 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1247.89 ms /   190 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 294 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'internal feature', 'object': 'baffles', 'start': 308.171, 'end': 325.335}, {'subject': 'baffle', 'predicate': 'prevents', 'object': 'fuel sloshing', 'start': 325.575, 'end': 365.911}, {'subject': 'baffles', 'predicate': 'location', 'object': 'perimeter of the tank', 'start': 308.171, 'end': 325.335}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'gas tank', 'predicate': 'construction method', 'object': 'stamping', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   294 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     374.31 ms /   305 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    41 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     548.13 ms /   139 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 320 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'check valve', 'predicate': 'allows fluid entry', 'object': 'into the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'diaphragm', 'predicate': 'can move', 'object': 'up and down', 'start': 281.796, 'end': 303.949}, {'subject': 'Check valve', 'predicate': 'acts as', 'object': 'a valve that releases air when a certain pressure is reached', 'start': 496.587, 'end': 534.03}, {'subject': 'fuel check valve', 'predicate': 'contains', 'object': 'lid', 'start': 281.796, 'end': 303.949}, {'subject': 'maximum fuel level', 'predicate': 'is limited by', 'object': 'the height of the fuel fill check valve', 'start': 325.575, 'end': 365.911}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   320 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1037.37 ms /   388 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   110 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1220.86 ms /   211 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 322 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 'absorb gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'check valves', 'predicate': 'function', 'object': 'control flow of gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'vacuum switching valve', 'predicate': 'allows', 'object': 'air flow from gas tank to charcoal canister', 'start': 435.808, 'end': 468.877}, {'subject': 'vacuum switching valve', 'predicate': 'functions as', 'object': 'redirecting air flow between gas tank and charcoal canister', 'start': 435.808, 'end': 468.877}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   322 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    39 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     680.68 ms /   361 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     874.06 ms /   177 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results saved to result_eval_Qwen2.5-7B-Instruct-Q4_K_M.json\n",
      "[{'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side', 'start': 1.448, 'end': 15.18}, {'subject': 'EVAP line', 'predicate': 'is held on by', 'object': 'four pieces of rust', 'start': 185.836, 'end': 194.882}, {'subject': 'canister', 'predicate': 'is_held_in_by', 'object': 'two 12mm pieces of rust', 'start': 15.2, 'end': 28.755}, {'subject': 'evap canister', 'predicate': 'has', 'object': 'two lines', 'start': 43.564, 'end': 148.419}, {'subject': 'EVAP_canister', 'predicate': 'can_be_removed', 'object': 'from above the subframe', 'start': 15.2, 'end': 28.755}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 9 prefix-match hit, remaining 305 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   305 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2319.07 ms /   360 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     7 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     847.29 ms /   107 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 299 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'location', 'object': 'underneath the rear seat of the vehicle', 'start': 29.356, 'end': 35.579}, {'subject': 'fuel pump', 'predicate': 'is located', 'object': 'at the lowest part of the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side', 'start': 1.448, 'end': 15.18}, {'subject': 'gas tank', 'predicate': 'will be opened', 'object': \"to see what's inside\", 'start': 281.796, 'end': 303.949}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   299 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    20 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     552.09 ms /   319 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     722.89 ms /   152 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 292 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'two_14mm_bolts', 'predicate': 'hold', 'object': 'strap_to_gas_tank', 'start': 35.679, 'end': 41.803}, {'subject': 'gas tank', 'predicate': 'fixing', 'object': 'two straps', 'start': 29.356, 'end': 35.579}, {'subject': 'fuel filler neck', 'predicate': 'held in by', 'object': '14mm bolt', 'start': 43.564, 'end': 148.419}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'big steel', 'start': 29.356, 'end': 35.579}, {'subject': 'gas tank', 'predicate': 'location', 'object': 'underneath the rear seat of the vehicle', 'start': 29.356, 'end': 35.579}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   292 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     428.29 ms /   311 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     338.06 ms /   114 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 306 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'fuel filler neck', 'predicate': 'held in by', 'object': '14mm bolt', 'start': 43.564, 'end': 148.419}, {'subject': 'gas filler neck', 'predicate': 'is part of', 'object': 'fuel system', 'start': 367.711, 'end': 378.521}, {'subject': 'fuel filler neck', 'predicate': 'contains', 'object': 'a one-way check valve', 'start': 325.575, 'end': 365.911}, {'subject': 'two_14mm_bolts', 'predicate': 'hold', 'object': 'strap_to_gas_tank', 'start': 35.679, 'end': 41.803}, {'subject': 'Filler neck', 'predicate': 'contains', 'object': 'hollow tube', 'start': 390.148, 'end': 420.44}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    29 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     521.89 ms /   335 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     343.35 ms /   120 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 283 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'tank side', 'predicate': 'disconnect', 'object': 'two ventilation hoses', 'start': 180.282, 'end': 183.305}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'vacuum_hoses', 'predicate': 'have_been_disconnected', 'object': 'from this side of the EVAP canister', 'start': 15.2, 'end': 28.755}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'Fuel tank vapor pressure sensor', 'predicate': 'has', 'object': '3 terminals', 'start': 390.148, 'end': 420.44}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   283 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     223.97 ms /   284 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    27 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     467.21 ms /   106 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 287 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'check valve', 'predicate': 'needs removal', 'object': 'six pieces of rust', 'start': 196.161, 'end': 222.294}, {'subject': 'pulling out', 'predicate': 'action', 'object': 'check valves', 'start': 489.851, 'end': 494.159}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'check valve', 'predicate': 'is present in', 'object': 'here', 'start': 378.841, 'end': 385.745}, {'subject': 'Ball check valve', 'predicate': 'is part of', 'object': 'the check valve', 'start': 496.587, 'end': 534.03}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     742.46 ms /   338 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   422 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4406.37 ms /   514 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 311 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'fill check valve', 'predicate': 'opens valve', 'object': 'inside canister', 'start': 224.015, 'end': 264.245}, {'subject': 'fill check valve', 'predicate': 'allows', 'object': 'excess vapor to pass into the canister', 'start': 148.639, 'end': 180.042}, {'subject': 'fill check valve', 'predicate': 'allows escape of vapor', 'object': 'when gas tank is rapidly filling up with vapor', 'start': 224.015, 'end': 264.245}, {'subject': 'check valve', 'predicate': 'allows fluid entry', 'object': 'into the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'check valve', 'predicate': 'is present in', 'object': 'here', 'start': 378.841, 'end': 385.745}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   311 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     355.17 ms /   320 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     243.60 ms /   109 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 306 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'evap float', 'predicate': 'prevents', 'object': 'liquid gasoline', 'start': 196.161, 'end': 222.294}, {'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'float', 'predicate': 'prevents', 'object': 'liquid from going inside your charcoal canister', 'start': 266.787, 'end': 280.936}, {'subject': 'fuel tank', 'predicate': 'has', 'object': 'evap canister', 'start': 43.564, 'end': 148.419}, {'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 'absorb gasoline vapors', 'start': 469.037, 'end': 482.005}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     424.78 ms /   325 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     991.63 ms /   164 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 287 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'construction material', 'object': 'sheet metal', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'big steel', 'start': 29.356, 'end': 35.579}, {'subject': 'gas tank', 'predicate': 'construction method', 'object': 'stamping', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'joining method', 'object': 'welding', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    43 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     651.02 ms /   330 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    10 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     254.11 ms /   109 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 304 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'maximum fuel level', 'predicate': 'is limited by', 'object': 'the height of the fuel fill check valve', 'start': 325.575, 'end': 365.911}, {'subject': 'fuel pump', 'predicate': 'is located', 'object': 'at the lowest part of the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'check valves', 'predicate': 'function', 'object': 'control flow of gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'gas filler neck', 'predicate': 'is part of', 'object': 'fuel system', 'start': 367.711, 'end': 378.521}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   304 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     445.80 ms /   319 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    78 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     927.17 ms /   170 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 294 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'internal feature', 'object': 'baffles', 'start': 308.171, 'end': 325.335}, {'subject': 'baffle', 'predicate': 'prevents', 'object': 'fuel sloshing', 'start': 325.575, 'end': 365.911}, {'subject': 'baffles', 'predicate': 'location', 'object': 'perimeter of the tank', 'start': 308.171, 'end': 325.335}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'gas tank', 'predicate': 'construction method', 'object': 'stamping', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   294 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     385.36 ms /   305 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    41 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     585.48 ms /   139 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 320 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'check valve', 'predicate': 'allows fluid entry', 'object': 'into the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'diaphragm', 'predicate': 'can move', 'object': 'up and down', 'start': 281.796, 'end': 303.949}, {'subject': 'Check valve', 'predicate': 'acts as', 'object': 'a valve that releases air when a certain pressure is reached', 'start': 496.587, 'end': 534.03}, {'subject': 'fuel check valve', 'predicate': 'contains', 'object': 'lid', 'start': 281.796, 'end': 303.949}, {'subject': 'maximum fuel level', 'predicate': 'is limited by', 'object': 'the height of the fuel fill check valve', 'start': 325.575, 'end': 365.911}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   320 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    78 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     975.65 ms /   398 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   110 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1131.57 ms /   212 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 322 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 'absorb gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'check valves', 'predicate': 'function', 'object': 'control flow of gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'vacuum switching valve', 'predicate': 'allows', 'object': 'air flow from gas tank to charcoal canister', 'start': 435.808, 'end': 468.877}, {'subject': 'vacuum switching valve', 'predicate': 'functions as', 'object': 'redirecting air flow between gas tank and charcoal canister', 'start': 435.808, 'end': 468.877}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   322 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     494.07 ms /   345 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    35 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     490.29 ms /   138 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results saved to result_eval_Llama-3.2-11B-Vision-Instruct.Q4_K_M.json\n",
      "[{'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side', 'start': 1.448, 'end': 15.18}, {'subject': 'EVAP line', 'predicate': 'is held on by', 'object': 'four pieces of rust', 'start': 185.836, 'end': 194.882}, {'subject': 'canister', 'predicate': 'is_held_in_by', 'object': 'two 12mm pieces of rust', 'start': 15.2, 'end': 28.755}, {'subject': 'evap canister', 'predicate': 'has', 'object': 'two lines', 'start': 43.564, 'end': 148.419}, {'subject': 'EVAP_canister', 'predicate': 'can_be_removed', 'object': 'from above the subframe', 'start': 15.2, 'end': 28.755}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 9 prefix-match hit, remaining 305 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   305 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    47 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     739.10 ms /   352 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1131.31 ms /   198 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 299 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'location', 'object': 'underneath the rear seat of the vehicle', 'start': 29.356, 'end': 35.579}, {'subject': 'fuel pump', 'predicate': 'is located', 'object': 'at the lowest part of the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side', 'start': 1.448, 'end': 15.18}, {'subject': 'gas tank', 'predicate': 'will be opened', 'object': \"to see what's inside\", 'start': 281.796, 'end': 303.949}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   299 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     814.51 ms /   347 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    26 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     466.76 ms /   120 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 292 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'two_14mm_bolts', 'predicate': 'hold', 'object': 'strap_to_gas_tank', 'start': 35.679, 'end': 41.803}, {'subject': 'gas tank', 'predicate': 'fixing', 'object': 'two straps', 'start': 29.356, 'end': 35.579}, {'subject': 'fuel filler neck', 'predicate': 'held in by', 'object': '14mm bolt', 'start': 43.564, 'end': 148.419}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'big steel', 'start': 29.356, 'end': 35.579}, {'subject': 'gas tank', 'predicate': 'location', 'object': 'underneath the rear seat of the vehicle', 'start': 29.356, 'end': 35.579}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   292 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    25 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     596.25 ms /   317 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    19 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     343.71 ms /   114 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 306 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'fuel filler neck', 'predicate': 'held in by', 'object': '14mm bolt', 'start': 43.564, 'end': 148.419}, {'subject': 'gas filler neck', 'predicate': 'is part of', 'object': 'fuel system', 'start': 367.711, 'end': 378.521}, {'subject': 'fuel filler neck', 'predicate': 'contains', 'object': 'a one-way check valve', 'start': 325.575, 'end': 365.911}, {'subject': 'two_14mm_bolts', 'predicate': 'hold', 'object': 'strap_to_gas_tank', 'start': 35.679, 'end': 41.803}, {'subject': 'Filler neck', 'predicate': 'contains', 'object': 'hollow tube', 'start': 390.148, 'end': 420.44}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    42 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     643.34 ms /   348 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     808.03 ms /   168 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 283 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'tank side', 'predicate': 'disconnect', 'object': 'two ventilation hoses', 'start': 180.282, 'end': 183.305}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'vacuum_hoses', 'predicate': 'have_been_disconnected', 'object': 'from this side of the EVAP canister', 'start': 15.2, 'end': 28.755}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'Fuel tank vapor pressure sensor', 'predicate': 'has', 'object': '3 terminals', 'start': 390.148, 'end': 420.44}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   283 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    59 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2910.67 ms /   342 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    42 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     580.89 ms /   121 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 287 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'check valve', 'predicate': 'needs removal', 'object': 'six pieces of rust', 'start': 196.161, 'end': 222.294}, {'subject': 'pulling out', 'predicate': 'action', 'object': 'check valves', 'start': 489.851, 'end': 494.159}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'check valve', 'predicate': 'is present in', 'object': 'here', 'start': 378.841, 'end': 385.745}, {'subject': 'Ball check valve', 'predicate': 'is part of', 'object': 'the check valve', 'start': 496.587, 'end': 534.03}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1042.23 ms /   366 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   397 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4390.30 ms /   489 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 311 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'fill check valve', 'predicate': 'opens valve', 'object': 'inside canister', 'start': 224.015, 'end': 264.245}, {'subject': 'fill check valve', 'predicate': 'allows', 'object': 'excess vapor to pass into the canister', 'start': 148.639, 'end': 180.042}, {'subject': 'fill check valve', 'predicate': 'allows escape of vapor', 'object': 'when gas tank is rapidly filling up with vapor', 'start': 224.015, 'end': 264.245}, {'subject': 'check valve', 'predicate': 'allows fluid entry', 'object': 'into the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'check valve', 'predicate': 'is present in', 'object': 'here', 'start': 378.841, 'end': 385.745}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   311 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     371.30 ms /   320 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     234.54 ms /   106 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 306 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'evap float', 'predicate': 'prevents', 'object': 'liquid gasoline', 'start': 196.161, 'end': 222.294}, {'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'float', 'predicate': 'prevents', 'object': 'liquid from going inside your charcoal canister', 'start': 266.787, 'end': 280.936}, {'subject': 'fuel tank', 'predicate': 'has', 'object': 'evap canister', 'start': 43.564, 'end': 148.419}, {'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 'absorb gasoline vapors', 'start': 469.037, 'end': 482.005}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     860.22 ms /   362 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     261.47 ms /   115 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 287 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'construction material', 'object': 'sheet metal', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'big steel', 'start': 29.356, 'end': 35.579}, {'subject': 'gas tank', 'predicate': 'construction method', 'object': 'stamping', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'joining method', 'object': 'welding', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     771.12 ms /   345 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1002.92 ms /   170 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 304 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'maximum fuel level', 'predicate': 'is limited by', 'object': 'the height of the fuel fill check valve', 'start': 325.575, 'end': 365.911}, {'subject': 'fuel pump', 'predicate': 'is located', 'object': 'at the lowest part of the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'check valves', 'predicate': 'function', 'object': 'control flow of gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'gas filler neck', 'predicate': 'is part of', 'object': 'fuel system', 'start': 367.711, 'end': 378.521}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   304 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     835.61 ms /   365 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     991.02 ms /   178 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 294 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'internal feature', 'object': 'baffles', 'start': 308.171, 'end': 325.335}, {'subject': 'baffle', 'predicate': 'prevents', 'object': 'fuel sloshing', 'start': 325.575, 'end': 365.911}, {'subject': 'baffles', 'predicate': 'location', 'object': 'perimeter of the tank', 'start': 308.171, 'end': 325.335}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'gas tank', 'predicate': 'construction method', 'object': 'stamping', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   294 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     536.42 ms /   317 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     783.65 ms /   162 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 320 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'check valve', 'predicate': 'allows fluid entry', 'object': 'into the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'diaphragm', 'predicate': 'can move', 'object': 'up and down', 'start': 281.796, 'end': 303.949}, {'subject': 'Check valve', 'predicate': 'acts as', 'object': 'a valve that releases air when a certain pressure is reached', 'start': 496.587, 'end': 534.03}, {'subject': 'fuel check valve', 'predicate': 'contains', 'object': 'lid', 'start': 281.796, 'end': 303.949}, {'subject': 'maximum fuel level', 'predicate': 'is limited by', 'object': 'the height of the fuel fill check valve', 'start': 325.575, 'end': 365.911}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   320 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     911.31 ms /   387 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   110 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     878.93 ms /   185 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 322 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 'absorb gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'check valves', 'predicate': 'function', 'object': 'control flow of gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'vacuum switching valve', 'predicate': 'allows', 'object': 'air flow from gas tank to charcoal canister', 'start': 435.808, 'end': 468.877}, {'subject': 'vacuum switching valve', 'predicate': 'functions as', 'object': 'redirecting air flow between gas tank and charcoal canister', 'start': 435.808, 'end': 468.877}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   322 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    44 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     660.33 ms /   366 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    33 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     473.03 ms /   136 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results saved to result_eval_gemma-2-9b-it-Q4_K_M.json\n",
      "[{'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side', 'start': 1.448, 'end': 15.18}, {'subject': 'EVAP line', 'predicate': 'is held on by', 'object': 'four pieces of rust', 'start': 185.836, 'end': 194.882}, {'subject': 'canister', 'predicate': 'is_held_in_by', 'object': 'two 12mm pieces of rust', 'start': 15.2, 'end': 28.755}, {'subject': 'evap canister', 'predicate': 'has', 'object': 'two lines', 'start': 43.564, 'end': 148.419}, {'subject': 'EVAP_canister', 'predicate': 'can_be_removed', 'object': 'from above the subframe', 'start': 15.2, 'end': 28.755}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 9 prefix-match hit, remaining 305 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   305 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     786.26 ms /   361 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     229.92 ms /   106 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 299 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'location', 'object': 'underneath the rear seat of the vehicle', 'start': 29.356, 'end': 35.579}, {'subject': 'fuel pump', 'predicate': 'is located', 'object': 'at the lowest part of the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'EVAP canister', 'predicate': 'location', 'object': 'underneath the vehicle on the right side', 'start': 1.448, 'end': 15.18}, {'subject': 'gas tank', 'predicate': 'will be opened', 'object': \"to see what's inside\", 'start': 281.796, 'end': 303.949}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   299 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    36 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     583.43 ms /   335 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    35 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     508.55 ms /   129 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 292 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'two_14mm_bolts', 'predicate': 'hold', 'object': 'strap_to_gas_tank', 'start': 35.679, 'end': 41.803}, {'subject': 'gas tank', 'predicate': 'fixing', 'object': 'two straps', 'start': 29.356, 'end': 35.579}, {'subject': 'fuel filler neck', 'predicate': 'held in by', 'object': '14mm bolt', 'start': 43.564, 'end': 148.419}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'big steel', 'start': 29.356, 'end': 35.579}, {'subject': 'gas tank', 'predicate': 'location', 'object': 'underneath the rear seat of the vehicle', 'start': 29.356, 'end': 35.579}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   292 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    36 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     571.17 ms /   328 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    32 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     439.91 ms /   127 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 306 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'fuel filler neck', 'predicate': 'held in by', 'object': '14mm bolt', 'start': 43.564, 'end': 148.419}, {'subject': 'gas filler neck', 'predicate': 'is part of', 'object': 'fuel system', 'start': 367.711, 'end': 378.521}, {'subject': 'fuel filler neck', 'predicate': 'contains', 'object': 'a one-way check valve', 'start': 325.575, 'end': 365.911}, {'subject': 'two_14mm_bolts', 'predicate': 'hold', 'object': 'strap_to_gas_tank', 'start': 35.679, 'end': 41.803}, {'subject': 'Filler neck', 'predicate': 'contains', 'object': 'hollow tube', 'start': 390.148, 'end': 420.44}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     847.36 ms /   371 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     231.95 ms /   110 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 283 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'tank side', 'predicate': 'disconnect', 'object': 'two ventilation hoses', 'start': 180.282, 'end': 183.305}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'vacuum_hoses', 'predicate': 'have_been_disconnected', 'object': 'from this side of the EVAP canister', 'start': 15.2, 'end': 28.755}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'Fuel tank vapor pressure sensor', 'predicate': 'has', 'object': '3 terminals', 'start': 390.148, 'end': 420.44}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   283 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    26 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     527.38 ms /   309 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    31 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     429.30 ms /   110 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 287 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'check valve', 'predicate': 'needs removal', 'object': 'six pieces of rust', 'start': 196.161, 'end': 222.294}, {'subject': 'pulling out', 'predicate': 'action', 'object': 'check valves', 'start': 489.851, 'end': 494.159}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'check valve', 'predicate': 'is present in', 'object': 'here', 'start': 378.841, 'end': 385.745}, {'subject': 'Ball check valve', 'predicate': 'is part of', 'object': 'the check valve', 'start': 496.587, 'end': 534.03}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   174 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1987.93 ms /   461 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   241 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2632.70 ms /   333 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 311 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'fill check valve', 'predicate': 'opens valve', 'object': 'inside canister', 'start': 224.015, 'end': 264.245}, {'subject': 'fill check valve', 'predicate': 'allows', 'object': 'excess vapor to pass into the canister', 'start': 148.639, 'end': 180.042}, {'subject': 'fill check valve', 'predicate': 'allows escape of vapor', 'object': 'when gas tank is rapidly filling up with vapor', 'start': 224.015, 'end': 264.245}, {'subject': 'check valve', 'predicate': 'allows fluid entry', 'object': 'into the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'check valve', 'predicate': 'is present in', 'object': 'here', 'start': 378.841, 'end': 385.745}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   311 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     9 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     426.32 ms /   320 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     6 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     259.01 ms /   106 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 306 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'evap float', 'predicate': 'prevents', 'object': 'liquid gasoline', 'start': 196.161, 'end': 222.294}, {'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'float', 'predicate': 'prevents', 'object': 'liquid from going inside your charcoal canister', 'start': 266.787, 'end': 280.936}, {'subject': 'fuel tank', 'predicate': 'has', 'object': 'evap canister', 'start': 43.564, 'end': 148.419}, {'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 'absorb gasoline vapors', 'start': 469.037, 'end': 482.005}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    20 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     581.23 ms /   326 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     278.55 ms /   115 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 287 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'construction material', 'object': 'sheet metal', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'number of parts', 'object': 'two halves', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'material', 'object': 'big steel', 'start': 29.356, 'end': 35.579}, {'subject': 'gas tank', 'predicate': 'construction method', 'object': 'stamping', 'start': 308.171, 'end': 325.335}, {'subject': 'gas tank', 'predicate': 'joining method', 'object': 'welding', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   287 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    30 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     527.99 ms /   317 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     958.00 ms /   182 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 304 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'maximum fuel level', 'predicate': 'is limited by', 'object': 'the height of the fuel fill check valve', 'start': 325.575, 'end': 365.911}, {'subject': 'fuel pump', 'predicate': 'is located', 'object': 'at the lowest part of the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'check valves', 'predicate': 'function', 'object': 'control flow of gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'gas filler neck', 'predicate': 'is part of', 'object': 'fuel system', 'start': 367.711, 'end': 378.521}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   304 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     376.65 ms /   319 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     772.51 ms /   156 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 294 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'gas tank', 'predicate': 'internal feature', 'object': 'baffles', 'start': 308.171, 'end': 325.335}, {'subject': 'baffle', 'predicate': 'prevents', 'object': 'fuel sloshing', 'start': 325.575, 'end': 365.911}, {'subject': 'baffles', 'predicate': 'location', 'object': 'perimeter of the tank', 'start': 308.171, 'end': 325.335}, {'subject': 'remaining tank area', 'predicate': 'is used for', 'object': 'fuel vapor accumulation', 'start': 325.575, 'end': 365.911}, {'subject': 'gas tank', 'predicate': 'construction method', 'object': 'stamping', 'start': 308.171, 'end': 325.335}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   294 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     348.95 ms /   305 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    11 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.40 ms /   109 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 320 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'check valve', 'predicate': 'allows fluid entry', 'object': 'into the tank', 'start': 325.575, 'end': 365.911}, {'subject': 'diaphragm', 'predicate': 'can move', 'object': 'up and down', 'start': 281.796, 'end': 303.949}, {'subject': 'Check valve', 'predicate': 'acts as', 'object': 'a valve that releases air when a certain pressure is reached', 'start': 496.587, 'end': 534.03}, {'subject': 'fuel check valve', 'predicate': 'contains', 'object': 'lid', 'start': 281.796, 'end': 303.949}, {'subject': 'maximum fuel level', 'predicate': 'is limited by', 'object': 'the height of the fuel fill check valve', 'start': 325.575, 'end': 365.911}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   320 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     896.46 ms /   376 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   110 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    25 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     466.62 ms /   135 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 322 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'EVAP charcoal canister', 'predicate': 'action', 'object': 'absorb gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'EVAP charcoal canister', 'predicate': 'responsibility', 'object': 'hold gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'check valves', 'predicate': 'function', 'object': 'control flow of gasoline vapors', 'start': 469.037, 'end': 482.005}, {'subject': 'vacuum switching valve', 'predicate': 'allows', 'object': 'air flow from gas tank to charcoal canister', 'start': 435.808, 'end': 468.877}, {'subject': 'vacuum switching valve', 'predicate': 'functions as', 'object': 'redirecting air flow between gas tank and charcoal canister', 'start': 435.808, 'end': 468.877}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   322 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    23 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     453.61 ms /   345 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     286.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1065.37 ms /   197 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results saved to result_eval_phi-4-14b-Q4_K_M.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import threading\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from llama_cpp import Llama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from json import JSONDecodeError\n",
    "\n",
    "# -------------------------------\n",
    "# The Chatbot and LlamaSingleton\n",
    "# -------------------------------\n",
    "\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/Qwen2.5-7B-Instruct.Q4_K_M.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=2048)\n",
    "            return cls._instance\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, \n",
    "                 messages_file='messages.json', \n",
    "                 knowledge_file='knowledge.json', \n",
    "                 faiss_index_file='faiss_index.pkl',\n",
    "                 model_name='all-MiniLM-L6-v2',\n",
    "                 llm_model_path=\"models/Qwen2.5-7B-Instruct.Q4_K_M.gguf\"):\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "        # Instantiate LlamaSingleton with the provided model path\n",
    "        self.llm = LlamaSingleton(model_path=llm_model_path).llm\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.knowledge_data = []\n",
    "        self.initialize_files()\n",
    "        self.load_faiss_index()\n",
    "\n",
    "    def initialize_files(self):\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def load_faiss_index(self):\n",
    "        if os.path.exists(self.faiss_index_file):\n",
    "            with open(self.faiss_index_file, 'rb') as f:\n",
    "                self.index, self.knowledge_data = pickle.load(f)\n",
    "        else:\n",
    "            self.index = None\n",
    "            self.knowledge_data = []\n",
    "\n",
    "    def search_knowledge(self, query, top_k=5):\n",
    "        if self.index is None or len(self.knowledge_data) == 0:\n",
    "            return []\n",
    "        query_embedding = self.model.encode([query])\n",
    "        distances, indices = self.index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            results.append(self.knowledge_data[idx])\n",
    "        return results\n",
    "\n",
    "    def generate_response_with_kb(self, user_message):\n",
    "        knowledge_matches = self.search_knowledge(user_message, top_k=5)\n",
    "        current_time = datetime.utcnow().isoformat()\n",
    "        system_message = f\"Current date and time: {current_time}\\n\"\n",
    "        if knowledge_matches:\n",
    "            system_message += \"Choose one answer based on retrieved knowledge, if it relates to the question:\\n\"\n",
    "            for t in knowledge_matches:\n",
    "                system_message += f\"- {t['subject']} {t['predicate']} {t['object']} (Videotimestamps: start: {t['start']}, end: {t['end']})\\n\"\n",
    "        else:\n",
    "            system_message += \"\\n\"\n",
    "        enriched_history = [{\"role\": \"system\", \"content\": f\"You are a helpful assistent; {system_message}\"}]\n",
    "        enriched_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        print(knowledge_matches)\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=enriched_history,\n",
    "            temperature=0.5,\n",
    "        )['choices'][0]['message']['content']\n",
    "        return response\n",
    "\n",
    "    def generate_response_without_kb(self, user_message):\n",
    "        current_time = datetime.utcnow().isoformat()\n",
    "        system_message = f\"Current date and time: {current_time}\\n\"\n",
    "        enriched_history = [{\"role\": \"system\", \"content\": f\"You are a helpful assistent. Choose one answer; {system_message}\"}]\n",
    "        enriched_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=enriched_history,\n",
    "            temperature=0.5,\n",
    "        )['choices'][0]['message']['content']\n",
    "        return response\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluation: Looping Over Questions and Generating Responses\n",
    "# -------------------------------\n",
    "\n",
    "def run_evaluation(llm_model_path, questions_path):\n",
    "    # Load the questions from the provided JSON file\n",
    "    with open(questions_path, 'r', encoding='utf-8') as f:\n",
    "        questions = json.load(f)\n",
    "\n",
    "    # Instantiate Chatbot using the provided model path\n",
    "    chatbot = Chatbot(llm_model_path=llm_model_path)\n",
    "\n",
    "    # Loop over all questions\n",
    "    for question in questions:\n",
    "        # Compose a prompt that includes the question text and options.\n",
    "        prompt = f\"Question: {question['question']}\\nOptions:\\n\"\n",
    "        for idx, opt in enumerate(question['options']):\n",
    "            prompt += f\"{idx+1}. {opt}\\n\"\n",
    "\n",
    "        # Generate responses using the two methods.\n",
    "        response_with_kb = chatbot.generate_response_with_kb(prompt)\n",
    "        response_without_kb = chatbot.generate_response_without_kb(prompt)\n",
    "\n",
    "        # Add the responses to the question JSON object.\n",
    "        question[\"llm_answer_with_kb\"] = response_with_kb\n",
    "        question[\"llm_answer_without_kb\"] = response_without_kb\n",
    "\n",
    "    # Derive the LLM name from the model path.\n",
    "    llm_name = os.path.splitext(os.path.basename(llm_model_path))[0]\n",
    "    result_filename = f\"result_eval_{llm_name}.json\"\n",
    "\n",
    "    # Save the updated questions JSON to the new file.\n",
    "    with open(result_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(questions, f, indent=4)\n",
    "\n",
    "    print(f\"Evaluation complete. Results saved to {result_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    # Check command-line arguments\n",
    "    model_paths = [\"models/Qwen2.5-0.5B-Instruct-f16.gguf\",\n",
    "                   \"models/Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "                   \"models/gemma-2-2b-it.F16.gguf\",\n",
    "                   \"models/Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "                   \"models/Qwen2.5-7B-Instruct-Q4_K_M.gguf\",\n",
    "                   \"models/Llama-3.2-11B-Vision-Instruct.Q4_K_M.gguf\",\n",
    "                   \"models/gemma-2-9b-it-Q4_K_M.gguf\",\n",
    "                   \"models/phi-4-14b-Q4_K_M.gguf\",\n",
    "                   ]\n",
    "    \n",
    "    for model_path in model_paths:\n",
    "        questions_path = \"questions.json\"\n",
    "        run_evaluation(model_path, questions_path)\n",
    "    \n",
    "    # model_path = \"models/Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n",
    "    # questions_path = \"questions.json\"\n",
    "    # run_evaluation(model_path, questions_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERIG: STAGE KNOWLEDGE EXTRACTION MET EXTRA VERIFICATION STEP (NOT IN FINAL BUILD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M3 Max) - 26602 MiB free\n",
      "llama_model_loader: loaded meta data with 27 key-value pairs and 339 tensors from models/DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Deepseek Ai\n",
      "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Qwen\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 1.5B\n",
      "llama_model_loader: - kv   6:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv   7:                       qwen2.context_length u32              = 131072\n",
      "llama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 1536\n",
      "llama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 8960\n",
      "llama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 12\n",
      "llama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = deepseek-r1-qwen\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 151646\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151654\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type q8_0:  198 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 1.76 GiB (8.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151646 '<｜begin▁of▁sentence｜>' is not marked as EOG\n",
      "load: control token: 151644 '<｜User｜>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: control token: 151647 '<|EOT|>' is not marked as EOG\n",
      "load: control token: 151643 '<｜end▁of▁sentence｜>' is not marked as EOG\n",
      "load: control token: 151645 '<｜Assistant｜>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 1536\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 12\n",
      "print_info: n_head_kv        = 2\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 6\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 8960\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 1.5B\n",
      "print_info: model params     = 1.78 B\n",
      "print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151646 '<｜begin▁of▁sentence｜>'\n",
      "print_info: EOS token        = 151643 '<｜end▁of▁sentence｜>'\n",
      "print_info: EOT token        = 151643 '<｜end▁of▁sentence｜>'\n",
      "print_info: PAD token        = 151654 '<|vision_pad|>'\n",
      "print_info: LF token         = 148848 'ÄĬ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<｜end▁of▁sentence｜>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 338 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/29 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  1801.09 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 28991.03 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x16c64fc00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x17fba0c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x17fba11a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x16c533360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x17facf2b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x106c14150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x17facf8e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x106a4ee00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x106a4f430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x17fba1460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x17fba1d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x16ff7e000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x16c650610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x16c650020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x16c651380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x16ff7ee50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x16ff7f110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x17face440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x17faced50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x16c651af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x16ff7f860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x106c14a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x17fba2180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x16ff804e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x106c15c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x17fba2dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x17fba32c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x16c651e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13df82bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x13df82e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x17fba3920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x17facfcd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x17fad0630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106a4e1a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106a50770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13df83220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106a50080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13df847f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x17fad1020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13df84ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13df84e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106c16590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106c17190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106c16d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x16ff808a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x106a51690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x16ff81370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13df85400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x17fad20c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13df85910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x106a51040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x17fba44b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x106a51c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x17fba3e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x16c652590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x17fad1cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x17fba4dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x17fba5540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13df865d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x16c533620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x16c533ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13df85f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x16c534100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13df870e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x17fad2e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x17fad3480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x106c179a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13df86c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x106c17fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x17fad3ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x16ff81790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x106c18cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x16ff81a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x16ff82b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13df873a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13df87cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x17fad24f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x17fba5090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x16c652bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x106a52220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x16c653030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x16ff831e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x106c18fb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x17fad4d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x17fad4370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x16ff83990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x17fad5050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x17fad5a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13df883c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x106c19500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x17fba5a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x16ff84010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x16ff846c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x16c5343c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13dd87c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x106c19bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106c1a0f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x17fad5d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x3de504780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106c1ae60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x16c653c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106c1b120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x16c534820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x16c653730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x3de504f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x3de505700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x16ff84980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x3de504250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13dd87f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x3de506920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x3de506d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x16c6532f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x16c534e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x16c654340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x16c655580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x16ff84c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13df892d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x3de5071c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x16c535200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13df88b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x17fba5ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x16ff85800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x16ff85140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x16ff85df0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x17fba6160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x106a531b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x106a53940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x106a54000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x106c1b5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x106c1bcb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x17fba73c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x16ff868f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x106c1c340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x106c1c9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x16ff87150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x17fba7960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x16c535970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13df89590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x16ff87410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x106c1d070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x17fba80d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x17fba8670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x17fba8930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x16ff876d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106c1dab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106c1e080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13dd88370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106c1e650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x17fad6160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x16ff87f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x16ff884b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x16ff88770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13df89850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106c1ed90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x16c655f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13df89b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x17fad7130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x16c535c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x17fba9030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x106a54680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x106c1e910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x106c1f050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x17fba96a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x17fad68a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x16c536260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x106a54cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x16ff88f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x16c536520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x106a55580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x16c536d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x16c655ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x16c656700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106c1fce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106c203b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x17fad78c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x16c536970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x17fba9cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x106c210e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x106c21790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13df89dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x106c21a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x17fad6bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x106c21ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x106c223e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x106c23150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x17fbaa270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x106c23510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x17fad9320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x16ff89970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x16c5370b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x3de508710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x16c656200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x106a55cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13df8a090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x106c237d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x16ff89d00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x13df8b930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x16c537490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x16ff8a970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x16ff8ac30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x17fbaa530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106a567d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x106c247c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x3de507780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x16c537ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x16c538230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x16ff8aef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13dd88930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x17fad95e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x16c656ed0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x3de404290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13dd89100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x16c657280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x3de509140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13df8c100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13dd89d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x16c538aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x16c538e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13dd8a610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x16c539150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x3de509470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x16c539ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x3de5097a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x3de50a380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x16c53a780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13dd8aba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x3de50a6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x16c53aef0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x16c657820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x17fbaa7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x16ff8b1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x17fbab130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x106c24a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x106c250c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x17fbab6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x17fbabc70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x106c254d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x106c25d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x106c25880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x106c27140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x3de509d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x106c27400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x106c276c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x16c53b560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x106c27fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x106a56d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x106c282a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x106c28ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x106a57b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x106c28de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x16ff8b970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x106c290a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x16ff8c3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x106c29360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106c29620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x16ff8cb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x16ff8d110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x16ff8d6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106c29920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x16ff8dcc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x16ff8e280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106c29f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x16ff8e870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x106c2a550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x106a57e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x106a58670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x16ff8eb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x16c657e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x16c658610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x17fbac210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x16c658ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13df8c980 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init:        CPU KV buffer size =    56.00 MiB\n",
      "llama_init_from_model: KV self size  =   56.00 MiB, K (f16):   28.00 MiB, V (f16):   28.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   299.75 MiB\n",
      "llama_init_from_model: graph nodes  = 986\n",
      "llama_init_from_model: graph splits = 450 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\\\n' + '```json' + '\\\\n' + tool['function']['arguments'] + '\\\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}\", 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '151654', 'tokenizer.ggml.eos_token_id': '151643', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '151646', 'tokenizer.ggml.pre': 'deepseek-r1-qwen', 'tokenizer.ggml.model': 'gpt2', 'qwen2.rope.freq_base': '10000.000000', 'general.basename': 'DeepSeek-R1-Distill-Qwen', 'qwen2.attention.head_count_kv': '2', 'general.size_label': '1.5B', 'qwen2.embedding_length': '1536', 'tokenizer.ggml.add_eos_token': 'false', 'qwen2.context_length': '131072', 'qwen2.attention.head_count': '12', 'general.architecture': 'qwen2', 'general.name': 'DeepSeek R1 Distill Qwen 1.5B', 'qwen2.feed_forward_length': '8960', 'qwen2.block_count': '28', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.organization': 'Deepseek Ai', 'general.file_type': '7'}\n",
      "Available chat formats from metadata: chat_template.default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunk 1 (Token count: 510, Sentences: 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1838.37 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   651 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /  1396 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   26720.63 ms /  2047 tokens\n",
      "Llama.generate: 12 prefix-match hit, remaining 254 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1838.37 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   254 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /  1781 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   32068.14 ms /  2035 tokens\n",
      "Llama.generate: 12 prefix-match hit, remaining 628 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and refined knowledge from a chunk: {'knowledge': []}\n",
      "\n",
      "Processing chunk 2 (Token count: 502, Sentences: 45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1838.37 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   628 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /  1407 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   26439.43 ms /  2035 tokens\n",
      "Llama.generate: 12 prefix-match hit, remaining 254 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1838.37 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   254 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /  1781 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   31369.40 ms /  2035 tokens\n",
      "Llama.generate: 12 prefix-match hit, remaining 646 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and refined knowledge from a chunk: {'knowledge': []}\n",
      "\n",
      "Processing chunk 3 (Token count: 499, Sentences: 44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    1838.37 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   646 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /  1389 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   25853.42 ms /  2035 tokens\n",
      "Llama.generate: 12 prefix-match hit, remaining 254 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    1838.37 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   254 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /  1781 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   33184.86 ms /  2035 tokens\n",
      "Llama.generate: 12 prefix-match hit, remaining 647 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and refined knowledge from a chunk: {'knowledge': []}\n",
      "\n",
      "Processing chunk 4 (Token count: 496, Sentences: 42)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 221\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknowledge.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for the extracted valuable knowledge.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 213\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Token count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;241m.\u001b[39mtoken_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Sentences: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunk\u001b[38;5;241m.\u001b[39msentences)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 213\u001b[0m     extracted_knowledge \u001b[38;5;241m=\u001b[39m \u001b[43mchatbot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_valuable_knowledge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extracted_knowledge:\n\u001b[1;32m    215\u001b[0m         chatbot\u001b[38;5;241m.\u001b[39msave_knowledge(extracted_knowledge)\n",
      "Cell \u001b[0;32mIn[2], line 80\u001b[0m, in \u001b[0;36mChatbot.extract_valuable_knowledge\u001b[0;34m(self, text_chunk)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Stage 1: Extraction of entities and context\u001b[39;00m\n\u001b[1;32m     63\u001b[0m stage1_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are an advanced information extractor. Given the text below, extract all relevant entities, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontextual metadata (e.g., document context, sentiment, source reliability), and any preliminary relationships \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_chunk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m )\n\u001b[0;32m---> 80\u001b[0m stage1_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a precise and methodical information extractor.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage1_prompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     stage1_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(stage1_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama.py:2012\u001b[0m, in \u001b[0;36mLlama.create_chat_completion\u001b[0;34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs)\u001b[0m\n\u001b[1;32m   1974\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate a chat completion from a list of messages.\u001b[39;00m\n\u001b[1;32m   1975\u001b[0m \n\u001b[1;32m   1976\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;124;03m    Generated chat completion or a stream of chat completion chunks.\u001b[39;00m\n\u001b[1;32m   2006\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2007\u001b[0m handler \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2008\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_handler\n\u001b[1;32m   2009\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_handlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   2010\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m llama_chat_format\u001b[38;5;241m.\u001b[39mget_chat_completion_handler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[1;32m   2011\u001b[0m )\n\u001b[0;32m-> 2012\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllama\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2039\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2042\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama_chat_format.py:663\u001b[0m, in \u001b[0;36mchat_formatter_to_chat_completion_handler.<locals>.chat_completion_handler\u001b[0;34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e), file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m    659\u001b[0m         grammar \u001b[38;5;241m=\u001b[39m llama_grammar\u001b[38;5;241m.\u001b[39mLlamaGrammar\u001b[38;5;241m.\u001b[39mfrom_string(\n\u001b[1;32m    660\u001b[0m             llama_grammar\u001b[38;5;241m.\u001b[39mJSON_GBNF, verbose\u001b[38;5;241m=\u001b[39mllama\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m    661\u001b[0m         )\n\u001b[0;32m--> 663\u001b[0m completion_or_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     tool_name \u001b[38;5;241m=\u001b[39m tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama.py:1846\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1846\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama.py:1321\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1319\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1320\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1321\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_vocab_is_eog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompletion_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama.py:912\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 912\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    914\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    915\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    916\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    930\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    931\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/llama.py:646\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    642\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    644\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    645\u001b[0m )\n\u001b[0;32m--> 646\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/llama_cpp/_internals.py:300\u001b[0m, in \u001b[0;36mLlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[0;32m--> 300\u001b[0m     return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import os\n",
    "# import threading\n",
    "# from datetime import datetime\n",
    "# from json import JSONDecodeError\n",
    "\n",
    "# from chonkie import SentenceChunker\n",
    "# from llama_cpp import Llama\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # -------------------------------\n",
    "# # The Chatbot and LlamaSingleton\n",
    "# # -------------------------------\n",
    "\n",
    "# class LlamaSingleton:\n",
    "#     _instance = None\n",
    "#     _lock = threading.Lock()\n",
    "\n",
    "#     def __new__(cls, model_path=\"models/DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf\", chat_format=\"chatml\"):\n",
    "#         with cls._lock:\n",
    "#             if cls._instance is None:\n",
    "#                 cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "#                 cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=2048)\n",
    "#             return cls._instance\n",
    "\n",
    "# class Chatbot:\n",
    "#     def __init__(self, \n",
    "#                  messages_file='messages.json', \n",
    "#                  knowledge_file='knowledge.json', \n",
    "#                  model_name='all-MiniLM-L6-v2'):\n",
    "#         self.messages_file = messages_file\n",
    "#         self.knowledge_file = knowledge_file\n",
    "#         self.llm = LlamaSingleton().llm\n",
    "#         self.model = SentenceTransformer(model_name)\n",
    "#         self.knowledge_data = []\n",
    "#         self.initialize_files()\n",
    "#         self.load_knowledge()\n",
    "\n",
    "#     def initialize_files(self):\n",
    "#         for file in [self.messages_file, self.knowledge_file]:\n",
    "#             if not os.path.exists(file):\n",
    "#                 with open(file, 'w') as f:\n",
    "#                     json.dump([], f)\n",
    "\n",
    "#     def load_json_data(self, file_path):\n",
    "#         with open(file_path, 'r') as f:\n",
    "#             return json.load(f)\n",
    "\n",
    "#     def save_json_data(self, file_path, data):\n",
    "#         with open(file_path, 'w') as f:\n",
    "#             json.dump(data, f, indent=4)\n",
    "\n",
    "#     def load_knowledge(self):\n",
    "#         self.knowledge_data = self.load_json_data(self.knowledge_file)\n",
    "\n",
    "#     def extract_valuable_knowledge(self, text_chunk):\n",
    "#         \"\"\"\n",
    "#         Use a two-stage chain-of-thought prompting strategy:\n",
    "#           1. Extract entities, contextual metadata, and any compound/n-ary relations from the text.\n",
    "#           2. Reason about the relationships among the extracted entities, self-critique the result, and refine the output.\n",
    "#         \"\"\"\n",
    "#         # Stage 1: Extraction of entities and context\n",
    "#         stage1_prompt = (\n",
    "#             \"You are an advanced information extractor. Given the text below, extract all relevant entities, \"\n",
    "#             \"contextual metadata (e.g., document context, sentiment, source reliability), and any preliminary relationships \"\n",
    "#             \"or compound relations that may exist. Return ONLY JSON following this schema:\\n\\n\"\n",
    "#             \"{\\n\"\n",
    "#             '  \"entities\": [{\"name\": \"EntityName\", \"type\": \"EntityType\"}],\\n'\n",
    "#             '  \"prelim_relations\": [\\n'\n",
    "#             \"      { \\\"entities\\\": [\\\"EntityName1\\\", \\\"EntityName2\\\"], \\\"relation_hint\\\": \\\"...\\\" }\\n\"\n",
    "#             \"  ],\\n\"\n",
    "#             '  \"context\": \"Brief context extracted from the document\",\\n'\n",
    "#             '  \"sentiment\": \"positive/negative/neutral\",\\n'\n",
    "#             '  \"source_reliability\": \"high/medium/low\"\\n'\n",
    "#             \"}\\n\\n\"\n",
    "#             \"If no relevant information is found, return empty values (e.g., empty arrays or empty strings).\\n\\n\"\n",
    "#             \"Text:\\n\"\n",
    "#             f\"{text_chunk}\"\n",
    "#         )\n",
    "#         stage1_response = self.llm.create_chat_completion(\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": \"You are a precise and methodical information extractor.\"},\n",
    "#                 {\"role\": \"user\", \"content\": stage1_prompt},\n",
    "#             ],\n",
    "#             temperature=0.5,\n",
    "#         )\n",
    "\n",
    "#         try:\n",
    "#             stage1_data = json.loads(stage1_response['choices'][0]['message']['content'])\n",
    "#         except (JSONDecodeError, KeyError):\n",
    "#             stage1_data = {\n",
    "#                 \"entities\": [],\n",
    "#                 \"prelim_relations\": [],\n",
    "#                 \"context\": \"\",\n",
    "#                 \"sentiment\": \"neutral\",\n",
    "#                 \"source_reliability\": \"medium\"\n",
    "#             }\n",
    "\n",
    "#         # Stage 2: Reasoning about relationships\n",
    "#         stage2_prompt = (\n",
    "#             \"Based on the following extracted information:\\n\\n\"\n",
    "#             f\"Entities: {json.dumps(stage1_data.get('entities', []))}\\n\"\n",
    "#             f\"Preliminary Relations: {json.dumps(stage1_data.get('prelim_relations', []))}\\n\"\n",
    "#             f\"Context: {stage1_data.get('context', '')}\\n\"\n",
    "#             f\"Sentiment: {stage1_data.get('sentiment', 'neutral')}\\n\"\n",
    "#             f\"Source Reliability: {stage1_data.get('source_reliability', 'medium')}\\n\\n\"\n",
    "#             \"Now, reason about the relationships among these entities. Identify any compound or n-ary relations, \"\n",
    "#             \"and provide refined relations with a confidence score (between 0 and 1). Use a chain-of-thought process \"\n",
    "#             \"to self-critique and refine your answer. Return ONLY JSON with the following schema:\\n\\n\"\n",
    "#             \"{\\n\"\n",
    "#             '  \"knowledge\": [\\n'\n",
    "#             \"      {\\n\"\n",
    "#             '          \"entities\": [{\"name\": \"EntityName\", \"type\": \"EntityType\"}],\\n'\n",
    "#             '          \"relations\": [\\n'\n",
    "#             \"              { \\\"entities\\\": [\\\"EntityName1\\\", \\\"EntityName2\\\", ...], \\\"predicate\\\": \\\"...\\\", \\\"confidence\\\": 0.95 }\\n\"\n",
    "#             \"          ],\\n\"\n",
    "#             '          \"context\": \"The document context\",\\n'\n",
    "#             '          \"sentiment\": \"positive/negative/neutral\",\\n'\n",
    "#             '          \"source_reliability\": \"high/medium/low\",\\n'\n",
    "#             '          \"timestamp\": \"ISO8601 formatted timestamp\"\\n'\n",
    "#             \"      }\\n\"\n",
    "#             \"  ]\\n\"\n",
    "#             \"}\\n\\n\"\n",
    "#             \"If no refined relations can be determined, return:\\n\"\n",
    "#             '{\"knowledge\": []}'\n",
    "#         )\n",
    "\n",
    "#         stage2_response = self.llm.create_chat_completion(\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": \"You are a reflective and expert knowledge aggregator.\"},\n",
    "#                 {\"role\": \"user\", \"content\": stage2_prompt},\n",
    "#             ],\n",
    "#             temperature=0.5,\n",
    "#         )\n",
    "\n",
    "#         try:\n",
    "#             knowledge_data = json.loads(stage2_response['choices'][0]['message']['content'])\n",
    "#             if \"knowledge\" not in knowledge_data:\n",
    "#                 knowledge_data[\"knowledge\"] = []\n",
    "#         except (JSONDecodeError, KeyError):\n",
    "#             knowledge_data = {\"knowledge\": []}\n",
    "\n",
    "#         print(\"Extracted and refined knowledge from a chunk:\", knowledge_data)\n",
    "#         return knowledge_data[\"knowledge\"]\n",
    "\n",
    "#     def save_knowledge(self, new_knowledge_items):\n",
    "#         \"\"\"\n",
    "#         Merge new knowledge items with existing ones. Use simple ontology-based reasoning to merge\n",
    "#         items that refer to the same entities or concepts.\n",
    "#         \"\"\"\n",
    "#         if not new_knowledge_items:\n",
    "#             return\n",
    "\n",
    "#         existing_knowledge = self.load_json_data(self.knowledge_file)\n",
    "#         merged_knowledge = existing_knowledge.copy()\n",
    "\n",
    "#         # A simple merging strategy: if two knowledge items share the same context and similar entities,\n",
    "#         # merge their relations. (In practice, you might use a more sophisticated ontology matching algorithm.)\n",
    "#         for new_item in new_knowledge_items:\n",
    "#             new_item['timestamp'] = datetime.utcnow().isoformat()\n",
    "#             merged = False\n",
    "#             for existing_item in merged_knowledge:\n",
    "#                 if existing_item.get(\"context\") == new_item.get(\"context\"):\n",
    "#                     # Compare entity sets (by names) for overlap\n",
    "#                     existing_entities = {e[\"name\"] for e in existing_item.get(\"entities\", [])}\n",
    "#                     new_entities = {e[\"name\"] for e in new_item.get(\"entities\", [])}\n",
    "#                     if existing_entities & new_entities:\n",
    "#                         # Merge relations (avoiding duplicates based on predicate and involved entities)\n",
    "#                         existing_relations = existing_item.get(\"relations\", [])\n",
    "#                         for rel in new_item.get(\"relations\", []):\n",
    "#                             if rel not in existing_relations:\n",
    "#                                 existing_relations.append(rel)\n",
    "#                         existing_item[\"relations\"] = existing_relations\n",
    "#                         merged = True\n",
    "#                         break\n",
    "#             if not merged:\n",
    "#                 merged_knowledge.append(new_item)\n",
    "\n",
    "#         self.save_json_data(self.knowledge_file, merged_knowledge)\n",
    "#         print(\"Knowledge saved. Total items:\", len(merged_knowledge))\n",
    "\n",
    "# # -------------------------------\n",
    "# # Main Processing: Chunk and Extract Knowledge\n",
    "# # -------------------------------\n",
    "\n",
    "# def main():\n",
    "#     # ----------------------------\n",
    "#     # Part 1: Chunk the input file\n",
    "#     # ----------------------------\n",
    "#     # Initialize the SentenceChunker with desired parameters\n",
    "#     chunker = SentenceChunker(\n",
    "#         chunk_size=512,\n",
    "#         chunk_overlap=128,\n",
    "#         min_sentences_per_chunk=40  # adjust as needed for text length\n",
    "#     )\n",
    "\n",
    "#     # Read the contents of input.txt\n",
    "#     with open('input.txt', 'r', encoding='utf-8') as file:\n",
    "#         text = file.read()\n",
    "\n",
    "#     # Get the list of chunks from the input text\n",
    "#     chunks = chunker.chunk(text)\n",
    "#     print(f\"Total chunks created: {len(chunks)}\")\n",
    "\n",
    "#     # ----------------------------\n",
    "#     # Part 2: Extract Knowledge from Each Chunk\n",
    "#     # ----------------------------\n",
    "#     chatbot = Chatbot()\n",
    "\n",
    "#     # Process each chunk\n",
    "#     for i, chunk in enumerate(chunks, start=1):\n",
    "#         print(f\"\\nProcessing chunk {i} (Token count: {chunk.token_count}, Sentences: {len(chunk.sentences)})\")\n",
    "#         extracted_knowledge = chatbot.extract_valuable_knowledge(chunk.text)\n",
    "#         if extracted_knowledge:\n",
    "#             chatbot.save_knowledge(extracted_knowledge)\n",
    "\n",
    "#     print(\"\\nKnowledge extraction complete.\")\n",
    "#     print(\"Please check 'knowledge.json' for the extracted valuable knowledge.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
