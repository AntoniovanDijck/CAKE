{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antonio van Dijck\n",
    "\n",
    "studentnumber: 12717673\n",
    "\n",
    "Email: antonio.van.dijck@student.uva.nl\n",
    "\n",
    "The Context-Aware Knowledge Extraction Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports \n",
    "The imports are also provided in the requirements.txt file. \n",
    "\n",
    "Some of the imports used in this notebook are mac specific, and may be altered to support other operating systems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y llama-cpp\n",
    "!pip install \"chonkie[semantic]\"\n",
    "!pip install faiss-cpu\n",
    "!pip install sentence_transformers\n",
    "!pip install transformers\n",
    "!pip install llama-cpp-python tqdm nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MACOS ONLY, SEE LLAMA.CPP GITHUB FOR OTHER PLATFORMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DGGML_METAL=on\" pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DGGML_METAL=on\" FORCE_CMAKE=1 python3 -m pip install \"git+https://github.com/abetlen/llama-cpp-python.git@refs/pull/1901/head\" --force-reinstall --upgrade --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"bartowski/Qwen2.5-7B-Instruct-GGUF\",\n",
    "    filename=\"*Q4_K_M.gguf\",\n",
    "    verbose=False,\n",
    "    local_dir=\"models\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Extraction\n",
    "The knowledge extraction is done using the Qwen 2.5 model. The model is downloaded from the hugginfacehub.\n",
    "\n",
    "The model extracts knowledge from each semantically chunked sentence(s) and stores it in a dictionary.\n",
    "\n",
    "The resulting dictionary is then used to generate vector representations of the knowledge extracted from the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import threading\n",
    "from llama_cpp import Llama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from json import JSONDecodeError\n",
    "\n",
    "# llama singleton to ensure one model is used for ram usage efficiency\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/Qwen2.5-7B-Instruct-Q4_K_M.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=2048)\n",
    "            return cls._instance\n",
    "\n",
    "# chatbot class to extract knowledge from chunks\n",
    "class Chatbot:\n",
    "    def __init__(self, \n",
    "                 messages_file='messages.json', \n",
    "                 knowledge_file='knowledge.json', \n",
    "                 faiss_index_file='faiss_index.pkl',\n",
    "                 model_name='all-MiniLM-L6-v2'):\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "        self.llm = LlamaSingleton().llm\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.knowledge_data = []\n",
    "        self.initialize_files()\n",
    "        self.load_faiss_index()\n",
    "\n",
    "    def initialize_files(self):\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def save_json_data(self, file_path, data):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "    # this function extracts the knowledge from the chunk text\n",
    "    def extract_valuable_knowledge(self, message):\n",
    "        \"\"\"\n",
    "        Sends the chunk text to the model and asks it to return JSON with\n",
    "        subject/predicate/object. No timestamps are generated by the model.\n",
    "        \"\"\"\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are a knowledge extractor. Try to extract any knowledge.\\n\"\n",
    "                        \"Return ONLY JSON with the following schema:\\n\"\n",
    "                        \"{\\n\"\n",
    "                        \"  \\\"valuable_knowledge\\\": [\\n\"\n",
    "                        \"    {\\n\"\n",
    "                        \"      \\\"subject\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"predicate\\\": \\\"...\\\",\\n\"\n",
    "                        \"      \\\"object\\\": \\\"...\\\"\\n\"\n",
    "                        \"    }\\n\"\n",
    "                        \"  ]\\n\"\n",
    "                        \"}\\n\"\n",
    "                        \"If no knowledge can be extracted, return:\\n\"\n",
    "                        \"{\\\"valuable_knowledge\\\": []}\"\n",
    "                    )\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"valuable_knowledge\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"subject\": {\"type\": \"string\"},\n",
    "                                    \"predicate\": {\"type\": \"string\"},\n",
    "                                    \"object\": {\"type\": \"string\"}\n",
    "                                },\n",
    "                                \"required\": [\"subject\", \"predicate\", \"object\"]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"valuable_knowledge\"],\n",
    "                },\n",
    "            },\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        try:\n",
    "            knowledge_data = json.loads(response['choices'][0]['message']['content'])\n",
    "            print(\"Extracted knowledge from a chunk:\", knowledge_data)\n",
    "            if \"valuable_knowledge\" not in knowledge_data:\n",
    "                knowledge_data[\"valuable_knowledge\"] = []\n",
    "            return knowledge_data[\"valuable_knowledge\"]\n",
    "        except (JSONDecodeError, KeyError):\n",
    "            return []\n",
    "\n",
    "    def save_knowledge(self, triplets):\n",
    "        \"\"\"\n",
    "        Persists triplets to `knowledge.json` and updates FAISS index if new triplets\n",
    "        are found. We do not add any timestamps here.\n",
    "        \"\"\"\n",
    "        if not triplets:\n",
    "            return\n",
    "        knowledge = self.load_json_data(self.knowledge_file)\n",
    "        existing_set = {(t['subject'], t['predicate'], t['object']) for t in knowledge}\n",
    "        new_triplets = []\n",
    "        for triplet in triplets:\n",
    "            key = (triplet['subject'], triplet['predicate'], triplet['object'])\n",
    "            if key not in existing_set:\n",
    "                knowledge.append(triplet)\n",
    "                new_triplets.append(triplet)\n",
    "                existing_set.add(key)\n",
    "        self.save_json_data(self.knowledge_file, knowledge)\n",
    "        if new_triplets:\n",
    "            self.update_faiss_index(new_triplets)\n",
    "\n",
    "    def update_faiss_index(self, triplets):\n",
    "        texts = [f\"{t['subject']} {t['predicate']} {t['object']}\" for t in triplets]\n",
    "        embeddings = self.model.encode(texts)\n",
    "        if self.index is None:\n",
    "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(np.array(embeddings, dtype=np.float32))\n",
    "        self.knowledge_data.extend(triplets)\n",
    "        self.save_faiss_index()\n",
    "\n",
    "    def save_faiss_index(self):\n",
    "        with open(self.faiss_index_file, 'wb') as f:\n",
    "            pickle.dump((self.index, self.knowledge_data), f)\n",
    "\n",
    "    def load_faiss_index(self):\n",
    "        if os.path.exists(self.faiss_index_file):\n",
    "            with open(self.faiss_index_file, 'rb') as f:\n",
    "                self.index, self.knowledge_data = pickle.load(f)\n",
    "        else:\n",
    "            self.index = None\n",
    "            self.knowledge_data = []\n",
    "\n",
    "def main():\n",
    "    # Load the chunks from output_chunks.json\n",
    "    with open('output_chunks.json', 'r', encoding='utf-8') as file:\n",
    "\n",
    "        data = json.load(file)\n",
    "    chunks = data.get(\"chunks\", [])\n",
    "    print(f\"Total chunks loaded: {len(chunks)}\")\n",
    "\n",
    "    # Extract Knowledge from Each Chunk\n",
    "    chatbot = Chatbot()\n",
    "\n",
    "    for i, chunk in enumerate(chunks, start=1):\n",
    "        text = chunk.get(\"text\", \"\")\n",
    "        start_time = chunk.get(\"start\")\n",
    "        end_time = chunk.get(\"end\")\n",
    "\n",
    "        print(f\"\\nProcessing chunk {i} (Start: {start_time}, End: {end_time})\")\n",
    "\n",
    "        # Extract valuable knowledge from the chunk text\n",
    "        extracted_knowledge = chatbot.extract_valuable_knowledge(text)\n",
    "\n",
    "        if extracted_knowledge:\n",
    "\n",
    "            # Attach the chunk's start/end timestamps of video to each extracted item\n",
    "            for triplet in extracted_knowledge:\n",
    "                triplet['start'] = start_time\n",
    "                triplet['end'] = end_time\n",
    "\n",
    "            # Save the extracted knowledge\n",
    "            chatbot.save_knowledge(extracted_knowledge)\n",
    "\n",
    "    print(\"\\nKnowledge extraction complete.\")\n",
    "    print(\"Please check 'knowledge.json' for the extracted valuable knowledge.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test generated knowledge base with LLM with the integrated knowledge base\n",
    "\n",
    "The knowledge base is tested with a LLM that uses the vector database to answer questions.\n",
    "\n",
    "The extracted knowledge is stored in a vector databse and the LLM uses the vector database to answer questions.\n",
    "\n",
    "The LLM is implemented using the python library `llama_cpp` and `torch`.\n",
    "\n",
    "The LLM has acces to five knowledge items from the vector database, this is specified with a topk parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from llama_cpp import Llama\n",
    "import threading\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from json import JSONDecodeError\n",
    "\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/Qwen2.5-7B-Instruct-Q4_K_M.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=2048)\n",
    "            return cls._instance\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, \n",
    "                 messages_file='messages.json', \n",
    "                 knowledge_file='knowledge.json', \n",
    "                 faiss_index_file='faiss_index.pkl',\n",
    "                 model_name='all-MiniLM-L6-v2'):\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "        self.llm = LlamaSingleton().llm\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.knowledge_data = []\n",
    "        self.initialize_files()\n",
    "        self.load_faiss_index()\n",
    "\n",
    "    def initialize_files(self):\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def save_json_data(self, file_path, data):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "    def save_message(self, role, content):\n",
    "        messages = self.load_json_data(self.messages_file)\n",
    "        message = {\"role\": role, \"content\": content, \"timestamp\": datetime.utcnow().isoformat()}\n",
    "        messages.append(message)\n",
    "        self.save_json_data(self.messages_file, messages)\n",
    "\n",
    "    def save_knowledge(self, triplets):\n",
    "        if not triplets:\n",
    "            return\n",
    "        knowledge = self.load_json_data(self.knowledge_file)\n",
    "        existing_set = {(t['subject'], t['predicate'], t['object']) for t in knowledge}\n",
    "        new_triplets = []\n",
    "        for triplet in triplets:\n",
    "            triplet['timestamp'] = datetime.utcnow().isoformat()\n",
    "            key = (triplet['subject'], triplet['predicate'], triplet['object'])\n",
    "            if key not in existing_set:\n",
    "                knowledge.append(triplet)\n",
    "                new_triplets.append(triplet)\n",
    "                existing_set.add(key)\n",
    "        self.save_json_data(self.knowledge_file, knowledge)\n",
    "        if new_triplets:\n",
    "            self.update_faiss_index(new_triplets)\n",
    "\n",
    "    def update_faiss_index(self, triplets):\n",
    "        texts = [f\"{t['subject']} {t['predicate']} {t['object']}\" for t in triplets]\n",
    "        embeddings = self.model.encode(texts)\n",
    "        if self.index is None:\n",
    "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(np.array(embeddings, dtype=np.float32))\n",
    "        self.knowledge_data.extend(triplets)\n",
    "        self.save_faiss_index()\n",
    "\n",
    "    def save_faiss_index(self):\n",
    "        with open(self.faiss_index_file, 'wb') as f:\n",
    "            pickle.dump((self.index, self.knowledge_data), f)\n",
    "\n",
    "    def load_faiss_index(self):\n",
    "        if os.path.exists(self.faiss_index_file):\n",
    "            with open(self.faiss_index_file, 'rb') as f:\n",
    "                self.index, self.knowledge_data = pickle.load(f)\n",
    "        else:\n",
    "            self.index = None\n",
    "            self.knowledge_data = []\n",
    "\n",
    "    def search_knowledge(self, query, top_k=5):\n",
    "        if self.index is None or len(self.knowledge_data) == 0:\n",
    "            return []\n",
    "        query_embedding = self.model.encode([query])\n",
    "        distances, indices = self.index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            results.append(self.knowledge_data[idx])\n",
    "        return results\n",
    "\n",
    "    # this function generates a response based on the conversation history and user message and the knowledge top k 5 matches\n",
    "    def generate_response(self, conversation_history, user_message):\n",
    "        knowledge_matches = self.search_knowledge(user_message, top_k=5)\n",
    "        current_time = datetime.utcnow().isoformat()\n",
    "        system_message = f\"Current date and time: {current_time}\\n\"\n",
    "        if knowledge_matches:\n",
    "            system_message += \"Answer based on retrieved knowledge:\\n\"\n",
    "            for t in knowledge_matches:\n",
    "                system_message += f\"- {t['subject']} {t['predicate']} {t['object']} (Videotimestamps: start: {t['start']}, end: {t['end']})\\n\"\n",
    "            \n",
    "        else:\n",
    "            system_message += \"No direct related knowledge found. Proceeding with general reasoning.\\n\"\n",
    "        enriched_history = [{\"role\": \"system\", \"content\": f\"You are a helpful assistent; {system_message}\"}] #+ conversation_history\n",
    "        enriched_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        print(enriched_history)\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=enriched_history,\n",
    "            temperature=0.7,\n",
    "        )['choices'][0]['message']['content']\n",
    "        return response\n",
    "\n",
    "    def chat(self):\n",
    "        print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "        while True:\n",
    "            user_message = input(\"You: \")\n",
    "            if user_message.lower().strip() in ['exit', 'quit']:\n",
    "                print(\"Chatbot: Goodbye!\")\n",
    "                break\n",
    "            self.save_message(role='user', content=user_message)\n",
    "            conversation = self.load_json_data(self.messages_file)[-3:]\n",
    "            assistant_response = self.generate_response(conversation, user_message)\n",
    "            print(f\"Assistant: {assistant_response}\")\n",
    "            self.save_message(role='assistant', content=assistant_response)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = Chatbot()\n",
    "    chatbot.chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUTION PIPELINE\n",
    "The evalution pipeline is implemented using the `llama_cpp` library.\n",
    "\n",
    "The evalution pipeline is used to evaluate the performance of the LLM.\n",
    "\n",
    "Gives each LLM a score based on the number of correct answers given by the LLM.\n",
    "\n",
    "Generates a automatic result by answering questions from the test set with or without the knowledge base.\n",
    "\n",
    "The pipeline can take multiple models as input and evaluate them all at once, and generate a score for each model. \n",
    "\n",
    "The pipeline can also be used to evaluate the performance of the LLM on a specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial version of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from llama_cpp import Llama\n",
    "import threading\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from json import JSONDecodeError\n",
    "\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/Qwen2.5-7B-Instruct-Q4_K_M.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=2048)\n",
    "            return cls._instance\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, \n",
    "                 messages_file='messages.json', \n",
    "                 knowledge_file='knowledge.json', \n",
    "                 faiss_index_file='faiss_index.pkl',\n",
    "                 model_name='all-MiniLM-L6-v2'):\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "        self.llm = LlamaSingleton().llm\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.knowledge_data = []\n",
    "        self.initialize_files()\n",
    "        self.load_faiss_index()\n",
    "\n",
    "    def initialize_files(self):\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "\n",
    "    def load_faiss_index(self):\n",
    "        if os.path.exists(self.faiss_index_file):\n",
    "            with open(self.faiss_index_file, 'rb') as f:\n",
    "                self.index, self.knowledge_data = pickle.load(f)\n",
    "        else:\n",
    "            self.index = None\n",
    "            self.knowledge_data = []\n",
    "\n",
    "    def search_knowledge(self, query, top_k=5):\n",
    "        if self.index is None or len(self.knowledge_data) == 0:\n",
    "            return []\n",
    "        query_embedding = self.model.encode([query])\n",
    "        distances, indices = self.index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            results.append(self.knowledge_data[idx])\n",
    "        return results\n",
    "\n",
    "    def generate_response(self, user_message):\n",
    "        knowledge_matches = self.search_knowledge(user_message, top_k=5)\n",
    "        current_time = datetime.utcnow().isoformat()\n",
    "        system_message = f\"Current date and time: {current_time}\\n\"\n",
    "        if knowledge_matches:\n",
    "            system_message += \"Answer based on retrieved knowledge, but only if it relates to the question:\\n\"\n",
    "            for t in knowledge_matches:\n",
    "                system_message += f\"- {t['subject']} {t['predicate']} {t['object']} (Videotimestamps: start: {t['start']}, end: {t['end']})\\n\"\n",
    "            \n",
    "        else:\n",
    "            system_message += \"\\n\"\n",
    "        enriched_history = [{\"role\": \"system\", \"content\": f\"You are a helpful assistent; {system_message}\"}] \n",
    "        enriched_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        print(enriched_history)\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=enriched_history,\n",
    "            temperature=0.5,\n",
    "        )['choices'][0]['message']['content']\n",
    "        return response\n",
    "\n",
    "    def chat(self):\n",
    "        print(\"Chatbot is ready! Type 'exit' to end the conversation.\")\n",
    "        while True:\n",
    "            user_message = input(\"You: \")\n",
    "            if user_message.lower().strip() in ['exit', 'quit']:\n",
    "                print(\"Chatbot: Goodbye!\")\n",
    "                break\n",
    "            assistant_response = self.generate_response(user_message)\n",
    "            print(f\"Assistant: {assistant_response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = Chatbot()\n",
    "    chatbot.chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final evaluation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import threading\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from llama_cpp import Llama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from json import JSONDecodeError\n",
    "\n",
    "class LlamaSingleton:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls, model_path=\"models/Qwen2.5-7B-Instruct-Q4_K_M.gguf\", chat_format=\"chatml\"):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super(LlamaSingleton, cls).__new__(cls)\n",
    "                cls._instance.llm = Llama(model_path=model_path, chat_format=chat_format, n_ctx=2048)\n",
    "            return cls._instance\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, \n",
    "                 messages_file='messages.json', \n",
    "                 knowledge_file='knowledge.json', \n",
    "                 faiss_index_file='faiss_index.pkl',\n",
    "                 model_name='all-MiniLM-L6-v2',\n",
    "                 llm_model_path=\"models/Qwen2.5-7B-Instruct-Q4_K_M.gguf\"):\n",
    "        self.messages_file = messages_file\n",
    "        self.knowledge_file = knowledge_file\n",
    "        self.faiss_index_file = faiss_index_file\n",
    "        # Instantiate LlamaSingleton with the provided model path\n",
    "        self.llm = LlamaSingleton(model_path=llm_model_path).llm\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.index = None\n",
    "        self.knowledge_data = []\n",
    "        self.initialize_files()\n",
    "        self.load_faiss_index()\n",
    "\n",
    "    def initialize_files(self):\n",
    "        for file in [self.messages_file, self.knowledge_file]:\n",
    "            if not os.path.exists(file):\n",
    "                with open(file, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "\n",
    "    def load_json_data(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def load_faiss_index(self):\n",
    "        if os.path.exists(self.faiss_index_file):\n",
    "            with open(self.faiss_index_file, 'rb') as f:\n",
    "                self.index, self.knowledge_data = pickle.load(f)\n",
    "        else:\n",
    "            self.index = None\n",
    "            self.knowledge_data = []\n",
    "\n",
    "    def search_knowledge(self, query, top_k=5):\n",
    "        if self.index is None or len(self.knowledge_data) == 0:\n",
    "            return []\n",
    "        query_embedding = self.model.encode([query])\n",
    "        distances, indices = self.index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "        results = []\n",
    "        for idx in indices[0]:\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            results.append(self.knowledge_data[idx])\n",
    "        return results\n",
    "\n",
    "    def generate_response_with_kb(self, user_message):\n",
    "        knowledge_matches = self.search_knowledge(user_message, top_k=5)\n",
    "        current_time = datetime.utcnow().isoformat()\n",
    "        system_message = f\"Current date and time: {current_time}\\n\"\n",
    "        if knowledge_matches:\n",
    "            system_message += \"Choose one answer based on retrieved knowledge, if it relates to the question:\\n\"\n",
    "            for t in knowledge_matches:\n",
    "                system_message += f\"- {t['subject']} {t['predicate']} {t['object']} (Videotimestamps: start: {t['start']}, end: {t['end']})\\n\"\n",
    "        else:\n",
    "            system_message += \"\\n\"\n",
    "        enriched_history = [{\"role\": \"system\", \"content\": f\"You are a helpful assistent; {system_message}\"}]\n",
    "        enriched_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        print(knowledge_matches)\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=enriched_history,\n",
    "            temperature=0.5,\n",
    "        )['choices'][0]['message']['content']\n",
    "        return response\n",
    "\n",
    "    def generate_response_without_kb(self, user_message):\n",
    "        current_time = datetime.utcnow().isoformat()\n",
    "        system_message = f\"Current date and time: {current_time}\\n\"\n",
    "        enriched_history = [{\"role\": \"system\", \"content\": f\"You are a helpful assistent. Choose one answer; {system_message}\"}]\n",
    "        enriched_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        response = self.llm.create_chat_completion(\n",
    "            messages=enriched_history,\n",
    "            temperature=0.5,\n",
    "        )['choices'][0]['message']['content']\n",
    "        return response\n",
    "\n",
    "def run_evaluation(llm_model_path, questions_path):\n",
    "\n",
    "    # Load the questions from the  JSON file\n",
    "    with open(questions_path, 'r', encoding='utf-8') as f:\n",
    "        questions = json.load(f)\n",
    "\n",
    "    # Chatbot using a model path\n",
    "    chatbot = Chatbot(llm_model_path=llm_model_path)\n",
    "\n",
    "    # Loop over all questions\n",
    "    for question in questions:\n",
    "\n",
    "        # make a prompt that includes the question text and options.\n",
    "        prompt = f\"Question: {question['question']}\\nOptions:\\n\"\n",
    "\n",
    "        for idx, opt in enumerate(question['options']):\n",
    "\n",
    "            prompt += f\"{opt}\\n\"\n",
    "\n",
    "        # Generate responses using the two methods.\n",
    "        response_with_kb = chatbot.generate_response_with_kb(prompt)\n",
    "        response_without_kb = chatbot.generate_response_without_kb(prompt)\n",
    "\n",
    "        # Add the responses to the question JSON object.\n",
    "        question[\"llm_answer_with_kb\"] = response_with_kb\n",
    "        question[\"llm_answer_without_kb\"] = response_without_kb\n",
    "\n",
    "    # Derive the LLM name from the model path.\n",
    "    llm_name = os.path.splitext(os.path.basename(llm_model_path))[0]\n",
    "    result_filename = f\"result_eval_{llm_name}.json\"\n",
    "\n",
    "    # Save the updated questions JSON to the new file.\n",
    "    with open(result_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(questions, f, indent=4)\n",
    "\n",
    "    print(f\"Evaluation complete. Results saved to {result_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    # Check command-line arguments\n",
    "    model_paths = [\"models/Qwen2.5-0.5B-Instruct-f16.gguf\",\n",
    "                   \"models/Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "                   \"models/gemma-2-2b-it.F16.gguf\",\n",
    "                   \"models/Llama-3.2-3B-Instruct-f16.gguf\",\n",
    "                   \"models/Qwen2.5-7B-Instruct-Q4_K_M.gguf\",\n",
    "                   \"models/Llama-3.2-11B-Vision-Instruct.Q4_K_M.gguf\",\n",
    "                   \"models/gemma-2-9b-it-Q4_K_M.gguf\",\n",
    "                   \"models/phi-4-14b-Q4_K_M.gguf\",\n",
    "                   ]\n",
    "    \n",
    "    for model_path in model_paths:\n",
    "        questions_path = \"questions.json\"\n",
    "        run_evaluation(model_path, questions_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
