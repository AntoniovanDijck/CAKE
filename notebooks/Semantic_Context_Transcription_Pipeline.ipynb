{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git\n",
    "!pip install git+https://github.com/federicotorrielli/BetterWhisperX.git\n",
    "!pip install pydub yt-dlp moviepy ffmpeg chonkie \"chonkie[semantic]\" whisperx==3.1.5 opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab cuda support (run dit alleen op Google Colab)\n",
    "- restart env en dan alles opnieuw runnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall ctranslate2==4.5.0 -y\n",
    "# !pip install ctranslate2==4.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input video downloaden (nu nog youtube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "import subprocess\n",
    "\n",
    "def download_and_convert_to_wav(youtube_url, output_wav_path):\n",
    "  \n",
    "    print(f\"Downloading audio from {youtube_url}...\")\n",
    "    audio_file = \"temp_audio.mp4\"  # Temporary file\n",
    "    os.system(f'yt-dlp -f \"bestaudio\" -o \"{audio_file}\" \"{youtube_url}\"')\n",
    "\n",
    "    print(\"Converting to WAV format...\")\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "    audio.export(output_wav_path, format=\"wav\")\n",
    "\n",
    "    os.remove(audio_file)\n",
    "    print(f\"Conversion complete! WAV file saved to: {output_wav_path}\")\n",
    "\n",
    "def download_video(youtube_url, output_video_path):\n",
    "    try:\n",
    "        print(f\"Downloading video from {youtube_url} (high quality video only)...\")\n",
    "        output_template = \"temp_video.mp4\"\n",
    "\n",
    "        command = [\n",
    "            \"yt-dlp\", \"-f\", \"bestvideo[ext=mp4]\", \"-o\", output_template, youtube_url\n",
    "        ]\n",
    "\n",
    "        subprocess.run(command, check=True)\n",
    "\n",
    "        if os.path.exists(output_template):\n",
    "            os.rename(output_template, output_video_path)\n",
    "            print(f\"Video saved as '{output_video_path}'.\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Failed to download video as MP4.\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error downloading video: {e}\")\n",
    "\n",
    "def process_links_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads YouTube links from a file and processes each one.\n",
    "    \"\"\"\n",
    "    os.makedirs(\"wav_files\", exist_ok=True)\n",
    "    os.makedirs(\"mp4_files\", exist_ok=True)\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        links = file.readlines()\n",
    "\n",
    "    for index, link in enumerate(links):\n",
    "        link = link.strip()\n",
    "        if link:\n",
    "            output_wav_path = os.path.join(\"wav_files\", f\"output_audio_{index + 1}.wav\")\n",
    "            output_video_path = os.path.join(\"mp4_files\", f\"input_video_{index + 1}.mp4\")\n",
    "            download_and_convert_to_wav(link, output_wav_path)\n",
    "            download_video(link, output_video_path)\n",
    "\n",
    "# Bestand met YouTube-links\n",
    "input_file = \"youtube_links.txt\"\n",
    "process_links_from_file(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx\n",
    "import gc\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Input\n",
    "wav_folder = \"wav_files\"\n",
    "output_folder = \"transcriptions\"\n",
    "unsupported_folder = \"unsupported_language\"\n",
    "model_dir = \"whisper-models\"\n",
    "\n",
    "# Ensure output folders exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(unsupported_folder, exist_ok=True)\n",
    "\n",
    "def transcribe(audio_file):\n",
    "    # Check system for compatibility\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        print(\"CUDA wordt gebruikt\")\n",
    "        compute_type = \"float16\"  # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "        batch_size = 16  # reduce if low on GPU mem\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"cpu\"\n",
    "        print(\"MPS (Apple Silicon) gebruikt\")\n",
    "        compute_type = \"int8\"\n",
    "        batch_size = 8\n",
    "    else:\n",
    "        print(\"CPU gebruikt\")\n",
    "        device = \"cpu\"\n",
    "        compute_type = \"int8\"\n",
    "        batch_size = 4\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, download_root=model_dir)\n",
    "    else:\n",
    "        model = whisperx.load_model(\"./whisper-models/models--Systran--faster-whisper-large-v2/snapshots/f0fe81560cb8b68660e564f55dd99207059c092e\", device, compute_type=compute_type)\n",
    "\n",
    "    audio = whisperx.load_audio(audio_file)\n",
    "\n",
    "    # Perform transcription with automatic language detection\n",
    "    result = model.transcribe(audio, batch_size=batch_size)\n",
    "    detected_language = result.get(\"language\", \"en\")\n",
    "\n",
    "    # Check if detected language is supported, otherwise move file to unsupported folder\n",
    "    if detected_language not in [\"en\", \"fr\", \"de\", \"es\"]:\n",
    "        print(f\"Language detected as {detected_language}, moving to unsupported folder.\")\n",
    "        os.rename(audio_file, os.path.join(unsupported_folder, os.path.basename(audio_file)))\n",
    "        return\n",
    "\n",
    "    print(f\"Detected language: {detected_language}\")\n",
    "\n",
    "   \n",
    "    try:\n",
    "        model_a, metadata = whisperx.load_align_model(language_code=detected_language, device=device)\n",
    "        result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        del model_a\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping alignment due to error: {e}\")\n",
    "\n",
    "    # Save as JSON\n",
    "    base_filename = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "    output_json_path = os.path.join(output_folder, f\"{base_filename}.json\")\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "\n",
    "    print(f\"Results saved to {output_json_path}\")\n",
    "\n",
    "# Process all WAV files in the folder\n",
    "for filename in os.listdir(wav_folder):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        audio_path = os.path.join(wav_folder, filename)\n",
    "        transcribe(audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full text of transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Input directory containing JSON files\n",
    "json_folder = 'transcriptions'\n",
    "output_dir = 'individual_texts'\n",
    "\n",
    "# Maak output directory aan als deze niet bestaat\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Output directory '{output_dir}' is ready.\")\n",
    "\n",
    "# Process each JSON file individually\n",
    "for json_file in os.listdir(json_folder):\n",
    "    if json_file.endswith('.json'):\n",
    "        json_path = os.path.join(json_folder, json_file)\n",
    "        print(f\"Processing file: {json_file}\")\n",
    "\n",
    "        # Controleer of het JSON-bestand geldig is\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "        except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "            print(f\"Error: Failed to process '{json_file}'. Details: {e}\")\n",
    "            continue\n",
    "\n",
    "        segments = data.get('segments', [])\n",
    "        if not segments:\n",
    "            print(f\"Warning: No segments found in '{json_file}'.\")\n",
    "            continue\n",
    "\n",
    "        # Create output text file for the individual transcript\n",
    "        individual_output_path = os.path.join(output_dir, json_file.replace('.json', '.txt'))\n",
    "\n",
    "        with open(individual_output_path, 'w', encoding='utf-8') as individual_file:\n",
    "            for i, segment in enumerate(segments, start=1):\n",
    "                text = segment.get('text', '').strip()\n",
    "                if not text:\n",
    "                    print(f\"Warning: Segment {i} in '{json_file}' is empty.\")\n",
    "                    continue\n",
    "                individual_file.write(f\"{text} \")\n",
    "\n",
    "        print(f\"Transcript saved to '{individual_output_path}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output chonkie chunks with timestamp (in JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from chonkie import SDPMChunker\n",
    "\n",
    "def load_document(file_path: str) -> str:\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file '{file_path}' does not exist.\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "def load_json(file_path: str) -> dict:\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Error: The JSON file '{file_path}' does not exist.\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            return json.load(f)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Error: Failed to decode JSON file. Details: {e}\")\n",
    "\n",
    "def create_chunker(embedding_model=\"minishlab/potion-base-8M\", chunk_size=512, min_sentences=1):\n",
    "\n",
    "    return SDPMChunker(\n",
    "        embedding_model=embedding_model,\n",
    "        chunk_size=chunk_size,\n",
    "        min_sentences=min_sentences\n",
    "    )\n",
    "\n",
    "def process_text_and_json(text_folder: str, json_folder: str, output_folder: str):\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for text_file in os.listdir(text_folder):\n",
    "        if text_file.endswith(\".txt\"):\n",
    "            base_name = os.path.splitext(text_file)[0]\n",
    "            text_path = os.path.join(text_folder, text_file)\n",
    "            json_path = os.path.join(json_folder, base_name + \".json\")\n",
    "\n",
    "            if not os.path.exists(json_path):\n",
    "                print(f\"Warning: No matching JSON file for {text_file}\")\n",
    "                continue\n",
    "\n",
    "            text_content = load_document(text_path)\n",
    "            json_data = load_json(json_path)\n",
    "            segments = json_data.get('word_segments', [])\n",
    "\n",
    "            if not segments:\n",
    "                raise ValueError(f\"Error: No segments found in the JSON file {json_path}.\")\n",
    "\n",
    "            word_list = [[seg.get('word', '').strip(), seg.get('start', ''), seg.get('end', '')] for seg in segments if seg.get('word', '').strip()]\n",
    "            chunker = create_chunker()\n",
    "            chunks = chunker.chunk(text_content)\n",
    "\n",
    "            final_chunks = []\n",
    "            current_word_index = 0\n",
    "            for chunk in chunks:\n",
    "                chunk_text = chunk.text\n",
    "                chunk_words = chunk_text.split()\n",
    "                chunk_word_data = []\n",
    "                chunk_start = None\n",
    "                chunk_end = None\n",
    "\n",
    "                for chunk_word in chunk_words:\n",
    "                    if current_word_index < len(word_list):\n",
    "                        word_info = word_list[current_word_index]\n",
    "                        if chunk_word == word_info[0]:\n",
    "                            chunk_word_data.append({\n",
    "                                \"word\": word_info[0],\n",
    "                                \"start\": word_info[1],\n",
    "                                \"end\": word_info[2]\n",
    "                            })\n",
    "                            if chunk_start is None:\n",
    "                                chunk_start = word_info[1]\n",
    "                            chunk_end = word_info[2]\n",
    "                            current_word_index += 1\n",
    "                        else:\n",
    "                            raise ValueError(f\"Word mismatch at chunk '{chunk_text}': Expected '{word_info[0]}', found '{chunk_word}'.\")\n",
    "                    else:\n",
    "                        raise IndexError(\"Ran out of words in word_data to match with chunks.\")\n",
    "\n",
    "                final_chunks.append({\n",
    "                    \"text\": chunk_text,\n",
    "                    \"start\": chunk_start,\n",
    "                    \"end\": chunk_end,\n",
    "                    \"words\": chunk_word_data\n",
    "                })\n",
    "\n",
    "            output_json_path = os.path.join(output_folder, base_name + \"_chunks.json\")\n",
    "            with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump({\"chunks\": final_chunks}, f, ensure_ascii=False, indent=4)\n",
    "                print(f\"Processed {text_file} and saved to {output_json_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text_folder = 'individual_texts'\n",
    "    json_folder = 'transcriptions'\n",
    "    output_folder = 'processed_json'\n",
    "    process_text_and_json(text_folder, json_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video segmentation chonkie\n",
    "### Het segmenteren van de video op basis van de nieuwe chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def create_segments(video_file, result):\n",
    "\n",
    "    if not os.path.exists(result):\n",
    "        raise FileNotFoundError(f\"Error: The result file '{result}' does not exist.\")\n",
    "\n",
    "    with open(result, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    segments = data.get('chunks', [])\n",
    "    if not segments:\n",
    "        raise ValueError(\"No segments found in the JSON file.\")\n",
    "\n",
    "    video_name = os.path.splitext(os.path.basename(video_file))[0]\n",
    "    output_dir = os.path.join('video_segments', video_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Output directory '{output_dir}' is ready.\")\n",
    "\n",
    "    for i, segment in enumerate(segments, start=1):\n",
    "        start = segment['start']\n",
    "        end = segment['end']\n",
    "        output_filename = f\"segment_{i}_{int(start)}_{int(end)}.mp4\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "        command = [\n",
    "            \"ffmpeg\",\n",
    "            \"-y\",\n",
    "            \"-i\", video_file,\n",
    "            \"-ss\", str(start),\n",
    "            \"-to\", str(end),\n",
    "            \"-c:v\", \"libx264\",\n",
    "            \"-c:a\", \"aac\",\n",
    "            output_path\n",
    "        ]\n",
    "        print(f\"Creating segment {i}: {start} to {end} seconds for {video_file}.\")\n",
    "        subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        print(f\"Segment {i} saved as '{output_filename}'.\")\n",
    "\n",
    "    print(f\"All segments for {video_file} have been processed.\")\n",
    "\n",
    "def process_videos_in_directory(video_directory, json_directory):\n",
    "\n",
    "    if not os.path.isdir(video_directory):\n",
    "        raise NotADirectoryError(f\"Error: The directory '{video_directory}' does not exist.\")\n",
    "    if not os.path.isdir(json_directory):\n",
    "        raise NotADirectoryError(f\"Error: The directory '{json_directory}' does not exist.\")\n",
    "\n",
    "    video_files = sorted(f for f in os.listdir(video_directory) if f.endswith('.mp4'))\n",
    "    json_files = sorted(f for f in os.listdir(json_directory) if f.endswith('.json'))\n",
    "\n",
    "    for video_file, json_file in zip(video_files, json_files):\n",
    "        video_path = os.path.join(video_directory, video_file)\n",
    "        json_path = os.path.join(json_directory, json_file)\n",
    "        create_segments(video_path, json_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_directory = \"mp4_files\"  # Replace with your video directory\n",
    "    json_directory = \"processed_json\"  # Replace with your JSON directory\n",
    "    process_videos_in_directory(video_directory, json_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middle frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_middle_frame(video_path, output_dir):\n",
    "    video_name = os.path.basename(video_path).replace(\".\", \"_\")\n",
    "    segment_output_dir = os.path.join(output_dir, *video_path.split(os.sep)[-2:])\n",
    "    os.makedirs(segment_output_dir, exist_ok=True)\n",
    "\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if total_frames == 0:\n",
    "        print(f\"Warning: No frames found in {video_path}\")\n",
    "        return\n",
    "\n",
    "    middle_frame_idx = total_frames // 2\n",
    "    vidcap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame_idx)\n",
    "    success, image = vidcap.read()\n",
    "\n",
    "    if success:\n",
    "        frame_path = os.path.join(segment_output_dir, f\"{video_name}_middle_frame.jpg\")\n",
    "        cv2.imwrite(frame_path, image)\n",
    "        print(f\"Extracted middle frame from {video_path} to {frame_path}\")\n",
    "    else:\n",
    "        print(f\"Error: Could not read frame {middle_frame_idx} from {video_path}\")\n",
    "\n",
    "    vidcap.release()\n",
    "\n",
    "def process_videos_in_directory(base_directory, output_directory):\n",
    "    if not os.path.isdir(base_directory):\n",
    "        raise NotADirectoryError(f\"Error: The directory '{base_directory}' does not exist.\")\n",
    "\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    print(f\"Output directory '{output_directory}' is ready.\")\n",
    "\n",
    "    for subdir in sorted(os.listdir(base_directory)):\n",
    "        subdir_path = os.path.join(base_directory, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            for filename in sorted(os.listdir(subdir_path)):\n",
    "                if filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                    video_path = os.path.join(subdir_path, filename)\n",
    "                    extract_middle_frame(video_path, output_directory)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_directory = \"video_segments\"  # Vervang dit door je video directory\n",
    "    output_directory = \"frames\"  # Map om de frames op te slaan\n",
    "    process_videos_in_directory(video_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 frame per 3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path, output_dir, interval_seconds=3):\n",
    "    video_name = os.path.basename(video_path).replace(\".\", \"_\")\n",
    "    segment_output_dir = os.path.join(output_dir, *video_path.split(os.sep)[-2:])\n",
    "    os.makedirs(segment_output_dir, exist_ok=True)\n",
    "\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps <= 0:\n",
    "        print(f\"Kan de FPS voor {video_path} niet ophalen.\")\n",
    "        return\n",
    "\n",
    "    frame_interval = int(fps * interval_seconds)\n",
    "    success, image = vidcap.read()\n",
    "    count = 0\n",
    "    frame_count = 0\n",
    "\n",
    "    while success:\n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame_path = os.path.join(segment_output_dir, f\"{video_name}_frame_{count}.jpg\")\n",
    "            cv2.imwrite(frame_path, image)\n",
    "            count += 1\n",
    "        success, image = vidcap.read()\n",
    "        frame_count += 1\n",
    "\n",
    "    vidcap.release()\n",
    "    print(f\"Geëxtraheerd {count} frames uit {video_path} naar {segment_output_dir}\")\n",
    "\n",
    "def process_videos_in_directory(base_directory, output_directory, interval_seconds=3):\n",
    "    if not os.path.isdir(base_directory):\n",
    "        raise NotADirectoryError(f\"Error: De map '{base_directory}' bestaat niet.\")\n",
    "\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    print(f\"Output directory '{output_directory}' is gereed.\")\n",
    "\n",
    "    for subdir in sorted(os.listdir(base_directory)):\n",
    "        subdir_path = os.path.join(base_directory, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            for filename in sorted(os.listdir(subdir_path)):\n",
    "                if filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                    video_path = os.path.join(subdir_path, filename)\n",
    "                    extract_frames(video_path, output_directory, interval_seconds)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_directory = \"video_segments\"  # Vervang dit door jouw basisvideo-segmentenmap\n",
    "    output_directory = \"frames\"         # Map om geëxtraheerde frames op te slaan\n",
    "    process_videos_in_directory(video_directory, output_directory, interval_seconds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import shutil  # Voor het verwijderen van directories\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    x: float\n",
    "    y: float\n",
    "    label: int  # 0 for background, 1 for foreground\n",
    "\n",
    "@dataclass\n",
    "class BoxPrompt:\n",
    "    \"\"\"Represents a bounding box by two corners.\"\"\"\n",
    "    x1: float\n",
    "    y1: float\n",
    "    x2: float\n",
    "    y2: float\n",
    "\n",
    "class AdvancedAutoSAM:\n",
    "    def __init__(self, input_size=(1024, 1024)):\n",
    "        \"\"\"\n",
    "        :param input_size: The (width, height) that the SAM image encoder expects\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.original_size = None\n",
    "        \n",
    "        self.image_encoder = None\n",
    "        self.prompt_encoder = None\n",
    "        self.mask_decoder = None\n",
    "        \n",
    "        self.image_embeddings = None  # Will store the results of the image encoder\n",
    "\n",
    "    def load_models(self, image_encoder_path, prompt_encoder_path, mask_decoder_path):\n",
    "        \"\"\"Load the three CoreML models: image encoder, prompt encoder, mask decoder.\"\"\"\n",
    "        start = time.time()\n",
    "        self.image_encoder = ct.models.MLModel(image_encoder_path)\n",
    "        self.prompt_encoder = ct.models.MLModel(prompt_encoder_path)\n",
    "        self.mask_decoder = ct.models.MLModel(mask_decoder_path)\n",
    "        print(f\"Models loaded in {time.time()-start:.2f} sec.\")\n",
    "\n",
    "    def _resize_image_for_encoder(self, image_path: str) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Loads an image from disk, converts to RGB, resizes to self.input_size.\n",
    "        \"\"\"\n",
    "        pil_image = Image.open(image_path).convert(\"RGB\")\n",
    "        self.original_size = pil_image.size  # (width, height) before resizing\n",
    "        pil_image = pil_image.resize(self.input_size, Image.Resampling.LANCZOS)\n",
    "        return pil_image\n",
    "\n",
    "    def get_image_embedding(self, image_path: str):\n",
    "        \"\"\"\n",
    "        Preprocess the image and run the image encoder. \n",
    "        Store the resulting embeddings for future use.\n",
    "        \"\"\"\n",
    "        if self.image_encoder is None:\n",
    "            raise ValueError(\"Image encoder model not loaded.\")\n",
    "        start = time.time()\n",
    "        image = self._resize_image_for_encoder(image_path)\n",
    "        self.image_embeddings = self.image_encoder.predict({\"image\": image})\n",
    "        print(f\"Image encoding done in {time.time()-start:.2f} sec.\")\n",
    "\n",
    "    def _transform_box_coords(\n",
    "        self, box: BoxPrompt, original_size: Tuple[int, int]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Scale box coordinates from original image size -> input_size\n",
    "        to match the resized image used by the encoder.\n",
    "        Returns shape: (1, 4) => [x1, y1, x2, y2] after scaling\n",
    "        (depending on your prompt encoder's specification).\n",
    "        \"\"\"\n",
    "        ow, oh = original_size\n",
    "        tw, th = self.input_size\n",
    "\n",
    "        scale_x = tw / float(ow)\n",
    "        scale_y = th / float(oh)\n",
    "\n",
    "        x1_s = box.x1 * scale_x\n",
    "        y1_s = box.y1 * scale_y\n",
    "        x2_s = box.x2 * scale_x\n",
    "        y2_s = box.y2 * scale_y\n",
    "        \n",
    "        # Create shape (1,4)\n",
    "        box_array = np.array([[x1_s, y1_s, x2_s, y2_s]], dtype=np.float32)\n",
    "        return box_array\n",
    "\n",
    "    def _transform_point_coords(\n",
    "        self, points: np.ndarray, original_size: Tuple[int, int]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Scale point coordinates from original image -> input_size\n",
    "        Returns shape (1, N, 2).\n",
    "        \"\"\"\n",
    "        ow, oh = original_size\n",
    "        tw, th = self.input_size\n",
    "\n",
    "        scale_x = tw / float(ow)\n",
    "        scale_y = th / float(oh)\n",
    "\n",
    "        # points shape: (N,2)\n",
    "        points_s = points.copy()\n",
    "        points_s[:, 0] *= scale_x\n",
    "        points_s[:, 1] *= scale_y\n",
    "\n",
    "        # expand to (1, N, 2)\n",
    "        points_s = np.expand_dims(points_s, axis=0).astype(np.float32)\n",
    "        return points_s\n",
    "\n",
    "    def _predict_mask(self, sparse_embeddings, dense_embeddings):\n",
    "        \"\"\"\n",
    "        Runs the mask decoder with the precomputed image_embeddings \n",
    "        plus the given prompt-encoder outputs (sparse & dense).\n",
    "        Returns a [low_res_mask] of shape ~ (4x?) or (256x256?), \n",
    "        along with scores. We pick the highest scoring mask.\n",
    "        \"\"\"\n",
    "        if self.mask_decoder is None:\n",
    "            raise ValueError(\"Mask decoder not loaded.\")\n",
    "\n",
    "        out = self.mask_decoder.predict({\n",
    "            \"image_embedding\": self.image_embeddings[\"image_embedding\"],\n",
    "            \"sparse_embedding\": sparse_embeddings,\n",
    "            \"dense_embedding\": dense_embeddings,\n",
    "            \"feats_s0\": self.image_embeddings[\"feats_s0\"],\n",
    "            \"feats_s1\": self.image_embeddings[\"feats_s1\"],\n",
    "        })\n",
    "\n",
    "        # out[\"scores\"] shape: (batch_size, numMasks) => typically (1,3)\n",
    "        scores = out[\"scores\"]\n",
    "        best_idx = np.argmax(scores)\n",
    "        low_res_mask = out[\"low_res_masks\"][0, best_idx]  # shape e.g. (256,256)\n",
    "        return low_res_mask, float(scores[0, best_idx])\n",
    "\n",
    "    def _resize_and_binarize_mask(\n",
    "        self, low_res_mask: np.ndarray, original_size: Tuple[int,int], threshold:float=0.0\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Resizes the mask from ~[256x256 or 1024x1024] back to the original image size.\n",
    "        Binarizes at the given threshold (default 0.0).\n",
    "        \"\"\"\n",
    "        ow, oh = original_size\n",
    "        # OpenCV expects (width, height) => (ow, oh)\n",
    "        mask_resized = cv2.resize(\n",
    "            low_res_mask,\n",
    "            (ow, oh),\n",
    "            interpolation=cv2.INTER_LINEAR\n",
    "        )\n",
    "        # Binarize\n",
    "        binary = (mask_resized > threshold).astype(np.uint8) * 255  # Multiply by 255 for proper visualization\n",
    "        return binary\n",
    "\n",
    "    ############################################################################\n",
    "    #                  MULTI-SCALE BOUNDING BOX PROPOSAL LOGIC                 #\n",
    "    ############################################################################\n",
    "\n",
    "    def _multi_scale_bounding_box_proposals(\n",
    "        self, image_path:str, scales=[1.0, 0.75, 0.5], edge_thresh=100\n",
    "    ) -> List[Tuple[BoxPrompt, float]]:\n",
    "        \"\"\"\n",
    "        Example: (Over)Simplified bounding-box proposals at multiple scales.\n",
    "        \n",
    "        1) For each scale in `scales`, we downsize the original image.\n",
    "        2) We run a naive edge detection or threshold to find regions.\n",
    "        3) We group edges via connected components or contours to get bounding boxes.\n",
    "        4) We scale those bounding boxes back up to the original coordinate space.\n",
    "        \n",
    "        Return => list of (BoxPrompt, score), where \"score\" can be e.g. area or edge magnitude.\n",
    "        \"\"\"\n",
    "        bgr_original = cv2.imread(image_path)\n",
    "        if bgr_original is None:\n",
    "            raise ValueError(f\"Could not read image at {image_path}\")\n",
    "        oh, ow = bgr_original.shape[:2]\n",
    "\n",
    "        proposals = []\n",
    "        \n",
    "        for s in scales:\n",
    "            if s <= 0:\n",
    "                continue\n",
    "            # Resize\n",
    "            w_s = int(ow*s)\n",
    "            h_s = int(oh*s)\n",
    "            small = cv2.resize(bgr_original, (w_s, h_s), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            # Convert to grayscale\n",
    "            gray_small = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)\n",
    "            # Simple edge detection\n",
    "            edges = cv2.Canny(gray_small, threshold1=edge_thresh, threshold2=3*edge_thresh)\n",
    "            # Alternatively: threshold => detect connected components => bounding boxes\n",
    "\n",
    "            # Find contours for bounding boxes\n",
    "            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            for cnt in contours:\n",
    "                x, y, w, h = cv2.boundingRect(cnt)\n",
    "                area = w*h\n",
    "                if area < 20:  # discard tiny proposals in scaled space\n",
    "                    continue\n",
    "                \n",
    "                # Scale box back to original image coordinates\n",
    "                # For scale s: (x / s, y / s) is top-left\n",
    "                # (x+w)/s, (y+h)/s is bottom-right\n",
    "                X1 = x / s\n",
    "                Y1 = y / s\n",
    "                X2 = (x + w) / s\n",
    "                Y2 = (y + h) / s\n",
    "\n",
    "                # Clip to original boundaries\n",
    "                X1 = max(0, min(X1, ow-1))\n",
    "                Y1 = max(0, min(Y1, oh-1))\n",
    "                X2 = max(0, min(X2, ow-1))\n",
    "                Y2 = max(0, min(Y2, oh-1))\n",
    "\n",
    "                # \"score\" can be approximate area in original scale\n",
    "                # area_in_original = (X2 - X1)*(Y2 - Y1)\n",
    "                # Or use mean edges, etc. We'll keep it simple:\n",
    "                box_score = float(area)\n",
    "\n",
    "                proposals.append((BoxPrompt(X1, Y1, X2, Y2), box_score))\n",
    "\n",
    "        return proposals\n",
    "\n",
    "    def _merge_duplicate_boxes(\n",
    "        self, boxes_with_scores: List[Tuple[BoxPrompt, float]], iou_thresh=0.5\n",
    "    ) -> List[BoxPrompt]:\n",
    "        \"\"\"\n",
    "        Non-maximum suppression (NMS) for bounding boxes.\n",
    "        Sort by box score, pick highest, remove boxes that overlap (IoU>iou_thresh).\n",
    "        Return final list of unique boxes.\n",
    "        \"\"\"\n",
    "        # Convert to a list of tuples\n",
    "        data = []\n",
    "        for (box, score) in boxes_with_scores:\n",
    "            data.append((box.x1, box.y1, box.x2, box.y2, score))\n",
    "        # Sort by descending score\n",
    "        data = sorted(data, key=lambda x: x[-1], reverse=True)\n",
    "\n",
    "        final_boxes = []\n",
    "        \n",
    "        def iou(boxA, boxB):\n",
    "            # boxA, boxB => (x1,y1,x2,y2)\n",
    "            xA = max(boxA[0], boxB[0])\n",
    "            yA = max(boxA[1], boxB[1])\n",
    "            xB = min(boxA[2], boxB[2])\n",
    "            yB = min(boxA[3], boxB[3])\n",
    "            interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "            areaA = (boxA[2]-boxA[0]) * (boxA[3]-boxA[1])\n",
    "            areaB = (boxB[2]-boxB[0]) * (boxB[3]-boxB[1])\n",
    "            union = areaA + areaB - interArea\n",
    "            return interArea / union if union > 0 else 0.0\n",
    "\n",
    "        suppressed = [False]*len(data)\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            if suppressed[i]:\n",
    "                continue\n",
    "            # select box i\n",
    "            bA = data[i]\n",
    "            final_boxes.append(BoxPrompt(bA[0], bA[1], bA[2], bA[3]))\n",
    "            \n",
    "            # suppress boxes with IoU > threshold\n",
    "            for j in range(i+1, len(data)):\n",
    "                if suppressed[j]:\n",
    "                    continue\n",
    "                bB = data[j]\n",
    "                if iou(bA, bB) > iou_thresh:\n",
    "                    suppressed[j] = True\n",
    "\n",
    "        return final_boxes\n",
    "\n",
    "    ############################################################################\n",
    "    #                 GENERATING MASKS FROM BOX PROMPTS                        #\n",
    "    ############################################################################\n",
    "\n",
    "    def _box_to_corners_as_prompt(\n",
    "        self, box: BoxPrompt\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        For an example prompt encoder that is compiled to accept exactly 2 points, \n",
    "        we can pass the bounding box corners as two points:\n",
    "          - Top-left (label=1 => foreground)\n",
    "          - Bottom-right (label=1 => foreground)\n",
    "        This is somewhat naive, but demonstrates how you might feed bounding boxes \n",
    "        into a 2-point (1 for each corner) prompt encoder.\n",
    "\n",
    "        If your prompt encoder can accept a shape (1, N, 2) for an arbitrary \n",
    "        number of points, you could supply all 4 corners (some as foreground, \n",
    "        or some as background).\n",
    "        \"\"\"\n",
    "        # We'll treat the bounding box corners as \"foreground\" points.\n",
    "        # If your model specifically needs background points, you can adjust.\n",
    "        points_np = np.array([\n",
    "            [box.x1, box.y1], \n",
    "            [box.x2, box.y2]\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        labels_np = np.array([1, 1], dtype=np.int32)  # both labeled as FG\n",
    "\n",
    "        return points_np, labels_np\n",
    "\n",
    "    def generate_masks_from_boxes(\n",
    "        self,\n",
    "        boxes: List[BoxPrompt],\n",
    "        original_size: Tuple[int,int],\n",
    "        min_mask_area: int = 500,\n",
    "        iou_merge_threshold: float = 0.8\n",
    "    ) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        For each bounding box, runs the prompt encoder => mask decoder => obtains a mask.\n",
    "        Then merges duplicates using IoU among final masks. \n",
    "        Returns a list of binary masks in original image resolution.\n",
    "        \"\"\"\n",
    "        if any(model is None for model in [self.image_encoder, self.prompt_encoder, self.mask_decoder]):\n",
    "            raise ValueError(\"Models not loaded or image embedding not computed.\")\n",
    "\n",
    "        final_masks = []\n",
    "        final_scores = []\n",
    "\n",
    "        for box in boxes:\n",
    "            # Convert bounding box to 2 \"foreground\" points\n",
    "            points, labels = self._box_to_corners_as_prompt(box)\n",
    "            \n",
    "            # Transform points from original coords -> resized coords\n",
    "            points_resized = self._transform_point_coords(points, original_size)\n",
    "            labels_resized = np.expand_dims(labels, axis=0).astype(np.int32)  # shape (1,2)\n",
    "\n",
    "            # Prompt encoder\n",
    "            prompt_out = self.prompt_encoder.predict({\n",
    "                \"points\": points_resized,\n",
    "                \"labels\": labels_resized\n",
    "            })\n",
    "            sparse_embeddings = prompt_out[\"sparse_embeddings\"]\n",
    "            dense_embeddings = prompt_out[\"dense_embeddings\"]\n",
    "\n",
    "            # Mask decoder\n",
    "            low_res_mask, score = self._predict_mask(sparse_embeddings, dense_embeddings)\n",
    "\n",
    "            # Resize + binarize\n",
    "            mask_bin = self._resize_and_binarize_mask(\n",
    "                low_res_mask, original_size, threshold=0.0\n",
    "            )\n",
    "            area = cv2.countNonZero(mask_bin)\n",
    "            if area < min_mask_area:\n",
    "                continue\n",
    "\n",
    "            # Simple deduplicate: IoU with previously accepted masks\n",
    "            keep = True\n",
    "            for existing_mask in final_masks:\n",
    "                inter = np.logical_and(existing_mask, mask_bin).sum()\n",
    "                union = np.logical_or(existing_mask, mask_bin).sum()\n",
    "                iou_val = float(inter) / float(union) if union > 0 else 0.0\n",
    "                if iou_val > iou_merge_threshold:\n",
    "                    keep = False\n",
    "                    break\n",
    "\n",
    "            if keep:\n",
    "                final_masks.append(mask_bin)\n",
    "                final_scores.append(score)\n",
    "\n",
    "        return final_masks\n",
    "\n",
    "    ############################################################################\n",
    "    #                          MAIN \"AUTO\" METHOD                               #\n",
    "    ############################################################################\n",
    "\n",
    "    def auto_generate_masks(\n",
    "        self,\n",
    "        image_path: str,\n",
    "        scales=[1.0, 0.75, 0.5],\n",
    "        edge_thresh=100,\n",
    "        iou_box_thresh=0.5,\n",
    "        min_mask_area=500,\n",
    "        iou_merge_threshold: float = 0.8\n",
    "    ) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        HIGH-LEVEL PIPELINE:\n",
    "          1) Generate bounding box proposals at multiple scales\n",
    "          2) Merge duplicates via NMS\n",
    "          3) Encode the full image once (image_encoder)\n",
    "          4) For each bounding box => get mask\n",
    "          5) Merge duplicate masks by IoU\n",
    "          6) Return final list of binary masks in original resolution\n",
    "        \"\"\"\n",
    "        # 1) multi-scale bounding box proposals\n",
    "        proposals_with_scores = self._multi_scale_bounding_box_proposals(\n",
    "            image_path,\n",
    "            scales=scales,\n",
    "            edge_thresh=edge_thresh\n",
    "        )\n",
    "\n",
    "        # 2) Non-maximum-suppression for boxes\n",
    "        merged_boxes = self._merge_duplicate_boxes(\n",
    "            proposals_with_scores,\n",
    "            iou_thresh=iou_box_thresh\n",
    "        )\n",
    "        print(f\"Detected {len(merged_boxes)} bounding boxes after NMS.\")\n",
    "\n",
    "        if not merged_boxes:\n",
    "            print(\"No bounding boxes detected. Exiting mask generation.\")\n",
    "            return []\n",
    "\n",
    "        # 3) Encode the full image once\n",
    "        self.get_image_embedding(image_path)\n",
    "        # original_size was stored in self.original_size\n",
    "        orig_size = (self.original_size[0], self.original_size[1])  # (width, height)\n",
    "\n",
    "        # 4) Generate masks from bounding boxes\n",
    "        final_masks = self.generate_masks_from_boxes(\n",
    "            merged_boxes,\n",
    "            original_size=orig_size,\n",
    "            min_mask_area=min_mask_area,\n",
    "            iou_merge_threshold=iou_merge_threshold\n",
    "        )\n",
    "        print(f\"Got {len(final_masks)} final masks after box -> mask filtering.\")\n",
    "        return final_masks\n",
    "\n",
    "    def save_masks_as_color_overlay(\n",
    "        self, masks: List[np.ndarray], image_path: str, output_path: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Overlays all instance masks in random colors on the original image \n",
    "        and saves as a single composite image. \n",
    "        Each element in `masks` is a 2D binary array (H,W).\n",
    "        \"\"\"\n",
    "        image_bgr = cv2.imread(image_path)\n",
    "        if image_bgr is None:\n",
    "            raise ValueError(f\"Failed to load {image_path}\")\n",
    "        overlay = image_bgr.copy()\n",
    "\n",
    "        for mask in masks:\n",
    "            color = np.random.randint(0, 255, size=3, dtype=np.uint8)\n",
    "            # Create a mask for the current instance\n",
    "            mask_indices = mask > 0\n",
    "            # Blend the color with the original image in the mask regions\n",
    "            overlay[mask_indices] = (0.5 * overlay[mask_indices] + 0.5 * color).astype(np.uint8)\n",
    "\n",
    "        cv2.imwrite(output_path, overlay)\n",
    "        print(f\"Overlay saved to {output_path} (with {len(masks)} masks).\")\n",
    "\n",
    "    def save_masks_individually(\n",
    "        self, masks: List[np.ndarray], output_folder: str, base_name: str = \"mask\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Saves each mask as an individual binary image in the specified folder.\n",
    "        Masks are saved as PNG images with filenames like mask_1.png, mask_2.png, etc.\n",
    "\n",
    "        :param masks: List of binary mask arrays (H, W) with values 0 or 255.\n",
    "        :param output_folder: Path to the folder where masks will be saved.\n",
    "        :param base_name: Base name for mask files.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "            print(f\"Created output folder at {output_folder}\")\n",
    "\n",
    "        for idx, mask in enumerate(masks, start=1):\n",
    "            mask_filename = f\"{base_name}_{idx}.png\"\n",
    "            mask_path = os.path.join(output_folder, mask_filename)\n",
    "            # Ensure mask is in uint8 format\n",
    "            mask_uint8 = mask.astype(np.uint8)\n",
    "            cv2.imwrite(mask_path, mask_uint8)\n",
    "            print(f\"Saved mask {idx} to {mask_path}\")\n",
    "\n",
    "    ############################################################################\n",
    "    #                     SAVE SEGMENTS FROM MASKS METHOD                      #\n",
    "    ############################################################################\n",
    "\n",
    "    def save_segments_individually(\n",
    "        self, masks: List[np.ndarray], image_path: str, output_folder: str, base_name: str = \"segment\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Saves each image segment corresponding to the mask as a separate image in the specified folder.\n",
    "        The background is set to black.\n",
    "\n",
    "        :param masks: List of binary mask arrays (H, W) with values 0 or 255.\n",
    "        :param image_path: Path to the original image.\n",
    "        :param output_folder: Path to the folder where segments will be saved.\n",
    "        :param base_name: Base name for segment files.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "            print(f\"Created output folder at {output_folder}\")\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Failed to load {image_path}\")\n",
    "\n",
    "        for idx, mask in enumerate(masks, start=1):\n",
    "            # Ensure mask is binary\n",
    "            mask_bin = mask.astype(np.uint8)\n",
    "            # Apply mask to image\n",
    "            segment = cv2.bitwise_and(image, image, mask=mask_bin)\n",
    "            # Optionally, set background to transparent by adding alpha channel\n",
    "            # Not directly supported in OpenCV, but can save with background as black\n",
    "            segment_filename = f\"{base_name}_{idx}.png\"\n",
    "            segment_path = os.path.join(output_folder, segment_filename)\n",
    "            cv2.imwrite(segment_path, segment)\n",
    "            print(f\"Saved segment {idx} to {segment_path}\")\n",
    "\n",
    "###############################################################################\n",
    "#                                USAGE EXAMPLE                                #\n",
    "###############################################################################\n",
    "\n",
    "def main():\n",
    "    # 1) Initialize advanced auto-SAM\n",
    "    auto_sam = AdvancedAutoSAM(input_size=(1024, 1024))\n",
    "\n",
    "    # 2) Load your CoreML models\n",
    "    auto_sam.load_models(\n",
    "        image_encoder_path=\"./models/SAM2_1LargeImageEncoderFLOAT16.mlpackage\",\n",
    "        prompt_encoder_path=\"./models/SAM2_1LargePromptEncoderFLOAT16.mlpackage\",\n",
    "        mask_decoder_path=\"./models/SAM2_1LargeMaskDecoderFLOAT16.mlpackage\",\n",
    "    )\n",
    "\n",
    "    # 3) Define input and output directories\n",
    "    input_frames_dir = \"frames/input_video_1/\"  # Base directory containing videos\n",
    "    overlay_base_output_dir = \"generated_overlays\"\n",
    "    masks_base_output_dir = \"generated_masks\"\n",
    "    segments_base_output_dir = \"generated_segments\"\n",
    "\n",
    "    # 4) Define supported image extensions\n",
    "    supported_extensions = ('.jpg', '.jpeg', '.png', '.bmp')\n",
    "\n",
    "    # 5) Clear existing output directories to replace with new outputs\n",
    "    for output_dir in [overlay_base_output_dir, masks_base_output_dir, segments_base_output_dir]:\n",
    "        if os.path.exists(output_dir):\n",
    "            try:\n",
    "                shutil.rmtree(output_dir)\n",
    "                print(f\"Cleared existing output directory: {output_dir}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error clearing directory {output_dir}: {e}\")\n",
    "                continue\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "    # 6) Iterate over all images in the input_frames_dir\n",
    "    for root, dirs, files in os.walk(input_frames_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(supported_extensions):\n",
    "                image_path = os.path.join(root, file)\n",
    "                # Determine the relative path to maintain directory structure in outputs\n",
    "                relative_path = os.path.relpath(root, input_frames_dir)\n",
    "                \n",
    "                # Define corresponding output directories\n",
    "                overlay_output_dir = os.path.join(overlay_base_output_dir, relative_path)\n",
    "                masks_output_dir = os.path.join(masks_base_output_dir, relative_path, os.path.splitext(file)[0] + \"_masks\")\n",
    "                segments_output_dir = os.path.join(segments_base_output_dir, relative_path, os.path.splitext(file)[0] + \"_segments\")\n",
    "                \n",
    "                # Create output directories if they don't exist\n",
    "                for directory in [overlay_output_dir, masks_output_dir, segments_output_dir]:\n",
    "                    if not os.path.exists(directory):\n",
    "                        os.makedirs(directory)\n",
    "                        print(f\"Created output directory: {directory}\")\n",
    "                \n",
    "                # Define output file paths\n",
    "                base_filename = os.path.splitext(file)[0]\n",
    "                overlay_output_path = os.path.join(overlay_output_dir, f\"{base_filename}_overlay.png\")\n",
    "                individual_masks_folder = masks_output_dir\n",
    "                individual_segments_folder = segments_output_dir\n",
    "                \n",
    "                # 7) Run automatic mask generation\n",
    "                print(f\"\\nProcessing image: {image_path}\")\n",
    "                t0 = time.time()\n",
    "                try:\n",
    "                    masks = auto_sam.auto_generate_masks(\n",
    "                        image_path,\n",
    "                        scales=[1.0, 0.75, 0.5],  # multi-scale\n",
    "                        edge_thresh=100,         # for Canny\n",
    "                        iou_box_thresh=0.3,      # stricter NMS for boxes\n",
    "                        min_mask_area=1000,      # discard small masks\n",
    "                        iou_merge_threshold=0.8  # merge duplicate masks that overlap >80%\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "                    continue\n",
    "                duration = time.time() - t0\n",
    "                print(f\"Auto-mask generation took {duration:.2f} sec, produced {len(masks)} masks.\")\n",
    "\n",
    "                if not masks:\n",
    "                    print(\"No masks generated for this image. Skipping saving steps.\")\n",
    "                    continue\n",
    "\n",
    "                # 8) Save overlay for visualization\n",
    "                try:\n",
    "                    auto_sam.save_masks_as_color_overlay(masks, image_path, overlay_output_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving overlay for {image_path}: {e}\")\n",
    "\n",
    "                # 9) Save individual masks\n",
    "                try:\n",
    "                    auto_sam.save_masks_individually(masks, individual_masks_folder, base_name=\"mask\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving masks for {image_path}: {e}\")\n",
    "\n",
    "                # 10) Save individual segments\n",
    "                try:\n",
    "                    auto_sam.save_segments_individually(masks, image_path, individual_segments_folder, base_name=\"segment\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving segments for {image_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning on the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "# Pad naar de hoofd directory met geëxtraheerde frames (alle video segmenten)\n",
    "frames_dir = \"frames/input_video_1\"  # Pas dit aan naar jouw frames hoofd directory\n",
    "\n",
    "# Output JSON bestand\n",
    "output_json_path = \"combined_frames.json\"\n",
    "\n",
    "\n",
    "def initialize_captioning_model():\n",
    "\n",
    "    print(\"Initialiseren van het image captioning model...\")\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "    feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "    print(\"Model geïnitieerd.\")\n",
    "    return model, feature_extractor, tokenizer\n",
    "\n",
    "\n",
    "def generate_caption(image_path, model, feature_extractor, tokenizer, device):\n",
    "\n",
    "    try:\n",
    "        # print(f\"Generating caption for: {image_path}\")  # Optioneel: Kan veel output genereren\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "        output_ids = model.generate(pixel_values, max_length=16, num_beams=4)\n",
    "        caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating caption for {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def process_all_frames(frames_dir, model, feature_extractor, tokenizer, device):\n",
    "    combined_data = []\n",
    "    supported_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff')\n",
    "\n",
    "    if not os.path.isdir(frames_dir):\n",
    "        print(f\"Error: Frames directory {frames_dir} bestaat niet.\")\n",
    "        return combined_data\n",
    "\n",
    "    # Itereer over alle subdirectories (video segmenten)\n",
    "    video_segments = [d for d in os.listdir(frames_dir) if os.path.isdir(os.path.join(frames_dir, d))]\n",
    "    total_segments = len(video_segments)\n",
    "    print(f\"Found {total_segments} video segments in '{frames_dir}'.\")\n",
    "\n",
    "    for seg_idx, segment in enumerate(sorted(video_segments), start=1):\n",
    "        segment_path = os.path.join(frames_dir, segment)\n",
    "        frame_files = [f for f in os.listdir(segment_path) if f.lower().endswith(supported_extensions)]\n",
    "\n",
    "        if not frame_files:\n",
    "            print(f\"error: Geen frames gevonden in segment '{segment}'.\")\n",
    "            continue\n",
    "\n",
    "        total_frames = len(frame_files)\n",
    "        print(f\"Processing segment {seg_idx}/{total_segments}: '{segment}' with {total_frames} frames.\")\n",
    "\n",
    "        for idx, frame_file in enumerate(sorted(frame_files), start=1):\n",
    "            frame_path = os.path.join(segment_path, frame_file)\n",
    "\n",
    "            # Genereer een caption voor het frame\n",
    "            caption = generate_caption(frame_path, model, feature_extractor, tokenizer, device)\n",
    "            # print(f\"Generated caption for {frame_file}: {caption}\")  # Optioneel: kan veel output genereren\n",
    "\n",
    "            # Voeg de gecombineerde data toe aan de lijst\n",
    "            combined_data.append({\n",
    "                \"video_segment\": segment,  # Naam van het video segment\n",
    "                \"frame_number\": idx - 1,  # Frames starten meestal bij 0\n",
    "                \"frame_filename\": frame_file,\n",
    "                \"caption\": caption\n",
    "            })\n",
    "\n",
    "            if idx % 100 == 0 or idx == total_frames:\n",
    "                print(f\"Segment '{segment}': Processed {idx}/{total_frames} frames.\")\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "\n",
    "def save_combined_data(combined_data, output_json_path):\n",
    "\n",
    "    try:\n",
    "        with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(combined_data, f, indent=4)\n",
    "        print(f\"Combined JSON saved to '{output_json_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON file: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Initialiseer het captioning model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    model, feature_extractor, tokenizer = initialize_captioning_model()\n",
    "    model.to(device)\n",
    "\n",
    "    # Genereer captions voor alle frames\n",
    "    combined_data = process_all_frames(\n",
    "        frames_dir=frames_dir,\n",
    "        model=model,\n",
    "        feature_extractor=feature_extractor,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    if not combined_data:\n",
    "        print(\"Geen gecombineerde data om op te slaan.\")\n",
    "        return\n",
    "\n",
    "    # Sla de gecombineerde data op als JSON\n",
    "    save_combined_data(combined_data, output_json_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect Segments and Captions to Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "\n",
    "def load_json(file_path: str) -> Any:\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data: Any, file_path: str):\n",
    "  \n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"JSON opgeslagen als '{file_path}'.\")\n",
    "\n",
    "def parse_segment_filename(filename: str) -> Dict[str, Any]:\n",
    "    pattern = r\"segment_(\\d+)_(\\d+)_(\\d+)\\.mp4\"\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        return {\n",
    "            \"index\": int(match.group(1)),\n",
    "            \"start\": float(match.group(2)),\n",
    "            \"end\": float(match.group(3))\n",
    "        }\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "def find_video_segments(chunks: List[Dict[str, Any]], video_segments_dir: str) -> Dict[tuple, str]:\n",
    " \n",
    "    video_segments = os.listdir(video_segments_dir)\n",
    "    segment_map = {}\n",
    "    for segment_file in video_segments:\n",
    "        parsed = parse_segment_filename(segment_file)\n",
    "        if not parsed:\n",
    "            print(f\" file '{segment_file}' not same as pattern\")\n",
    "            continue\n",
    "        key = (parsed['start'], parsed['end'])\n",
    "        segment_map[key] = os.path.join(video_segments_dir, segment_file)\n",
    "    return segment_map\n",
    "\n",
    "def find_frames_for_segment(frames_dir: str, segment_filename: str) -> List[str]:\n",
    "\n",
    "    segment_frame_dir = os.path.join(frames_dir, segment_filename)\n",
    "    if not os.path.isdir(segment_frame_dir):\n",
    "        print(f\"error: Frames directory '{segment_frame_dir}' not exist.\")\n",
    "        return []\n",
    "    frames = [os.path.join(segment_frame_dir, f) for f in os.listdir(segment_frame_dir) \n",
    "              if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n",
    "    return sorted(frames)  # Sorteer frames op naam\n",
    "\n",
    "def find_object_segments_for_frame(generated_segments_dir: str, segment_filename: str, frame_filename: str) -> List[str]:\n",
    "    # Voorbeeld pad:\n",
    "    # generated_segments/segment_1_0_32.mp4/segment_1_0_32_mp4_frame_000003_segments/segment_1.png\n",
    "    frame_basename = os.path.splitext(os.path.basename(frame_filename))[0]  # segment_1_0_32_mp4_frame_000003\n",
    "    segments_folder = os.path.join(generated_segments_dir, segment_filename, f\"{frame_basename}_segments\")\n",
    "    if not os.path.isdir(segments_folder):\n",
    "        print(f\"Waarschuwing: Object segments directory '{segments_folder}' bestaat niet.\")\n",
    "        return []\n",
    "    object_segments = [os.path.join(segments_folder, f) for f in os.listdir(segments_folder) \n",
    "                       if f.lower().endswith('.png')]\n",
    "    return sorted(object_segments)  # Sorteer object segmenten op naam\n",
    "\n",
    "def load_captions(captions_json_path: str) -> Dict[str, str]:\n",
    "    captions_data = load_json(captions_json_path)\n",
    "    caption_map = {}\n",
    "    for entry in captions_data:\n",
    "        video_segment = entry.get('video_segment')\n",
    "        frame_filename = entry.get('frame_filename')\n",
    "        caption = entry.get('caption', \"\")\n",
    "        if video_segment and frame_filename:\n",
    "            key = f\"{video_segment}/{frame_filename}\"\n",
    "            caption_map[key] = caption\n",
    "    print(f\"Loaded captions for {len(caption_map)} frames.\")\n",
    "    return caption_map\n",
    "\n",
    "def update_chunks_with_segments_and_captions(original_chunks: List[Dict[str, Any]], \n",
    "                                segment_map: Dict[tuple, str],\n",
    "                                frames_dir: str,\n",
    "                                generated_segments_dir: str,\n",
    "                                captions_map: Dict[str, str]) -> List[Dict[str, Any]]:\n",
    "\n",
    "    updated_chunks = []\n",
    "    for chunk in original_chunks:\n",
    "        chunk_start = chunk['start']\n",
    "        chunk_end = chunk['end']\n",
    "        key = (int(chunk_start), int(chunk_end))\n",
    "        video_segment_path = segment_map.get(key)\n",
    "        if not video_segment_path:\n",
    "            print(f\"error: no video segment found for chunk start {chunk_start} and end {chunk_end}.\")\n",
    "            continue  # Of handle anders, afhankelijk van behoeften\n",
    "        \n",
    "        segment_filename = os.path.basename(video_segment_path)\n",
    "        frames = find_frames_for_segment(frames_dir, segment_filename)\n",
    "        frames_info = []\n",
    "        for frame_path in frames:\n",
    "            frame_filename = os.path.basename(frame_path)\n",
    "            object_segments = find_object_segments_for_frame(generated_segments_dir, segment_filename, frame_filename)\n",
    "            # Koppel de caption op basis van video_segment en frame_filename\n",
    "            caption_key = f\"{segment_filename}/{frame_filename}\"\n",
    "            caption = captions_map.get(caption_key, \"\")\n",
    "            frames_info.append({\n",
    "                \"frame_path\": frame_path,\n",
    "                \"object_segments\": object_segments,\n",
    "                \"caption\": caption\n",
    "            })\n",
    "        \n",
    "        # Voeg video segment en frames info toe aan de chunk\n",
    "        updated_chunk = {\n",
    "            \"text\": chunk['text'],\n",
    "            \"start\": chunk['start'],\n",
    "            \"end\": chunk['end'],\n",
    "            \"video_segment\": video_segment_path,\n",
    "            \"frames\": frames_info,\n",
    "            \"words\": chunk.get('words', [])\n",
    "        }\n",
    "        updated_chunks.append(updated_chunk)\n",
    "    \n",
    "    return updated_chunks\n",
    "\n",
    "def main():\n",
    "    # Definieer paden\n",
    "    original_json_path = \"processed_json/output_audio_1_chunks.json\"  # Originele JSON met chunks\n",
    "    video_segments_dir = \"video_segments/input_video_1\"  # Map met video segmenten\n",
    "    frames_dir = \"frames/input_video_1\"  # Hoofd map met frames per segment\n",
    "    generated_segments_dir = \"generated_segments\"  # Map met object segmenten\n",
    "    captions_json_path = \"combined_frames.json\"  # JSON met image captions\n",
    "    new_json_path = \"updated_chunks_with_segments_and_captions.json\"  # Nieuwe JSON output\n",
    "\n",
    "    # Controleer of alle benodigde bestanden en directories bestaan\n",
    "    if not os.path.exists(original_json_path):\n",
    "        print(f\"Error: Originele JSON bestand '{original_json_path}' bestaat niet.\")\n",
    "        return\n",
    "    if not os.path.isdir(video_segments_dir):\n",
    "        print(f\"Error: Video segments directory '{video_segments_dir}' bestaat niet.\")\n",
    "        return\n",
    "    if not os.path.isdir(frames_dir):\n",
    "        print(f\"Error: Frames directory '{frames_dir}' bestaat niet.\")\n",
    "        return\n",
    "    if not os.path.isdir(generated_segments_dir):\n",
    "        print(f\"Error: Generated segments directory '{generated_segments_dir}' bestaat niet.\")\n",
    "        return\n",
    "    if not os.path.exists(captions_json_path):\n",
    "        print(f\"Error: Captions JSON bestand '{captions_json_path}' bestaat niet.\")\n",
    "        return\n",
    "\n",
    "    # Laad originele JSON\n",
    "    original_data = load_json(original_json_path)\n",
    "    chunks = original_data.get('chunks', [])\n",
    "    if not chunks:\n",
    "        print(\"Geen chunks gevonden in de originele JSON.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loaded {len(chunks)} chunks from '{original_json_path}'.\")\n",
    "\n",
    "    # Maak een mapping van (start, end) tijden naar video segment paden\n",
    "    segment_map = find_video_segments(chunks, video_segments_dir)\n",
    "    print(f\"Found {len(segment_map)} video segments.\")\n",
    "\n",
    "    # Laad captions en maak een mapping\n",
    "    captions_map = load_captions(captions_json_path)\n",
    "\n",
    "    # Update chunks met video segmenten, frames, object segmenten en captions\n",
    "    updated_chunks = update_chunks_with_segments_and_captions(\n",
    "        chunks, \n",
    "        segment_map, \n",
    "        frames_dir, \n",
    "        generated_segments_dir, \n",
    "        captions_map\n",
    "    )\n",
    "\n",
    "    print(f\"Updated {len(updated_chunks)} chunks with segments and captions.\")\n",
    "\n",
    "    # Maak de nieuwe JSON structuur\n",
    "    new_data = {\n",
    "        \"chunks\": updated_chunks\n",
    "    }\n",
    "\n",
    "    # Sla de nieuwe JSON op\n",
    "    save_json(new_data, new_json_path)\n",
    "    print(f\"new JSON with connected segments and captions saved as '{new_json_path}'.\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Transformers Captions Linking to Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_json(file_path: str) -> Any:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data: Any, file_path: str):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"JSON opgeslagen als '{file_path}'.\")\n",
    "\n",
    "def preprocess_chunks(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    processed_chunks = []\n",
    "    for chunk in chunks:\n",
    "        text = chunk.get('text', \"\")\n",
    "        if text:\n",
    "            # Gebruik zowel start als end tijden als float voor unieke identificatie\n",
    "            processed_chunks.append({\n",
    "                \"chunk_id\": f\"{chunk['start']}_{chunk['end']}\",\n",
    "                \"text\": text,\n",
    "                \"start\": chunk['start'],\n",
    "                \"end\": chunk['end']\n",
    "            })\n",
    "    return processed_chunks\n",
    "\n",
    "def preprocess_captions(captions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    processed_captions = []\n",
    "    for caption in captions:\n",
    "        text = caption.get('caption', \"\")\n",
    "        if text:\n",
    "            processed_captions.append({\n",
    "                \"video_segment\": caption.get('video_segment', \"\"),\n",
    "                \"frame_filename\": caption.get('frame_filename', \"\"),\n",
    "                \"caption\": text\n",
    "            })\n",
    "    return processed_captions\n",
    "\n",
    "def compute_embeddings(model: SentenceTransformer, texts: List[str], batch_size: int = 32) -> torch.Tensor:\n",
    "\n",
    "    embeddings = model.encode(texts, batch_size=batch_size, convert_to_tensor=True, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "def link_captions_to_chunks(\n",
    "    chunks: List[Dict[str, Any]],\n",
    "    captions: List[Dict[str, Any]],\n",
    "    similarity_threshold: float = 0.3\n",
    ") -> List[Dict[str, Any]]:\n",
    "\n",
    "    # Initialiseer het SentenceTransformer model\n",
    "    model = SentenceTransformer('all-mpnet-base-v2')  # Een model dat langere teksten ondersteunt\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Voorbereiden van teksten\n",
    "    chunk_texts = [chunk['text'] for chunk in chunks]\n",
    "    caption_texts = [caption['caption'] for caption in captions]\n",
    "\n",
    "    # Compute embeddings\n",
    "    print(\"Computing embeddings for chunks...\")\n",
    "    chunk_embeddings = compute_embeddings(model, chunk_texts).to(device)\n",
    "    print(\"Computing embeddings for captions...\")\n",
    "    caption_embeddings = compute_embeddings(model, caption_texts).to(device)\n",
    "\n",
    "    # Bereken cosine similarity tussen elke caption en alle chunks\n",
    "    print(\"Calculating cosine similarities...\")\n",
    "    cosine_similarities = util.cos_sim(caption_embeddings, chunk_embeddings)  # Shape: (num_captions, num_chunks)\n",
    "\n",
    "    # Voor elke caption, vind de chunk met hoogste similarity\n",
    "    print(\"Linking captions to chunks based on similarity...\")\n",
    "    links = []\n",
    "    for idx, caption in enumerate(tqdm(captions, desc=\"Linking Captions\")):\n",
    "        sim_scores = cosine_similarities[idx]\n",
    "        top_result = torch.argmax(sim_scores).item()\n",
    "        top_score = sim_scores[top_result].item()\n",
    "        if top_score >= similarity_threshold:\n",
    "            linked_chunk = chunks[top_result]\n",
    "            links.append({\n",
    "                \"caption_index\": idx,\n",
    "                \"frame_filename\": caption['frame_filename'],\n",
    "                \"video_segment\": caption['video_segment'],\n",
    "                \"caption\": caption['caption'],\n",
    "                \"linked_chunk_id\": linked_chunk['chunk_id'],\n",
    "                \"linked_chunk_text\": linked_chunk['text'],\n",
    "                \"similarity_score\": top_score\n",
    "            })\n",
    "        else:\n",
    "            links.append({\n",
    "                \"caption_index\": idx,\n",
    "                \"frame_filename\": caption['frame_filename'],\n",
    "                \"video_segment\": caption['video_segment'],\n",
    "                \"caption\": caption['caption'],\n",
    "                \"linked_chunk_id\": None,\n",
    "                \"linked_chunk_text\": None,\n",
    "                \"similarity_score\": top_score\n",
    "            })\n",
    "    return links\n",
    "\n",
    "def filter_linked_captions(links: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Filter de links om alleen de gekoppelde captions te behouden.\n",
    "    \"\"\"\n",
    "    filtered_links = [link for link in links if link['linked_chunk_id'] is not None]\n",
    "    print(f\"Filtered captions: {len(filtered_links)} out of {len(links)} were successfully linked.\")\n",
    "    return filtered_links\n",
    "\n",
    "def main():\n",
    "    # Definieer paden\n",
    "    chunks_json_path = \"processed_json/output_audio_1_chunks.json\"        # Originele JSON met chunks\n",
    "    captions_json_path = \"combined_frames.json\"              # JSON met image captions\n",
    "    output_mapping_path = \"captions_to_chunks_mapping.json\" # Nieuwe JSON output (alle koppelingen)\n",
    "    filtered_output_path = \"filtered_captions_to_chunks_mapping.json\" # Nieuwe JSON output (gekoppelde captions)\n",
    "\n",
    "    # Controleer of alle benodigde bestanden bestaan\n",
    "    required_files = [chunks_json_path, captions_json_path]\n",
    "    for file in required_files:\n",
    "        if not os.path.exists(file):\n",
    "            print(f\"Error: Vereist bestand '{file}' bestaat niet.\")\n",
    "            return\n",
    "\n",
    "    # Laad de JSON-bestanden\n",
    "    print(\"Loading JSON files...\")\n",
    "    chunks_data = load_json(chunks_json_path)\n",
    "    captions_data = load_json(captions_json_path)\n",
    "\n",
    "    chunks = chunks_data.get('chunks', [])\n",
    "    captions = captions_data  # Verondersteld dat combined_frames.json een lijst is\n",
    "\n",
    "    if not chunks:\n",
    "        print(\"Geen chunks gevonden in de hoofd JSON.\")\n",
    "        return\n",
    "    if not captions:\n",
    "        print(\"Geen captions gevonden in de captions JSON.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loaded {len(chunks)} chunks and {len(captions)} captions.\")\n",
    "\n",
    "    # Voorbereiden van data\n",
    "    processed_chunks = preprocess_chunks(chunks)\n",
    "    processed_captions = preprocess_captions(captions)\n",
    "\n",
    "    # Koppel captions aan chunks via Sentence Transformers\n",
    "    links = link_captions_to_chunks(processed_chunks, processed_captions, similarity_threshold=0.3)\n",
    "    print(f\"Generated {len(links)} caption links.\")\n",
    "\n",
    "    # Sla de volledige mapping op als een nieuwe JSON\n",
    "    save_json(links, output_mapping_path)\n",
    "    print(f\"Captions to chunks mapping saved to '{output_mapping_path}'.\")\n",
    "\n",
    "    # Filter de gekoppelde captions\n",
    "    filtered_links = filter_linked_captions(links)\n",
    "\n",
    "    # Sla de gefilterde mapping op als een nieuwe JSON\n",
    "    save_json(filtered_links, filtered_output_path)\n",
    "    print(f\"Filtered captions to chunks mapping saved to '{filtered_output_path}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diamond",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
